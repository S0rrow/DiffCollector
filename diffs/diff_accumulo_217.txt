diff --git a/test/src/test/java/org/apache/accumulo/test/functional/SplitIT.java b/test/src/test/java/org/apache/accumulo/test/functional/SplitIT.java
index 9eab5ebdc3..e8a9d80d5a 100644
--- a/test/src/test/java/org/apache/accumulo/test/functional/SplitIT.java
+++ b/test/src/test/java/org/apache/accumulo/test/functional/SplitIT.java
@@ -27,9 +27,6 @@ import org.apache.accumulo.core.cli.BatchWriterOpts;
 import org.apache.accumulo.core.cli.ScannerOpts;
 import org.apache.accumulo.core.client.Connector;
 import org.apache.accumulo.core.client.Scanner;
-import org.apache.accumulo.core.client.admin.InstanceOperations;
-import org.apache.accumulo.core.client.security.tokens.PasswordToken;
-import org.apache.accumulo.core.conf.AccumuloConfiguration;
 import org.apache.accumulo.core.conf.Property;
 import org.apache.accumulo.core.data.Key;
 import org.apache.accumulo.core.data.KeyExtent;
@@ -38,100 +35,37 @@ import org.apache.accumulo.core.metadata.MetadataTable;
 import org.apache.accumulo.core.metadata.schema.MetadataSchema;
 import org.apache.accumulo.core.security.Authorizations;
 import org.apache.accumulo.core.util.UtilWaitThread;
-import org.apache.accumulo.harness.AccumuloClusterIT;
-import org.apache.accumulo.minicluster.ServerType;
-import org.apache.accumulo.minicluster.impl.MiniAccumuloConfigImpl;
+import org.apache.accumulo.minicluster.MiniAccumuloConfig;
 import org.apache.accumulo.server.util.CheckForMetadataProblems;
 import org.apache.accumulo.test.TestIngest;
 import org.apache.accumulo.test.VerifyIngest;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Text;
-import org.junit.After;
-import org.junit.Before;
 import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.base.Charsets;
-
-public class SplitIT extends AccumuloClusterIT {
-  private static final Logger log = LoggerFactory.getLogger(SplitIT.class);
 
+public class SplitIT extends MacTest {
+  
   @Override
-  public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {
+  public void configure(MiniAccumuloConfig cfg) {
     Map<String,String> siteConfig = new HashMap<String,String>();
     siteConfig.put(Property.TSERV_MAXMEM.getKey(), "5K");
     siteConfig.put(Property.TSERV_MAJC_DELAY.getKey(), "100ms");
     cfg.setSiteConfig(siteConfig);
   }
-
-  @Override
-  protected int defaultTimeoutSeconds() {
-    return 4 * 60;
-  }
-
-  private String tservMaxMem, tservMajcDelay;
-
-  @Before
-  public void alterConfig() throws Exception {
-    InstanceOperations iops = getConnector().instanceOperations();
-    Map<String,String> config = iops.getSystemConfiguration();
-    tservMaxMem = config.get(Property.TSERV_MAXMEM.getKey());
-    tservMajcDelay = config.get(Property.TSERV_MAJC_DELAY.getKey());
-
-    if (!tservMajcDelay.equals("100ms")) {
-      iops.setProperty(Property.TSERV_MAJC_DELAY.getKey(), "100ms");
-    }
-
-    // Property.TSERV_MAXMEM can't be altered on a running server
-    boolean restarted = false;
-    if (!tservMaxMem.equals("5K")) {
-      iops.setProperty(Property.TSERV_MAXMEM.getKey(), "5K");
-      getCluster().getClusterControl().stopAllServers(ServerType.TABLET_SERVER);
-      getCluster().getClusterControl().startAllServers(ServerType.TABLET_SERVER);
-      restarted = true;
-    }
-
-    // If we restarted the tservers, we don't need to re-wait for the majc delay
-    if (!restarted) {
-      long millis = AccumuloConfiguration.getTimeInMillis(tservMajcDelay);
-      log.info("Waiting for majc delay period: {}ms", millis);
-      Thread.sleep(millis);
-      log.info("Finished waiting for majc delay period");
-    }
-  }
-
-  @After
-  public void resetConfig() throws Exception {
-    if (null != tservMaxMem) {
-      log.info("Resetting {}={}", Property.TSERV_MAXMEM.getKey(), tservMaxMem);
-      getConnector().instanceOperations().setProperty(Property.TSERV_MAXMEM.getKey(), tservMaxMem);
-      tservMaxMem = null;
-    }
-    if (null != tservMajcDelay) {
-      log.info("Resetting {}={}", Property.TSERV_MAJC_DELAY.getKey(), tservMajcDelay);
-      getConnector().instanceOperations().setProperty(Property.TSERV_MAJC_DELAY.getKey(), tservMajcDelay);
-      tservMajcDelay = null;
-    }
-  }
-
-  @Test
+  
+  @Test(timeout = 120 * 1000)
   public void tabletShouldSplit() throws Exception {
     Connector c = getConnector();
-    String table = getUniqueNames(1)[0];
-    c.tableOperations().create(table);
-    c.tableOperations().setProperty(table, Property.TABLE_SPLIT_THRESHOLD.getKey(), "256K");
-    c.tableOperations().setProperty(table, Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE.getKey(), "1K");
+    c.tableOperations().create("test_ingest");
+    c.tableOperations().setProperty("test_ingest", Property.TABLE_SPLIT_THRESHOLD.getKey(), "256K");
+    c.tableOperations().setProperty("test_ingest", Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE.getKey(), "1K");
     TestIngest.Opts opts = new TestIngest.Opts();
     opts.rows = 100000;
-    opts.tableName = table;
     TestIngest.ingest(c, opts, new BatchWriterOpts());
     VerifyIngest.Opts vopts = new VerifyIngest.Opts();
     vopts.rows = opts.rows;
-    vopts.tableName = table;
     VerifyIngest.verifyIngest(c, vopts, new ScannerOpts());
     UtilWaitThread.sleep(15 * 1000);
-    String id = c.tableOperations().tableIdMap().get(table);
+    String id = c.tableOperations().tableIdMap().get("test_ingest");
     Scanner s = c.createScanner(MetadataTable.NAME, Authorizations.EMPTY);
     KeyExtent extent = new KeyExtent(new Text(id), null, null);
     s.setRange(extent.toMetadataRange());
@@ -144,44 +78,33 @@ public class SplitIT extends AccumuloClusterIT {
         shortened++;
       count++;
     }
-    assertTrue("Shortened should be greater than zero: " + shortened, shortened > 0);
-    assertTrue("Count should be cgreater than 10: " + count, count > 10);
-    PasswordToken token = (PasswordToken) getToken();
-    assertEquals(
-        0,
-        getCluster().getClusterControl().exec(CheckForMetadataProblems.class,
-            new String[] {"-i", cluster.getInstanceName(), "-u", "root", "-p", new String(token.getPassword(), Charsets.UTF_8), "-z", cluster.getZooKeepers()}));
+    assertTrue(shortened > 0);
+    assertTrue(count > 10);
+    assertEquals(0,
+        cluster.exec(CheckForMetadataProblems.class, "-i", cluster.getInstanceName(), "-u", "root", "-p", MacTest.PASSWORD, "-z", cluster.getZooKeepers())
+            .waitFor());
   }
-
-  @Test
+  
+  @Test(timeout = 60 * 1000)
   public void interleaveSplit() throws Exception {
     Connector c = getConnector();
-    String tableName = getUniqueNames(1)[0];
-    c.tableOperations().create(tableName);
-    c.tableOperations().setProperty(tableName, Property.TABLE_SPLIT_THRESHOLD.getKey(), "10K");
-    c.tableOperations().setProperty(tableName, Property.TABLE_FILE_COMPRESSION_TYPE.getKey(), "none");
-    UtilWaitThread.sleep(5 * 1000);
-    ReadWriteIT.interleaveTest(c, tableName);
-    UtilWaitThread.sleep(5 * 1000);
-    int numSplits = c.tableOperations().listSplits(tableName).size();
-    assertTrue("Expected at least 20 splits, saw " + numSplits, numSplits > 20);
+    c.tableOperations().create("test_ingest");
+    c.tableOperations().setProperty("test_ingest", Property.TABLE_SPLIT_THRESHOLD.getKey(), "10K");
+    c.tableOperations().setProperty("test_ingest", Property.TABLE_FILE_COMPRESSION_TYPE.getKey(), "none");
+    ReadWriteIT.interleaveTest(c);
+    UtilWaitThread.sleep(5*1000);
+    assertTrue(c.tableOperations().listSplits("test_ingest").size() > 20);
   }
-
-  @Test
+  
+  @Test(timeout = 120 * 1000)
   public void deleteSplit() throws Exception {
     Connector c = getConnector();
-    String tableName = getUniqueNames(1)[0];
-    c.tableOperations().create(tableName);
-    PasswordToken token = (PasswordToken) getToken();
-    c.tableOperations().setProperty(tableName, Property.TABLE_SPLIT_THRESHOLD.getKey(), "10K");
-    DeleteIT.deleteTest(c, getCluster(), new String(token.getPassword(), Charsets.UTF_8), tableName);
-    c.tableOperations().flush(tableName, null, null, true);
-    for (int i = 0; i < 5; i++) {
-      UtilWaitThread.sleep(10 * 1000);
-      if (c.tableOperations().listSplits(tableName).size() > 20)
-        break;
-    }
-    assertTrue(c.tableOperations().listSplits(tableName).size() > 20);
+    c.tableOperations().create("test_ingest");
+    c.tableOperations().setProperty("test_ingest", Property.TABLE_SPLIT_THRESHOLD.getKey(), "10K");
+    DeleteIT.deleteTest(c);
+    c.tableOperations().flush("test_ingest", null, null, true);
+    UtilWaitThread.sleep(10*1000);
+    assertTrue(c.tableOperations().listSplits("test_ingest").size() > 30);
   }
-
+  
 }