diff --git a/server/src/main/java/org/apache/accumulo/server/tabletserver/log/DfsLogger.java b/server/src/main/java/org/apache/accumulo/server/tabletserver/log/DfsLogger.java
index 01c2448cb3..9ab9f28693 100644
--- a/server/src/main/java/org/apache/accumulo/server/tabletserver/log/DfsLogger.java
+++ b/server/src/main/java/org/apache/accumulo/server/tabletserver/log/DfsLogger.java
@@ -32,7 +32,6 @@ import java.util.Collections;
 import java.util.EnumSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
 import java.util.Set;
 import java.util.UUID;
 import java.util.concurrent.CountDownLatch;
@@ -44,7 +43,6 @@ import org.apache.accumulo.core.conf.Property;
 import org.apache.accumulo.core.data.KeyExtent;
 import org.apache.accumulo.core.data.Mutation;
 import org.apache.accumulo.core.util.Daemon;
-import org.apache.accumulo.core.util.Pair;
 import org.apache.accumulo.core.util.StringUtil;
 import org.apache.accumulo.server.logger.LogFileKey;
 import org.apache.accumulo.server.logger.LogFileValue;
@@ -55,6 +53,8 @@ import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+//import org.apache.hadoop.fs.CreateFlag;
+//import org.apache.hadoop.fs.Syncable;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.util.Progressable;
 import org.apache.log4j.Logger;
@@ -66,50 +66,50 @@ import org.apache.log4j.Logger;
 public class DfsLogger {
   // Package private so that LogSorter can find this
   static final String LOG_FILE_HEADER_V2 = "--- Log File Header (v2) ---";
-
+  
   private static Logger log = Logger.getLogger(DfsLogger.class);
-
+  
   public static class LogClosedException extends IOException {
     private static final long serialVersionUID = 1L;
-
+    
     public LogClosedException() {
       super("LogClosed");
     }
   }
-
+  
   public interface ServerResources {
     AccumuloConfiguration getConfiguration();
-
+    
     FileSystem getFileSystem();
-
+    
     Set<TServerInstance> getCurrentTServers();
   }
-
+  
   private final LinkedBlockingQueue<DfsLogger.LogWork> workQueue = new LinkedBlockingQueue<DfsLogger.LogWork>();
-
+  
   private final Object closeLock = new Object();
-
-  private static final DfsLogger.LogWork CLOSED_MARKER = new DfsLogger.LogWork(null);
-
+  
+  private static final DfsLogger.LogWork CLOSED_MARKER = new DfsLogger.LogWork(null, null);
+  
   private static final LogFileValue EMPTY = new LogFileValue();
-
+  
   private boolean closed = false;
-
+  
   private class LogSyncingTask implements Runnable {
-
+    
     @Override
     public void run() {
       ArrayList<DfsLogger.LogWork> work = new ArrayList<DfsLogger.LogWork>();
       while (true) {
         work.clear();
-
+        
         try {
           work.add(workQueue.take());
         } catch (InterruptedException ex) {
           continue;
         }
         workQueue.drainTo(work);
-
+        
         synchronized (closeLock) {
           if (!closed) {
             try {
@@ -126,14 +126,14 @@ public class DfsLogger {
             }
           }
         }
-
+        
         boolean sawClosedMarker = false;
         for (DfsLogger.LogWork logWork : work)
           if (logWork == CLOSED_MARKER)
             sawClosedMarker = true;
           else
             logWork.latch.countDown();
-
+        
         if (sawClosedMarker) {
           synchronized (closeLock) {
             closeLock.notifyAll();
@@ -143,30 +143,32 @@ public class DfsLogger {
       }
     }
   }
-
+  
   static class LogWork {
+    List<TabletMutations> mutations;
     CountDownLatch latch;
     volatile Exception exception;
-
-    public LogWork(CountDownLatch latch) {
+    
+    public LogWork(List<TabletMutations> mutations, CountDownLatch latch) {
+      this.mutations = mutations;
       this.latch = latch;
     }
   }
-
+  
   public static class LoggerOperation {
     private final LogWork work;
-
+    
     public LoggerOperation(LogWork work) {
       this.work = work;
     }
-
+    
     public void await() throws IOException {
       try {
         work.latch.await();
       } catch (InterruptedException e) {
         throw new RuntimeException(e);
       }
-
+      
       if (work.exception != null) {
         if (work.exception instanceof IOException)
           throw (IOException) work.exception;
@@ -177,7 +179,12 @@ public class DfsLogger {
       }
     }
   }
-
+  
+  /*
+   * (non-Javadoc)
+   * 
+   * @see org.apache.accumulo.server.tabletserver.log.IRemoteLogger#equals(java.lang.Object)
+   */
   @Override
   public boolean equals(Object obj) {
     // filename is unique
@@ -187,34 +194,39 @@ public class DfsLogger {
       return getFileName().equals(((DfsLogger) obj).getFileName());
     return false;
   }
-
+  
+  /*
+   * (non-Javadoc)
+   * 
+   * @see org.apache.accumulo.server.tabletserver.log.IRemoteLogger#hashCode()
+   */
   @Override
   public int hashCode() {
     // filename is unique
     return getFileName().hashCode();
   }
-
+  
   private final ServerResources conf;
   private FSDataOutputStream logFile;
   private DataOutputStream encryptingLogFile = null;
   private Method sync;
   private Path logPath;
   private String logger;
-
+  
   public DfsLogger(ServerResources conf) throws IOException {
     this.conf = conf;
   }
-
+  
   public DfsLogger(ServerResources conf, String logger, String filename) throws IOException {
     this.conf = conf;
     this.logger = logger;
     this.logPath = new Path(Constants.getWalDirectory(conf.getConfiguration()), filename);
   }
-
+  
   public static FSDataInputStream readHeader(FileSystem fs, Path path, Map<String,String> opts) throws IOException {
     FSDataInputStream file = fs.open(path);
     try {
-      byte[] magic = LOG_FILE_HEADER_V2.getBytes(Constants.UTF8);
+      byte[] magic = LOG_FILE_HEADER_V2.getBytes();
       byte[] buffer = new byte[magic.length];
       file.readFully(buffer);
       if (Arrays.equals(buffer, magic)) {
@@ -234,25 +246,19 @@ public class DfsLogger {
       return file;
     }
   }
-
-  @SuppressWarnings("deprecation")
-  private static short _getReplication(FileSystem fs) {
-    // use fs.getDefaultReplication(logPath) in hadoop 1.2 or greater
-    return fs.getDefaultReplication();
-  }
-
+  
   public synchronized void open(String address) throws IOException {
     String filename = UUID.randomUUID().toString();
     logger = StringUtil.join(Arrays.asList(address.split(":")), "+");
-
+    
     log.debug("DfsLogger.open() begin");
-
+    
     logPath = new Path(Constants.getWalDirectory(conf.getConfiguration()) + "/" + logger + "/" + filename);
     try {
       FileSystem fs = conf.getFileSystem();
       short replication = (short) conf.getConfiguration().getCount(Property.TSERV_WAL_REPLICATION);
       if (replication == 0)
-        replication = _getReplication(fs);
+        replication = fs.getDefaultReplication();
       long blockSize = conf.getConfiguration().getMemoryInBytes(Property.TSERV_WAL_BLOCKSIZE);
       if (blockSize == 0)
         blockSize = (long) (conf.getConfiguration().getMemoryInBytes(Property.TSERV_WALOG_MAX_SIZE) * 1.1);
@@ -263,40 +269,38 @@ public class DfsLogger {
         logFile = create(fs, logPath, true, fs.getConf().getInt("io.file.buffer.size", 4096), replication, blockSize);
       else
         logFile = fs.create(logPath, true, fs.getConf().getInt("io.file.buffer.size", 4096), replication, blockSize);
-
-      String syncMethod = conf.getConfiguration().get(Property.TSERV_WAL_SYNC_METHOD);
+      
       try {
+        // sync: send data to datanodes
+        sync = logFile.getClass().getMethod("sync");
         try {
-          // hsync: send data to datanodes and sync the data to disk
-          sync = logFile.getClass().getMethod(syncMethod);
+          // hsych: send data to datanodes and sync the data to disk
+          sync = logFile.getClass().getMethod("hsync");
         } catch (NoSuchMethodException ex) {
-          log.warn("Could not find configured " + syncMethod + " method, trying to fall back to old Hadoop sync method", ex);
-
-          // sync: send data to datanodes
-          sync = logFile.getClass().getMethod("sync");
         }
       } catch (Exception e) {
         throw new RuntimeException(e);
       }
-
+      
+      
       // Initialize the crypto operations.
       @SuppressWarnings("deprecation")
       org.apache.accumulo.core.security.crypto.CryptoModule cryptoModule = org.apache.accumulo.core.security.crypto.CryptoModuleFactory.getCryptoModule(conf
           .getConfiguration().get(Property.CRYPTO_MODULE_CLASS));
-
+      
       // Initialize the log file with a header and the crypto params used to set up this log file.
-      logFile.write(LOG_FILE_HEADER_V2.getBytes(Constants.UTF8));
+      logFile.write(LOG_FILE_HEADER_V2.getBytes());
       Map<String,String> cryptoOpts = conf.getConfiguration().getAllPropertiesWithPrefix(Property.CRYPTO_PREFIX);
-
+      
       logFile.writeInt(cryptoOpts.size());
-      for (Entry<String,String> entry : cryptoOpts.entrySet()) {
-        logFile.writeUTF(entry.getKey());
-        logFile.writeUTF(entry.getValue());
+      for (String key : cryptoOpts.keySet()) {
+        logFile.writeUTF(key);
+        logFile.writeUTF(cryptoOpts.get(key));
       }
-
+      
       @SuppressWarnings("deprecation")
       OutputStream encipheringOutputStream = cryptoModule.getEncryptingOutputStream(logFile, cryptoOpts);
-
+      
       // If the module just kicks back our original stream, then just use it, don't wrap it in
       // another data OutputStream.
       if (encipheringOutputStream == logFile) {
@@ -304,7 +308,7 @@ public class DfsLogger {
       } else {
         encryptingLogFile = new DataOutputStream(encipheringOutputStream);
       }
-
+      
       LogFileKey key = new LogFileKey();
       key.event = OPEN;
       key.tserverSession = filename;
@@ -318,28 +322,28 @@ public class DfsLogger {
       logFile = null;
       throw ex;
     }
-
+    
     Thread t = new Daemon(new LogSyncingTask());
     t.setName("Accumulo WALog thread " + toString());
     t.start();
   }
-
+  
   private FSDataOutputStream create(FileSystem fs, Path logPath, boolean b, int buffersize, short replication, long blockSize) throws IOException {
     try {
-      // This...
-      // EnumSet<CreateFlag> set = EnumSet.of(CreateFlag.SYNC_BLOCK, CreateFlag.CREATE);
-      // return fs.create(logPath, FsPermission.getDefault(), set, buffersize, replication, blockSize, null);
+      // This... 
+      //    EnumSet<CreateFlag> set = EnumSet.of(CreateFlag.SYNC_BLOCK, CreateFlag.CREATE);
+      //    return fs.create(logPath, FsPermission.getDefault(), set, buffersize, replication, blockSize, null);
       // Becomes this:
       Class<?> createFlags = Class.forName("org.apache.hadoop.fs.CreateFlag");
       List<Enum<?>> flags = new ArrayList<Enum<?>>();
       if (createFlags.isEnum()) {
         for (Object constant : createFlags.getEnumConstants()) {
           if (constant.toString().equals("SYNC_BLOCK")) {
-            flags.add((Enum<?>) constant);
+            flags.add((Enum<?>)constant);
             log.debug("Found synch enum " + constant);
           }
           if (constant.toString().equals("CREATE")) {
-            flags.add((Enum<?>) constant);
+            flags.add((Enum<?>)constant);
             log.debug("Found CREATE enum " + constant);
           }
         }
@@ -347,11 +351,11 @@ public class DfsLogger {
       Object set = EnumSet.class.getMethod("of", java.lang.Enum.class, java.lang.Enum.class).invoke(null, flags.get(0), flags.get(1));
       log.debug("CreateFlag set: " + set);
       if (fs instanceof TraceFileSystem) {
-        fs = ((TraceFileSystem) fs).getImplementation();
+        fs = ((TraceFileSystem)fs).getImplementation();
       }
       Method create = fs.getClass().getMethod("create", Path.class, FsPermission.class, EnumSet.class, Integer.TYPE, Short.TYPE, Long.TYPE, Progressable.class);
       log.debug("creating " + logPath + " with SYNCH_BLOCK flag");
-      return (FSDataOutputStream) create.invoke(fs, logPath, FsPermission.getDefault(), set, buffersize, replication, blockSize, null);
+      return (FSDataOutputStream)create.invoke(fs, logPath, FsPermission.getDefault(), set, buffersize, replication, blockSize, null);
     } catch (ClassNotFoundException ex) {
       // Expected in hadoop 1.0
       return fs.create(logPath, b, buffersize, replication, blockSize);
@@ -361,21 +365,26 @@ public class DfsLogger {
     }
   }
 
+  /*
+   * (non-Javadoc)
+   * 
+   * @see org.apache.accumulo.server.tabletserver.log.IRemoteLogger#toString()
+   */
   @Override
   public String toString() {
     return getLogger() + "/" + getFileName();
   }
-
+  
   public String getLogger() {
     return logger;
   }
-
+  
   public String getFileName() {
     return logPath.getName();
   }
-
+  
   public void close() throws IOException {
-
+    
     synchronized (closeLock) {
       if (closed)
         return;
@@ -393,7 +402,7 @@ public class DfsLogger {
           log.info("Interrupted");
         }
     }
-
+    
     if (logFile != null)
       try {
         logFile.close();
@@ -402,7 +411,7 @@ public class DfsLogger {
         throw new LogClosedException();
       }
   }
-
+  
   public synchronized void defineTablet(int seq, int tid, KeyExtent tablet) throws IOException {
     // write this log to the METADATA table
     final LogFileKey key = new LogFileKey();
@@ -418,70 +427,78 @@ public class DfsLogger {
       throw ex;
     }
   }
-
+  
+  /**
+   * @param key
+   * @param empty2
+   * @throws IOException
+   */
   private synchronized void write(LogFileKey key, LogFileValue value) throws IOException {
     key.write(encryptingLogFile);
     value.write(encryptingLogFile);
   }
-
+  
   public LoggerOperation log(int seq, int tid, Mutation mutation) throws IOException {
     return logManyTablets(Collections.singletonList(new TabletMutations(tid, seq, Collections.singletonList(mutation))));
   }
-
-  private LoggerOperation logFileData(List<Pair<LogFileKey,LogFileValue>> keys) throws IOException {
-    DfsLogger.LogWork work = new DfsLogger.LogWork(new CountDownLatch(1));
+  
+  public LoggerOperation logManyTablets(List<TabletMutations> mutations) throws IOException {
+    DfsLogger.LogWork work = new DfsLogger.LogWork(mutations, new CountDownLatch(1));
+    
     synchronized (DfsLogger.this) {
       try {
-        for (Pair<LogFileKey,LogFileValue> pair : keys) {
-          write(pair.getFirst(), pair.getSecond());
+        for (TabletMutations tabletMutations : mutations) {
+          LogFileKey key = new LogFileKey();
+          key.event = MANY_MUTATIONS;
+          key.seq = tabletMutations.getSeq();
+          key.tid = tabletMutations.getTid();
+          LogFileValue value = new LogFileValue();
+          value.mutations = tabletMutations.getMutations();
+          write(key, value);
         }
       } catch (Exception e) {
         log.error(e, e);
         work.exception = e;
       }
     }
-
+    
     synchronized (closeLock) {
       // use a different lock for close check so that adding to work queue does not need
       // to wait on walog I/O operations
-
+      
       if (closed)
         throw new LogClosedException();
       workQueue.add(work);
     }
-
+    
     return new LoggerOperation(work);
   }
-
-  public LoggerOperation logManyTablets(List<TabletMutations> mutations) throws IOException {
-    List<Pair<LogFileKey,LogFileValue>> data = new ArrayList<Pair<LogFileKey,LogFileValue>>();
-    for (TabletMutations tabletMutations : mutations) {
-      LogFileKey key = new LogFileKey();
-      key.event = MANY_MUTATIONS;
-      key.seq = tabletMutations.getSeq();
-      key.tid = tabletMutations.getTid();
-      LogFileValue value = new LogFileValue();
-      value.mutations = tabletMutations.getMutations();
-      data.add(new Pair<LogFileKey,LogFileValue>(key, value));
-    }
-    return logFileData(data);
-  }
-
-  public LoggerOperation minorCompactionFinished(int seq, int tid, String fqfn) throws IOException {
+  
+  public synchronized void minorCompactionFinished(int seq, int tid, String fqfn) throws IOException {
     LogFileKey key = new LogFileKey();
     key.event = COMPACTION_FINISH;
     key.seq = seq;
     key.tid = tid;
-    return logFileData(Collections.singletonList(new Pair<LogFileKey,LogFileValue>(key, EMPTY)));
+    try {
+      write(key, EMPTY);
+    } catch (IOException ex) {
+      log.error(ex);
+      throw ex;
+    }
   }
-
-  public LoggerOperation minorCompactionStarted(int seq, int tid, String fqfn) throws IOException {
+  
+  public synchronized void minorCompactionStarted(int seq, int tid, String fqfn) throws IOException {
     LogFileKey key = new LogFileKey();
     key.event = COMPACTION_START;
     key.seq = seq;
     key.tid = tid;
     key.filename = fqfn;
-    return logFileData(Collections.singletonList(new Pair<LogFileKey,LogFileValue>(key, EMPTY)));
+    try {
+      write(key, EMPTY);
+    } catch (IOException ex) {
+      log.error(ex);
+      throw ex;
+    }
   }
-
+  
 }