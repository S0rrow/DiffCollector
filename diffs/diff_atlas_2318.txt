diff --git a/addons/hive-bridge/src/main/java/org/apache/hadoop/metadata/hive/hook/HiveHook.java b/addons/hive-bridge/src/main/java/org/apache/hadoop/metadata/hive/hook/HiveHook.java
old mode 100755
new mode 100644
index 3cad462fd..7bc11c15f
--- a/addons/hive-bridge/src/main/java/org/apache/hadoop/metadata/hive/hook/HiveHook.java
+++ b/addons/hive-bridge/src/main/java/org/apache/hadoop/metadata/hive/hook/HiveHook.java
@@ -37,9 +37,11 @@ package org.apache.hadoop.metadata.hive.hook;
 
 
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import org.antlr.runtime.tree.Tree;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.QueryPlan;
-import org.apache.hadoop.hive.ql.exec.ExplainTask;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.hooks.Entity;
 import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
@@ -47,43 +49,48 @@ import org.apache.hadoop.hive.ql.hooks.HookContext;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.HiveParser;
+import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHook;
+import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
+import org.apache.hadoop.hive.ql.parse.ParseDriver;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.HiveOperation;
+import org.apache.hadoop.metadata.MetadataServiceClient;
+import org.apache.hadoop.metadata.MetadataServiceException;
 import org.apache.hadoop.metadata.hive.bridge.HiveMetaStoreBridge;
 import org.apache.hadoop.metadata.hive.model.HiveDataTypes;
 import org.apache.hadoop.metadata.typesystem.Referenceable;
-import org.json.JSONObject;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.io.Serializable;
 import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
+import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 
 /**
  * DgiHook sends lineage information to the DgiSever.
  */
-public class HiveHook implements ExecuteWithHookContext {
-
-    private static final Logger LOG = LoggerFactory.getLogger(HiveHook.class);
+public class HiveHook implements ExecuteWithHookContext, HiveSemanticAnalyzerHook {
 
+    private static final Log LOG = LogFactory.getLog(HiveHook.class.getName());
     // wait time determines how long we wait before we exit the jvm on
     // shutdown. Pending requests after that will not be sent.
     private static final int WAIT_TIME = 3;
     private static ExecutorService executor;
 
-    private static final String MIN_THREADS = "hive.hook.dgi.minThreads";
-    private static final String MAX_THREADS = "hive.hook.dgi.maxThreads";
-    private static final String KEEP_ALIVE_TIME = "hive.hook.dgi.keepAliveTime";
-
-    private static final int minThreadsDefault = 5;
-    private static final int maxThreadsDefault = 5;
-    private static final long keepAliveTimeDefault = 10;
-    private static boolean typesRegistered = false;
-
     static {
         // anything shared should be initialized here and destroyed in the
         // shutdown hook The hook contract is weird in that it creates a
@@ -92,17 +99,15 @@ public class HiveHook implements ExecuteWithHookContext {
         // initialize the async facility to process hook calls. We don't
         // want to do this inline since it adds plenty of overhead for the
         // query.
-        HiveConf hiveConf = new HiveConf();
-        int minThreads = hiveConf.getInt(MIN_THREADS, minThreadsDefault);
-        int maxThreads = hiveConf.getInt(MAX_THREADS, maxThreadsDefault);
-        long keepAliveTime = hiveConf.getLong(KEEP_ALIVE_TIME, keepAliveTimeDefault);
-
-        executor = new ThreadPoolExecutor(minThreads, maxThreads, keepAliveTime, TimeUnit.MILLISECONDS,
-                new LinkedBlockingQueue<Runnable>(),
-                new ThreadFactoryBuilder().setDaemon(true).setNameFormat("DGI Logger %d").build());
+        executor = Executors.newSingleThreadExecutor(
+                new ThreadFactoryBuilder()
+                        .setDaemon(true)
+                        .setNameFormat("DGI Logger %d")
+                        .build());
 
         try {
-            Runtime.getRuntime().addShutdownHook(new Thread() {
+            Runtime.getRuntime().addShutdownHook(
+                    new Thread() {
                         @Override
                         public void run() {
                             try {
@@ -114,25 +119,14 @@ public class HiveHook implements ExecuteWithHookContext {
                             }
                             // shutdown client
                         }
-                    });
+                    }
+            );
         } catch (IllegalStateException is) {
             LOG.info("Attempting to send msg while shutdown in progress.");
         }
 
         LOG.info("Created DGI Hook");
-    }
-
-    class HiveEvent {
-        public HiveConf conf;
-
-        public Set<ReadEntity> inputs;
-        public Set<WriteEntity> outputs;
-
-        public String user;
-        public HiveOperation operation;
-        public QueryPlan queryPlan;
-        public HookContext.HookType hookType;
-        public JSONObject jsonPlan;
+        executor.shutdown();
     }
 
     @Override
@@ -143,200 +137,391 @@ public class HiveHook implements ExecuteWithHookContext {
         }
 
         // clone to avoid concurrent access
-        final HiveEvent event = new HiveEvent();
         final HiveConf conf = new HiveConf(hookContext.getConf());
-        boolean debug = conf.get("hive.hook.dgi.synchronous", "false").equals("true");
-
-        event.conf = conf;
-        event.inputs = hookContext.getInputs();
-        event.outputs = hookContext.getOutputs();
-
-        event.user = hookContext.getUserName() == null ? hookContext.getUgi().getUserName() : hookContext.getUserName();
-        event.operation = HiveOperation.valueOf(hookContext.getOperationName());
-        event.queryPlan = hookContext.getQueryPlan();
-        event.hookType = hookContext.getHookType();
-
-        event.jsonPlan = getQueryPlan(event);
+        boolean debug = conf.get("debug", "false").equals("true");
 
         if (debug) {
-            fireAndForget(event);
+            fireAndForget(hookContext, conf);
         } else {
-            executor.submit(new Runnable() {
-                        @Override
-                        public void run() {
-                            try {
-                                fireAndForget(event);
-                            } catch (Throwable e) {
-                                LOG.info("DGI hook failed", e);
-                            }
+            executor.submit(
+                new Runnable() {
+                    @Override
+                    public void run() {
+                        try {
+                            fireAndForget(hookContext, conf);
+                        } catch (Throwable e) {
+                            LOG.info("DGI hook failed", e);
                         }
-                    });
+                    }
+                }
+            );
         }
     }
 
-    private void fireAndForget(HiveEvent event) throws Exception {
-        assert event.hookType == HookContext.HookType.POST_EXEC_HOOK : "Non-POST_EXEC_HOOK not supported!";
-
-        LOG.info("Entered DGI hook for hook type {} operation {}", event.hookType, event.operation);
-        HiveMetaStoreBridge dgiBridge = new HiveMetaStoreBridge(event.conf);
-
-        if (!typesRegistered) {
-            dgiBridge.registerHiveDataModel();
-            typesRegistered = true;
+    private void fireAndForget(HookContext hookContext, HiveConf conf) throws Exception {
+        LOG.info("Entered DGI hook for query hook " + hookContext.getHookType());
+        if (hookContext.getHookType() != HookContext.HookType.POST_EXEC_HOOK) {
+            LOG.debug("No-op for query hook " + hookContext.getHookType());
         }
 
-        switch (event.operation) {
-        case CREATEDATABASE:
-            handleCreateDB(dgiBridge, event);
-            break;
-
-        case CREATETABLE:
-            handleCreateTable(dgiBridge, event);
-            break;
-
-        case CREATETABLE_AS_SELECT:
-        case CREATEVIEW:
-        case LOAD:
-        case EXPORT:
-        case IMPORT:
-        case QUERY:
-            registerProcess(dgiBridge, event);
-            break;
-
-        case ALTERTABLE_RENAME:
-        case ALTERVIEW_RENAME:
-            renameTable(dgiBridge, event);
-            break;
-
-        case ALTERVIEW_AS:
-            //update inputs/outputs?
-            break;
-
-        case ALTERTABLE_ADDCOLS:
-        case ALTERTABLE_REPLACECOLS:
-        case ALTERTABLE_RENAMECOL:
-            break;
-
-        default:
-        }
-    }
+        HiveMetaStoreBridge dgiBridge = new HiveMetaStoreBridge(conf);
 
-    private void renameTable(HiveMetaStoreBridge dgiBridge, HiveEvent event) throws Exception {
-        //crappy, no easy of getting new name
-        assert event.inputs != null && event.inputs.size() == 1;
-        assert event.outputs != null && event.outputs.size() > 0;
+        HiveOperation operation = HiveOperation.valueOf(hookContext.getOperationName());
 
-        Table oldTable = event.inputs.iterator().next().getTable();
-        Table newTable = null;
-        for (WriteEntity writeEntity : event.outputs) {
-            if (writeEntity.getType() == Entity.Type.TABLE) {
-                Table table = writeEntity.getTable();
-                if (table.getDbName().equals(oldTable.getDbName()) && !table.getTableName()
-                        .equals(oldTable.getTableName())) {
-                    newTable = table;
-                    break;
+        switch (operation) {
+            case CREATEDATABASE:
+                Set<WriteEntity> outputs = hookContext.getOutputs();
+                for (WriteEntity entity : outputs) {
+                    if (entity.getType() == Entity.Type.DATABASE) {
+                        dgiBridge.registerDatabase(entity.getDatabase().getName());
+                    }
                 }
-            }
-        }
-        if (newTable == null) {
-            LOG.warn("Failed to deduct new name for " + event.queryPlan.getQueryStr());
-            return;
-        }
-
-        Referenceable dbReferenceable = dgiBridge.registerDatabase(oldTable.getDbName().toLowerCase());
-        Referenceable tableReferenceable =
-                dgiBridge.registerTable(dbReferenceable, oldTable.getDbName(), oldTable.getTableName());
-        dgiBridge.getMetadataServiceClient().updateEntity(tableReferenceable.getId()._getId(), "name",
-                newTable.getTableName());
-    }
+                break;
+
+            case CREATETABLE:
+                outputs = hookContext.getOutputs();
+                for (WriteEntity entity : outputs) {
+                    if (entity.getType() == Entity.Type.TABLE) {
+
+                        Table table = entity.getTable();
+                        //TODO table.getDbName().toLowerCase() is required as hive stores in lowercase, but table.getDbName() is not lowercase
+                        Referenceable dbReferenceable = getDatabaseReference(dgiBridge, table.getDbName().toLowerCase());
+                        dgiBridge.registerTable(dbReferenceable, table.getDbName(), table.getTableName());
+                    }
+                }
+                break;
 
-    private void handleCreateTable(HiveMetaStoreBridge dgiBridge, HiveEvent event) throws Exception {
-        for (WriteEntity entity : event.outputs) {
-            if (entity.getType() == Entity.Type.TABLE) {
+            case CREATETABLE_AS_SELECT:
+                registerCTAS(dgiBridge, hookContext);
+                break;
 
-                Table table = entity.getTable();
-                //TODO table.getDbName().toLowerCase() is required as hive stores in lowercase,
-                // but table.getDbName() is not lowercase
-                Referenceable dbReferenceable = dgiBridge.registerDatabase(table.getDbName().toLowerCase());
-                dgiBridge.registerTable(dbReferenceable, table.getDbName(), table.getTableName());
-            }
+            default:
         }
     }
 
-    private void handleCreateDB(HiveMetaStoreBridge dgiBridge, HiveEvent event) throws Exception {
-        for (WriteEntity entity : event.outputs) {
-            if (entity.getType() == Entity.Type.DATABASE) {
-                dgiBridge.registerDatabase(entity.getDatabase().getName());
-            }
-        }
-    }
-
-    private void registerProcess(HiveMetaStoreBridge dgiBridge, HiveEvent event) throws Exception {
-        Set<ReadEntity> inputs = event.inputs;
-        Set<WriteEntity> outputs = event.outputs;
-
-        //Even explain CTAS has operation name as CREATETABLE_AS_SELECT
-        if (inputs.isEmpty() && outputs.isEmpty()) {
-            LOG.info("Explain statement. Skipping...");
-        }
-
-        if (event.queryPlan == null) {
-            LOG.info("Query plan is missing. Skipping...");
+    private void registerCTAS(HiveMetaStoreBridge dgiBridge, HookContext hookContext) throws Exception {
+        Set<ReadEntity> inputs = hookContext.getInputs();
+        Set<WriteEntity> outputs = hookContext.getOutputs();
+        String user = hookContext.getUserName();
+        HiveOperation operation = HiveOperation.valueOf(hookContext.getOperationName());
+        String queryId = null;
+        String queryStr = null;
+        long queryStartTime = 0;
+
+        QueryPlan plan = hookContext.getQueryPlan();
+        if (plan != null) {
+            queryId = plan.getQueryId();
+            queryStr = plan.getQueryString();
+            queryStartTime = plan.getQueryStartTime();
         }
 
-        String queryId = event.queryPlan.getQueryId();
-        String queryStr = event.queryPlan.getQueryStr();
-        long queryStartTime = event.queryPlan.getQueryStartTime();
 
-        LOG.debug("Registering CTAS query: {}", queryStr);
         Referenceable processReferenceable = new Referenceable(HiveDataTypes.HIVE_PROCESS.getName());
-        processReferenceable.set("name", event.operation.getOperationName());
+        processReferenceable.set("processName", operation.getOperationName());
         processReferenceable.set("startTime", queryStartTime);
-        processReferenceable.set("userName", event.user);
+        processReferenceable.set("userName", user);
         List<Referenceable> source = new ArrayList<>();
         for (ReadEntity readEntity : inputs) {
-            if (readEntity.getType() == Entity.Type.TABLE) {
+            if (readEntity.getTyp() == Entity.Type.TABLE) {
                 Table table = readEntity.getTable();
                 String dbName = table.getDbName().toLowerCase();
-                source.add(dgiBridge.registerTable(dbName, table.getTableName()));
-            }
-            if (readEntity.getType() == Entity.Type.PARTITION) {
-                dgiBridge.registerPartition(readEntity.getPartition());
+                source.add(getTableReference(dgiBridge, dbName, table.getTableName()));
             }
         }
-        processReferenceable.set("inputTables", source);
+        processReferenceable.set("sourceTableNames", source);
         List<Referenceable> target = new ArrayList<>();
         for (WriteEntity writeEntity : outputs) {
-            if (writeEntity.getType() == Entity.Type.TABLE || writeEntity.getType() == Entity.Type.PARTITION) {
+            if (writeEntity.getTyp() == Entity.Type.TABLE) {
                 Table table = writeEntity.getTable();
                 String dbName = table.getDbName().toLowerCase();
-                target.add(dgiBridge.registerTable(dbName, table.getTableName()));
-            }
-            if (writeEntity.getType() == Entity.Type.PARTITION) {
-                dgiBridge.registerPartition(writeEntity.getPartition());
+                target.add(getTableReference(dgiBridge, dbName, table.getTableName()));
             }
         }
-        processReferenceable.set("outputTables", target);
+        processReferenceable.set("targetTableNames", target);
         processReferenceable.set("queryText", queryStr);
         processReferenceable.set("queryId", queryId);
-        processReferenceable.set("queryPlan", event.jsonPlan.toString());
-        processReferenceable.set("endTime", System.currentTimeMillis());
-
         //TODO set
+        processReferenceable.set("endTime", queryStartTime);
+        processReferenceable.set("queryPlan", "queryPlan");
         processReferenceable.set("queryGraph", "queryGraph");
         dgiBridge.createInstance(processReferenceable);
     }
 
+    /**
+     * Gets reference for the database. Creates new instance if it doesn't exist
+     * @param dgiBridge
+     * @param dbName database name
+     * @return Reference for database
+     * @throws Exception
+     */
+    private Referenceable getDatabaseReference(HiveMetaStoreBridge dgiBridge, String dbName) throws Exception {
+        String typeName = HiveDataTypes.HIVE_DB.getName();
+        MetadataServiceClient dgiClient = dgiBridge.getMetadataServiceClient();
+
+        JSONObject result = dgiClient.search(typeName, "name", dbName);
+        JSONArray results = (JSONArray) result.get("results");
+
+        if (results.length() == 0) {
+            //Create new instance
+            return dgiBridge.registerDatabase(dbName);
+
+        } else {
+            String guid = (String) ((JSONObject) results.get(0)).get("guid");
+            return new Referenceable(guid, typeName, null);
+        }
+    }
+
+    /**
+     * Gets reference for the table. Creates new instance if it doesn't exist
+     * @param dgiBridge
+     * @param dbName
+     * @param tableName table name
+     * @return table reference
+     * @throws Exception
+     */
+    private Referenceable getTableReference(HiveMetaStoreBridge dgiBridge, String dbName, String tableName) throws Exception {
+        String typeName = HiveDataTypes.HIVE_TABLE.getName();
+        MetadataServiceClient dgiClient = dgiBridge.getMetadataServiceClient();
+
+        JSONObject result = dgiClient.search(typeName, "tableName", tableName);
+        JSONArray results = (JSONArray) result.get("results");
+
+        if (results.length() == 0) {
+            Referenceable dbRererence = getDatabaseReference(dgiBridge, dbName);
+            return dgiBridge.registerTable(dbRererence, dbName, tableName).first;
+
+        } else {
+            //There should be just one instance with the given name
+            String guid = (String) ((JSONObject) results.get(0)).get("guid");
+            return new Referenceable(guid, typeName, null);
+        }
+    }
+
+
+    //TODO Do we need this??
+    //We need to somehow get the sem associated with the plan and
+    // use it here.
+    //MySemanticAnaylzer sem = new MySemanticAnaylzer(conf);
+    //sem.setInputs(plan.getInputs());
+    //ExplainWork ew = new ExplainWork(null, null, rootTasks,
+    // plan.getFetchTask(), null, sem,
+    //        false, true, false, false, false);
+    //JSONObject explainPlan =
+    //        explain.getJSONLogicalPlan(null, ew);
+
+    private void analyzeHiveParseTree(ASTNode ast) {
+        String astStr = ast.dump();
+        Tree tab = ast.getChild(0);
+        String fullTableName;
+        boolean isExternal = false;
+        boolean isTemporary = false;
+        String inputFormat = null;
+        String outputFormat = null;
+        String serde = null;
+        String storageHandler = null;
+        String likeTableName = null;
+        String comment = null;
+        Tree ctasNode = null;
+        Tree rowFormatNode = null;
+        String location = null;
+        Map<String, String> serdeProps = new HashMap<>();
 
-    private JSONObject getQueryPlan(HiveEvent event) throws Exception {
         try {
-            ExplainTask explain = new ExplainTask();
-            explain.initialize(event.conf, event.queryPlan, null);
-            List<Task<?>> rootTasks = event.queryPlan.getRootTasks();
-            return explain.getJSONPlan(null, null, rootTasks, event.queryPlan.getFetchTask(), true, false, false);
-        } catch(Exception e) {
-            LOG.warn("Failed to get queryplan", e);
-            return new JSONObject();
+
+            BufferedWriter fw = new BufferedWriter(
+                    new FileWriter(new File("/tmp/dgi/", "ASTDump"), true));
+
+            fw.write("Full AST Dump" + astStr);
+
+
+            switch (ast.getToken().getType()) {
+                case HiveParser.TOK_CREATETABLE:
+
+
+                    if (tab.getType() != HiveParser.TOK_TABNAME ||
+                            (tab.getChildCount() != 1 && tab.getChildCount() != 2)) {
+                        LOG.error("Ignoring malformed Create table statement");
+                    }
+                    if (tab.getChildCount() == 2) {
+                        String dbName = BaseSemanticAnalyzer
+                                .unescapeIdentifier(tab.getChild(0).getText());
+                        String tableName = BaseSemanticAnalyzer
+                                .unescapeIdentifier(tab.getChild(1).getText());
+
+                        fullTableName = dbName + "." + tableName;
+                    } else {
+                        fullTableName = BaseSemanticAnalyzer
+                                .unescapeIdentifier(tab.getChild(0).getText());
+                    }
+                    LOG.info("Creating table " + fullTableName);
+                    int numCh = ast.getChildCount();
+
+                    for (int num = 1; num < numCh; num++) {
+                        ASTNode child = (ASTNode) ast.getChild(num);
+                        // Handle storage format
+                        switch (child.getToken().getType()) {
+                            case HiveParser.TOK_TABLEFILEFORMAT:
+                                if (child.getChildCount() < 2) {
+                                    throw new SemanticException(
+                                            "Incomplete specification of File Format. " +
+                                                    "You must provide InputFormat, OutputFormat.");
+                                }
+                                inputFormat = BaseSemanticAnalyzer
+                                        .unescapeSQLString(child.getChild(0).getText());
+                                outputFormat = BaseSemanticAnalyzer
+                                        .unescapeSQLString(child.getChild(1).getText());
+                                if (child.getChildCount() == 3) {
+                                    serde = BaseSemanticAnalyzer
+                                            .unescapeSQLString(child.getChild(2).getText());
+                                }
+                                break;
+                            case HiveParser.TOK_STORAGEHANDLER:
+                                storageHandler = BaseSemanticAnalyzer
+                                        .unescapeSQLString(child.getChild(0).getText());
+                                if (child.getChildCount() == 2) {
+                                    BaseSemanticAnalyzer.readProps(
+                                            (ASTNode) (child.getChild(1).getChild(0)),
+                                            serdeProps);
+                                }
+                                break;
+                            case HiveParser.TOK_FILEFORMAT_GENERIC:
+                                ASTNode grandChild = (ASTNode) child.getChild(0);
+                                String name = (grandChild == null ? "" : grandChild.getText())
+                                        .trim().toUpperCase();
+                                if (name.isEmpty()) {
+                                    LOG.error("File format in STORED AS clause is empty");
+                                    break;
+                                }
+                                break;
+                        }
+
+                        switch (child.getToken().getType()) {
+
+                            case HiveParser.KW_EXTERNAL:
+                                isExternal = true;
+                                break;
+                            case HiveParser.KW_TEMPORARY:
+                                isTemporary = true;
+                                break;
+                            case HiveParser.TOK_LIKETABLE:
+                                if (child.getChildCount() > 0) {
+                                    likeTableName = BaseSemanticAnalyzer
+                                            .getUnescapedName((ASTNode) child.getChild(0));
+                                }
+                                break;
+                            case HiveParser.TOK_QUERY:
+                                ctasNode = child;
+                                break;
+                            case HiveParser.TOK_TABLECOMMENT:
+                                comment = BaseSemanticAnalyzer
+                                        .unescapeSQLString(child.getChild(0).getText());
+                                break;
+                            case HiveParser.TOK_TABLEPARTCOLS:
+                            case HiveParser.TOK_TABCOLLIST:
+                            case HiveParser.TOK_ALTERTABLE_BUCKETS:
+                                break;
+                            case HiveParser.TOK_TABLEROWFORMAT:
+                                rowFormatNode = child;
+                                break;
+                            case HiveParser.TOK_TABLELOCATION:
+                                location = BaseSemanticAnalyzer
+                                        .unescapeSQLString(child.getChild(0).getText());
+                                break;
+                            case HiveParser.TOK_TABLEPROPERTIES:
+                                break;
+                            case HiveParser.TOK_TABLESERIALIZER:
+                                child = (ASTNode) child.getChild(0);
+                                serde = BaseSemanticAnalyzer
+                                        .unescapeSQLString(child.getChild(0).getText());
+                                break;
+                            case HiveParser.TOK_TABLESKEWED:
+                                break;
+                            default:
+                                throw new AssertionError("Unknown token: " + child.getToken());
+                        }
+                    }
+                    StringBuilder sb = new StringBuilder(1024);
+                    sb.append("Full table name: ").append(fullTableName).append('\n');
+                    sb.append("\tisTemporary: ").append(isTemporary).append('\n');
+                    sb.append("\tIsExternal: ").append(isExternal).append('\n');
+                    if (inputFormat != null) {
+                        sb.append("\tinputFormat: ").append(inputFormat).append('\n');
+                    }
+                    if (outputFormat != null) {
+                        sb.append("\toutputFormat: ").append(outputFormat).append('\n');
+                    }
+                    if (serde != null) {
+                        sb.append("\tserde: ").append(serde).append('\n');
+                    }
+                    if (storageHandler != null) {
+                        sb.append("\tstorageHandler: ").append(storageHandler).append('\n');
+                    }
+                    if (likeTableName != null) {
+                        sb.append("\tlikeTableName: ").append(likeTableName);
+                    }
+                    if (comment != null) {
+                        sb.append("\tcomment: ").append(comment);
+                    }
+                    if (location != null) {
+                        sb.append("\tlocation: ").append(location);
+                    }
+                    if (ctasNode != null) {
+                        sb.append("\tctasNode: ").append(((ASTNode) ctasNode).dump());
+                    }
+                    if (rowFormatNode != null) {
+                        sb.append("\trowFormatNode: ").append(((ASTNode) rowFormatNode).dump());
+                    }
+                    fw.write(sb.toString());
+            }
+            fw.flush();
+            fw.close();
+        } catch (Exception e) {
+            LOG.error("Unable to log logical plan to file", e);
+        }
+    }
+
+    private void parseQuery(String sqlText) throws Exception {
+        ParseDriver parseDriver = new ParseDriver();
+        ASTNode node = parseDriver.parse(sqlText);
+        analyzeHiveParseTree(node);
+    }
+
+    /**
+     * This is  an attempt to use the parser.  Sematnic issues are not handled here.
+     *
+     * Trying to recompile the query runs into some issues in the preExec
+     * hook but we need to make sure all the semantic issues are handled.  May be we should save the AST in the
+     * Semantic analyzer and have it available in the preExec hook so that we walk with it freely.
+     *
+     * @param context
+     * @param ast
+     * @return
+     * @throws SemanticException
+     */
+    @Override
+    public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)
+            throws SemanticException {
+        analyzeHiveParseTree(ast);
+        return ast;
+    }
+
+    @Override
+    public void postAnalyze(HiveSemanticAnalyzerHookContext context,
+                            List<Task<? extends Serializable>> rootTasks) throws SemanticException {
+
+    }
+
+    private class MySemanticAnaylzer extends BaseSemanticAnalyzer {
+        public MySemanticAnaylzer(HiveConf conf) throws SemanticException {
+            super(conf);
+        }
+
+        public void analyzeInternal(ASTNode ast) throws SemanticException {
+            throw new RuntimeException("Not implemented");
+        }
+
+        public void setInputs(HashSet<ReadEntity> inputs) {
+            this.inputs = inputs;
         }
     }
 }