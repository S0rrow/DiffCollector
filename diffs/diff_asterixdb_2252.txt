diff --git a/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/nc/RecoveryManager.java b/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/nc/RecoveryManager.java
index 273d83287f..07f341d7d5 100644
--- a/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/nc/RecoveryManager.java
+++ b/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/nc/RecoveryManager.java
@@ -39,17 +39,16 @@ import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
-import java.util.stream.Collectors;
+import java.util.logging.Level;
+import java.util.logging.Logger;
 
 import org.apache.asterix.common.api.IDatasetLifecycleManager;
-import org.apache.asterix.common.api.INcApplicationContext;
 import org.apache.asterix.common.config.ReplicationProperties;
-import org.apache.asterix.common.dataflow.DatasetLocalResource;
 import org.apache.asterix.common.exceptions.ACIDException;
 import org.apache.asterix.common.ioopcallbacks.AbstractLSMIOOperationCallback;
-import org.apache.asterix.common.storage.DatasetResourceReference;
-import org.apache.asterix.common.storage.IIndexCheckpointManagerProvider;
+import org.apache.asterix.common.replication.IReplicaResourcesManager;
 import org.apache.asterix.common.transactions.Checkpoint;
+import org.apache.asterix.common.transactions.IAppRuntimeContextProvider;
 import org.apache.asterix.common.transactions.ICheckpointManager;
 import org.apache.asterix.common.transactions.ILogReader;
 import org.apache.asterix.common.transactions.ILogRecord;
@@ -57,25 +56,23 @@ import org.apache.asterix.common.transactions.IRecoveryManager;
 import org.apache.asterix.common.transactions.ITransactionContext;
 import org.apache.asterix.common.transactions.ITransactionSubsystem;
 import org.apache.asterix.common.transactions.LogType;
+import org.apache.asterix.common.transactions.Resource;
 import org.apache.asterix.transaction.management.opcallbacks.AbstractIndexModificationOperationCallback;
 import org.apache.asterix.transaction.management.resource.PersistentLocalResourceRepository;
 import org.apache.asterix.transaction.management.service.logging.LogManager;
 import org.apache.asterix.transaction.management.service.recovery.AbstractCheckpointManager;
-import org.apache.asterix.transaction.management.service.recovery.TxnEntityId;
+import org.apache.asterix.transaction.management.service.recovery.TxnId;
 import org.apache.asterix.transaction.management.service.transaction.TransactionManagementConstants;
 import org.apache.commons.io.FileUtils;
 import org.apache.hyracks.api.application.INCServiceContext;
-import org.apache.hyracks.api.exceptions.ErrorCode;
 import org.apache.hyracks.api.exceptions.HyracksDataException;
 import org.apache.hyracks.api.lifecycle.ILifeCycleComponent;
-import org.apache.hyracks.storage.am.common.impls.NoOpIndexAccessParameters;
+import org.apache.hyracks.storage.am.common.api.IIndex;
+import org.apache.hyracks.storage.am.common.impls.NoOpOperationCallback;
 import org.apache.hyracks.storage.am.lsm.common.api.ILSMIndex;
 import org.apache.hyracks.storage.am.lsm.common.api.ILSMIndexAccessor;
 import org.apache.hyracks.storage.am.lsm.common.impls.AbstractLSMIndex;
-import org.apache.hyracks.storage.common.IIndex;
-import org.apache.hyracks.storage.common.LocalResource;
-import org.apache.logging.log4j.Level;
-import org.apache.logging.log4j.Logger;
+import org.apache.hyracks.storage.common.file.LocalResource;
 
 /**
  * This is the Recovery Manager and is responsible for rolling back a
@@ -84,27 +81,27 @@ import org.apache.logging.log4j.Logger;
 public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
 
     public static final boolean IS_DEBUG_MODE = false;
-    private static final Logger LOGGER = org.apache.logging.log4j.LogManager.getLogger();
+    private static final Logger LOGGER = Logger.getLogger(RecoveryManager.class.getName());
     private final ITransactionSubsystem txnSubsystem;
     private final LogManager logMgr;
     private final boolean replicationEnabled;
     private static final String RECOVERY_FILES_DIR_NAME = "recovery_temp";
-    private Map<Long, JobEntityCommits> jobId2WinnerEntitiesMap = null;
+    private Map<Integer, JobEntityCommits> jobId2WinnerEntitiesMap = null;
     private final long cachedEntityCommitsPerJobSize;
     private final PersistentLocalResourceRepository localResourceRepository;
     private final ICheckpointManager checkpointManager;
     private SystemState state;
     private final INCServiceContext serviceCtx;
-    private final INcApplicationContext appCtx;
 
     public RecoveryManager(ITransactionSubsystem txnSubsystem, INCServiceContext serviceCtx) {
         this.serviceCtx = serviceCtx;
         this.txnSubsystem = txnSubsystem;
-        this.appCtx = txnSubsystem.getApplicationContext();
         logMgr = (LogManager) txnSubsystem.getLogManager();
-        ReplicationProperties repProperties = appCtx.getReplicationProperties();
-        replicationEnabled = repProperties.isReplicationEnabled();
-        localResourceRepository = (PersistentLocalResourceRepository) appCtx.getLocalResourceRepository();
+        ReplicationProperties repProperties =
+                txnSubsystem.getAsterixAppRuntimeContextProvider().getAppContext().getReplicationProperties();
+        replicationEnabled = repProperties.isParticipant(txnSubsystem.getId());
+        localResourceRepository = (PersistentLocalResourceRepository) txnSubsystem.getAsterixAppRuntimeContextProvider()
+                .getLocalResourceRepository();
         cachedEntityCommitsPerJobSize = txnSubsystem.getTransactionProperties().getJobRecoveryMemorySize();
         checkpointManager = txnSubsystem.getCheckpointManager();
     }
@@ -125,29 +122,68 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
             //The checkpoint file doesn't exist => Failure happened during NC initialization.
             //Retry to initialize the NC by setting the state to PERMANENT_DATA_LOSS
             state = SystemState.PERMANENT_DATA_LOSS;
-            LOGGER.info("The checkpoint file doesn't exist: systemState = PERMANENT_DATA_LOSS");
+            if (LOGGER.isLoggable(Level.INFO)) {
+                LOGGER.info("The checkpoint file doesn't exist: systemState = PERMANENT_DATA_LOSS");
+            }
             return state;
         }
-        long readableSmallestLSN = logMgr.getReadableSmallestLSN();
-        if (logMgr.getAppendLSN() == readableSmallestLSN) {
-            if (checkpointObject.getMinMCTFirstLsn() != AbstractCheckpointManager.SHARP_CHECKPOINT_LSN) {
-                LOGGER.warn("Some(or all) of transaction log files are lost.");
-                //No choice but continuing when the log files are lost.
-            }
-            state = SystemState.HEALTHY;
-        } else if (checkpointObject.getCheckpointLsn() == logMgr.getAppendLSN()
-                && checkpointObject.getMinMCTFirstLsn() == AbstractCheckpointManager.SHARP_CHECKPOINT_LSN) {
-            state = SystemState.HEALTHY;
+
+        if (replicationEnabled) {
+            if (checkpointObject.getMinMCTFirstLsn() == AbstractCheckpointManager.SHARP_CHECKPOINT_LSN) {
+                //no logs exist
+                state = SystemState.HEALTHY;
+            } else if (checkpointObject.getCheckpointLsn() == logMgr.getAppendLSN() && checkpointObject.isSharp()) {
+                //only remote logs exist
+                state = SystemState.HEALTHY;
+            } else {
+                //need to perform remote recovery
+                state = SystemState.CORRUPTED;
+            }
         } else {
-            state = SystemState.CORRUPTED;
+            long readableSmallestLSN = logMgr.getReadableSmallestLSN();
+            if (logMgr.getAppendLSN() == readableSmallestLSN) {
+                if (checkpointObject.getMinMCTFirstLsn() != AbstractCheckpointManager.SHARP_CHECKPOINT_LSN) {
+                    LOGGER.warning("Some(or all) of transaction log files are lost.");
+                    //No choice but continuing when the log files are lost.
+                }
+                state = SystemState.HEALTHY;
+            } else if (checkpointObject.getCheckpointLsn() == logMgr.getAppendLSN()
+                    && checkpointObject.getMinMCTFirstLsn() == AbstractCheckpointManager.SHARP_CHECKPOINT_LSN) {
+                state = SystemState.HEALTHY;
+            } else {
+                state = SystemState.CORRUPTED;
+            }
         }
         return state;
     }
 
+    //This method is used only when replication is disabled.
+    @Override
+    public void startRecovery(boolean synchronous) throws IOException, ACIDException {
+        state = SystemState.RECOVERING;
+        LOGGER.log(Level.INFO, "starting recovery ...");
+
+        long readableSmallestLSN = logMgr.getReadableSmallestLSN();
+        Checkpoint checkpointObject = checkpointManager.getLatest();
+        long lowWaterMarkLSN = checkpointObject.getMinMCTFirstLsn();
+        if (lowWaterMarkLSN < readableSmallestLSN) {
+            lowWaterMarkLSN = readableSmallestLSN;
+        }
+
+        //delete any recovery files from previous failed recovery attempts
+        deleteRecoveryTemporaryFiles();
+
+        //get active partitions on this node
+        Set<Integer> activePartitions = localResourceRepository.getNodeOrignalPartitions();
+        replayPartitionsLogs(activePartitions, logMgr.getLogReader(true), lowWaterMarkLSN);
+    }
+
     @Override
     public void startLocalRecovery(Set<Integer> partitions) throws IOException, ACIDException {
         state = SystemState.RECOVERING;
-        LOGGER.info("starting recovery ...");
+        if (LOGGER.isLoggable(Level.INFO)) {
+            LOGGER.info("starting recovery ...");
+        }
 
         long readableSmallestLSN = logMgr.getReadableSmallestLSN();
         Checkpoint checkpointObject = checkpointManager.getLatest();
@@ -167,7 +203,7 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
     public synchronized void replayPartitionsLogs(Set<Integer> partitions, ILogReader logReader, long lowWaterMarkLSN)
             throws IOException, ACIDException {
         try {
-            Set<Long> winnerJobSet = startRecoverysAnalysisPhase(partitions, logReader, lowWaterMarkLSN);
+            Set<Integer> winnerJobSet = startRecoverysAnalysisPhase(partitions, logReader, lowWaterMarkLSN);
             startRecoveryRedoPhase(partitions, logReader, lowWaterMarkLSN, winnerJobSet);
         } finally {
             logReader.close();
@@ -175,17 +211,17 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
         }
     }
 
-    private synchronized Set<Long> startRecoverysAnalysisPhase(Set<Integer> partitions, ILogReader logReader,
+    private synchronized Set<Integer> startRecoverysAnalysisPhase(Set<Integer> partitions, ILogReader logReader,
             long lowWaterMarkLSN) throws IOException, ACIDException {
         int updateLogCount = 0;
         int entityCommitLogCount = 0;
         int jobCommitLogCount = 0;
         int abortLogCount = 0;
-        Set<Long> winnerJobSet = new HashSet<>();
+        Set<Integer> winnerJobSet = new HashSet<>();
         jobId2WinnerEntitiesMap = new HashMap<>();
         //set log reader to the lowWaterMarkLsn
         ILogRecord logRecord;
-        logReader.setPosition(lowWaterMarkLSN);
+        logReader.initializeScan(lowWaterMarkLSN);
         logRecord = logReader.next();
         while (logRecord != null) {
             if (IS_DEBUG_MODE) {
@@ -198,8 +234,8 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
                     }
                     break;
                 case LogType.JOB_COMMIT:
-                    winnerJobSet.add(logRecord.getTxnId());
-                    cleanupTxnCommits(logRecord.getTxnId());
+                    winnerJobSet.add(logRecord.getJobId());
+                    cleanupJobCommits(logRecord.getJobId());
                     jobCommitLogCount++;
                     break;
                 case LogType.ENTITY_COMMIT:
@@ -233,77 +269,76 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
         return winnerJobSet;
     }
 
-    private void cleanupTxnCommits(long txnId) {
-        if (jobId2WinnerEntitiesMap.containsKey(txnId)) {
-            JobEntityCommits jobEntityWinners = jobId2WinnerEntitiesMap.get(txnId);
+    private void cleanupJobCommits(int jobId) {
+        if (jobId2WinnerEntitiesMap.containsKey(jobId)) {
+            JobEntityCommits jobEntityWinners = jobId2WinnerEntitiesMap.get(jobId);
             //to delete any spilled files as well
             jobEntityWinners.clear();
-            jobId2WinnerEntitiesMap.remove(txnId);
+            jobId2WinnerEntitiesMap.remove(jobId);
         }
     }
 
     private void analyzeEntityCommitLog(ILogRecord logRecord) throws IOException {
-        long txnId = logRecord.getTxnId();
+        int jobId = logRecord.getJobId();
         JobEntityCommits jobEntityWinners;
-        if (!jobId2WinnerEntitiesMap.containsKey(txnId)) {
-            jobEntityWinners = new JobEntityCommits(txnId);
+        if (!jobId2WinnerEntitiesMap.containsKey(jobId)) {
+            jobEntityWinners = new JobEntityCommits(jobId);
             if (needToFreeMemory()) {
                 // If we don't have enough memory for one more job,
                 // we will force all jobs to spill their cached entities to disk.
                 // This could happen only when we have many jobs with small
                 // number of records and none of them have job commit.
-                freeJobsCachedEntities(txnId);
+                freeJobsCachedEntities(jobId);
             }
-            jobId2WinnerEntitiesMap.put(txnId, jobEntityWinners);
+            jobId2WinnerEntitiesMap.put(jobId, jobEntityWinners);
         } else {
-            jobEntityWinners = jobId2WinnerEntitiesMap.get(txnId);
+            jobEntityWinners = jobId2WinnerEntitiesMap.get(jobId);
         }
         jobEntityWinners.add(logRecord);
     }
 
     private synchronized void startRecoveryRedoPhase(Set<Integer> partitions, ILogReader logReader,
-            long lowWaterMarkLSN, Set<Long> winnerTxnSet) throws IOException, ACIDException {
+            long lowWaterMarkLSN, Set<Integer> winnerJobSet) throws IOException, ACIDException {
         int redoCount = 0;
-        long txnId = 0;
+        int jobId = -1;
 
         long resourceId;
         long maxDiskLastLsn;
         long lsn = -1;
         ILSMIndex index = null;
         LocalResource localResource = null;
-        DatasetLocalResource localResourceMetadata = null;
+        Resource localResourceMetadata = null;
         boolean foundWinner = false;
         JobEntityCommits jobEntityWinners = null;
 
-        IDatasetLifecycleManager datasetLifecycleManager = appCtx.getDatasetLifecycleManager();
-        final IIndexCheckpointManagerProvider indexCheckpointManagerProvider =
-                ((INcApplicationContext) (serviceCtx.getApplicationContext())).getIndexCheckpointManagerProvider();
+        IAppRuntimeContextProvider appRuntimeContext = txnSubsystem.getAsterixAppRuntimeContextProvider();
+        IDatasetLifecycleManager datasetLifecycleManager = appRuntimeContext.getDatasetLifecycleManager();
 
         Map<Long, LocalResource> resourcesMap = localResourceRepository.loadAndGetAllResources();
         Map<Long, Long> resourceId2MaxLSNMap = new HashMap<>();
-        TxnEntityId tempKeyTxnEntityId = new TxnEntityId(-1, -1, -1, null, -1, false);
+        TxnId tempKeyTxnId = new TxnId(-1, -1, -1, null, -1, false);
 
         ILogRecord logRecord = null;
         try {
-            logReader.setPosition(lowWaterMarkLSN);
+            logReader.initializeScan(lowWaterMarkLSN);
             logRecord = logReader.next();
             while (logRecord != null) {
                 if (IS_DEBUG_MODE) {
                     LOGGER.info(logRecord.getLogRecordForDisplay());
                 }
                 lsn = logRecord.getLSN();
-                txnId = logRecord.getTxnId();
+                jobId = logRecord.getJobId();
                 foundWinner = false;
                 switch (logRecord.getLogType()) {
                     case LogType.UPDATE:
                         if (partitions.contains(logRecord.getResourcePartition())) {
-                            if (winnerTxnSet.contains(txnId)) {
+                            if (winnerJobSet.contains(jobId)) {
                                 foundWinner = true;
-                            } else if (jobId2WinnerEntitiesMap.containsKey(txnId)) {
-                                jobEntityWinners = jobId2WinnerEntitiesMap.get(txnId);
-                                tempKeyTxnEntityId.setTxnId(txnId, logRecord.getDatasetId(), logRecord.getPKHashValue(),
+                            } else if (jobId2WinnerEntitiesMap.containsKey(jobId)) {
+                                jobEntityWinners = jobId2WinnerEntitiesMap.get(jobId);
+                                tempKeyTxnId.setTxnId(jobId, logRecord.getDatasetId(), logRecord.getPKHashValue(),
                                         logRecord.getPKValue(), logRecord.getPKValueSize());
-                                if (jobEntityWinners.containsEntityCommitForTxnId(lsn, tempKeyTxnEntityId)) {
+                                if (jobEntityWinners.containsEntityCommitForTxnId(lsn, tempKeyTxnId)) {
                                     foundWinner = true;
                                 }
                             }
@@ -324,7 +359,6 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
                                  * log record.
                                  *******************************************************************/
                                 if (localResource == null) {
-                                    LOGGER.log(Level.WARN, "resource was not found for resource id " + resourceId);
                                     logRecord = logReader.next();
                                     continue;
                                 }
@@ -334,29 +368,32 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
                                 //if index is not registered into IndexLifeCycleManager,
                                 //create the index using LocalMetadata stored in LocalResourceRepository
                                 //get partition path in this node
-                                localResourceMetadata = (DatasetLocalResource) localResource.getResource();
+                                localResourceMetadata = (Resource) localResource.getResource();
                                 index = (ILSMIndex) datasetLifecycleManager.get(localResource.getPath());
                                 if (index == null) {
                                     //#. create index instance and register to indexLifeCycleManager
-                                    index = (ILSMIndex) localResourceMetadata.createInstance(serviceCtx);
+                                    index = localResourceMetadata.createIndexInstance(serviceCtx, localResource);
                                     datasetLifecycleManager.register(localResource.getPath(), index);
                                     datasetLifecycleManager.open(localResource.getPath());
+
+                                    //#. get maxDiskLastLSN
+                                    ILSMIndex lsmIndex = index;
                                     try {
-                                        final DatasetResourceReference resourceReference =
-                                                DatasetResourceReference.of(localResource);
                                         maxDiskLastLsn =
-                                                indexCheckpointManagerProvider.get(resourceReference).getLowWatermark();
+                                                ((AbstractLSMIOOperationCallback) lsmIndex.getIOOperationCallback())
+                                                        .getComponentLSN(lsmIndex.getImmutableComponents());
                                     } catch (HyracksDataException e) {
                                         datasetLifecycleManager.close(localResource.getPath());
                                         throw e;
                                     }
+
                                     //#. set resourceId and maxDiskLastLSN to the map
                                     resourceId2MaxLSNMap.put(resourceId, maxDiskLastLsn);
                                 } else {
                                     maxDiskLastLsn = resourceId2MaxLSNMap.get(resourceId);
                                 }
-                                // lsn @ maxDiskLastLsn is either a flush log or a master replica log
-                                if (lsn >= maxDiskLastLsn) {
+
+                                if (lsn > maxDiskLastLsn) {
                                     redo(logRecord, datasetLifecycleManager);
                                     redoCount++;
                                 }
@@ -378,7 +415,6 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
             }
             LOGGER.info("Logs REDO phase completed. Redo logs count: " + redoCount);
         } finally {
-            txnSubsystem.getTransactionManager().ensureMaxTxnId(txnId);
             //close all indexes
             Set<Long> resourceIdList = resourceId2MaxLSNMap.keySet();
             for (long r : resourceIdList) {
@@ -406,7 +442,8 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
 
     @Override
     public long getLocalMinFirstLSN() throws HyracksDataException {
-        final IDatasetLifecycleManager datasetLifecycleManager = appCtx.getDatasetLifecycleManager();
+        IDatasetLifecycleManager datasetLifecycleManager =
+                txnSubsystem.getAsterixAppRuntimeContextProvider().getDatasetLifecycleManager();
         List<IIndex> openIndexList = datasetLifecycleManager.getOpenResources();
         long firstLSN;
         //the min first lsn can only be the current append or smaller
@@ -424,53 +461,16 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
         return minFirstLSN;
     }
 
-    private long getRemoteMinFirstLSN() throws HyracksDataException {
-        // find the min first lsn of partitions that are replicated on this node
-        final Set<Integer> allPartitions = localResourceRepository.getAllPartitions();
-        final Set<Integer> masterPartitions = appCtx.getReplicaManager().getPartitions();
-        allPartitions.removeAll(masterPartitions);
-        return getPartitionsMinLSN(allPartitions);
-    }
-
-    private long getPartitionsMinLSN(Set<Integer> partitions) throws HyracksDataException {
-        final IIndexCheckpointManagerProvider idxCheckpointMgrProvider = appCtx.getIndexCheckpointManagerProvider();
-        long minRemoteLSN = Long.MAX_VALUE;
-        for (Integer partition : partitions) {
-            final List<DatasetResourceReference> partitionResources = localResourceRepository.getResources(resource -> {
-                DatasetLocalResource dsResource = (DatasetLocalResource) resource.getResource();
-                return dsResource.getPartition() == partition;
-            }).values().stream().map(DatasetResourceReference::of).collect(Collectors.toList());
-            for (DatasetResourceReference indexRef : partitionResources) {
-                long remoteIndexMaxLSN = idxCheckpointMgrProvider.get(indexRef).getLowWatermark();
-                minRemoteLSN = Math.min(minRemoteLSN, remoteIndexMaxLSN);
-            }
-        }
-        return minRemoteLSN;
-    }
-
-    @Override
-    public void replayReplicaPartitionLogs(Set<Integer> partitions, boolean flush) throws HyracksDataException {
-        long minLSN = getPartitionsMinLSN(partitions);
-        long readableSmallestLSN = logMgr.getReadableSmallestLSN();
-        if (minLSN < readableSmallestLSN) {
-            minLSN = readableSmallestLSN;
-        }
-
-        //replay logs > minLSN that belong to these partitions
-        try {
-            replayPartitionsLogs(partitions, logMgr.getLogReader(true), minLSN);
-            if (flush) {
-                appCtx.getDatasetLifecycleManager().flushAllDatasets();
-            }
-        } catch (IOException | ACIDException e) {
-            throw HyracksDataException.create(e);
-        }
+    private long getRemoteMinFirstLSN() {
+        IReplicaResourcesManager remoteResourcesManager =
+                txnSubsystem.getAsterixAppRuntimeContextProvider().getAppContext().getReplicaResourcesManager();
+        return remoteResourcesManager.getPartitionsMinLSN(localResourceRepository.getInactivePartitions());
     }
 
     @Override
-    public File createJobRecoveryFile(long txnId, String fileName) throws IOException {
+    public File createJobRecoveryFile(int jobId, String fileName) throws IOException {
         String recoveryDirPath = getRecoveryDirPath();
-        Path jobRecoveryFolder = Paths.get(recoveryDirPath + File.separator + txnId);
+        Path jobRecoveryFolder = Paths.get(recoveryDirPath + File.separator + jobId);
         if (!Files.exists(jobRecoveryFolder)) {
             Files.createDirectories(jobRecoveryFolder);
         }
@@ -478,10 +478,10 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
         File jobRecoveryFile = new File(jobRecoveryFolder.toString() + File.separator + fileName);
         if (!jobRecoveryFile.exists()) {
             if (!jobRecoveryFile.createNewFile()) {
-                throw new IOException("Failed to create file: " + fileName + " for txn id(" + txnId + ")");
+                throw new IOException("Failed to create file: " + fileName + " for job id(" + jobId + ")");
             }
         } else {
-            throw new IOException("File: " + fileName + " for txn id(" + txnId + ") already exists");
+            throw new IOException("File: " + fileName + " for job id(" + jobId + ") already exists");
         }
         return jobRecoveryFile;
     }
@@ -502,11 +502,11 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
         return logDir + RECOVERY_FILES_DIR_NAME;
     }
 
-    private void freeJobsCachedEntities(long requestingTxnId) throws IOException {
+    private void freeJobsCachedEntities(int requestingJobId) throws IOException {
         if (jobId2WinnerEntitiesMap != null) {
-            for (Entry<Long, JobEntityCommits> jobEntityCommits : jobId2WinnerEntitiesMap.entrySet()) {
+            for (Entry<Integer, JobEntityCommits> jobEntityCommits : jobId2WinnerEntitiesMap.entrySet()) {
                 //if the job is not the requester, free its memory
-                if (jobEntityCommits.getKey() != requestingTxnId) {
+                if (jobEntityCommits.getKey() != requestingJobId) {
                     jobEntityCommits.getValue().spillToDiskAndfreeMemory();
                 }
             }
@@ -515,10 +515,10 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
 
     @Override
     public void rollbackTransaction(ITransactionContext txnContext) throws ACIDException {
-        long abortedTxnId = txnContext.getTxnId().getId();
+        int abortedJobId = txnContext.getJobId().getId();
         // Obtain the first/last log record LSNs written by the Job
         long firstLSN = txnContext.getFirstLSN();
-        /*
+        /**
          * The effect of any log record with LSN below minFirstLSN has already been written to disk and
          * will not be rolled back. Therefore, we will set the first LSN of the job to the maximum of
          * minFirstLSN and the job's first LSN.
@@ -530,35 +530,35 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
             throw new ACIDException(e);
         }
         long lastLSN = txnContext.getLastLSN();
-        if (LOGGER.isInfoEnabled()) {
+        if (LOGGER.isLoggable(Level.INFO)) {
             LOGGER.info("rollbacking transaction log records from " + firstLSN + " to " + lastLSN);
         }
         // check if the transaction actually wrote some logs.
         if (firstLSN == TransactionManagementConstants.LogManagerConstants.TERMINAL_LSN || firstLSN > lastLSN) {
-            if (LOGGER.isInfoEnabled()) {
-                LOGGER.info("no need to roll back as there were no operations by the txn " + txnContext.getTxnId());
+            if (LOGGER.isLoggable(Level.INFO)) {
+                LOGGER.info("no need to roll back as there were no operations by the job " + txnContext.getJobId());
             }
             return;
         }
 
         // While reading log records from firstLsn to lastLsn, collect uncommitted txn's Lsns
-        if (LOGGER.isInfoEnabled()) {
+        if (LOGGER.isLoggable(Level.INFO)) {
             LOGGER.info("collecting loser transaction's LSNs from " + firstLSN + " to " + lastLSN);
         }
 
-        Map<TxnEntityId, List<Long>> jobLoserEntity2LSNsMap = new HashMap<>();
-        TxnEntityId tempKeyTxnEntityId = new TxnEntityId(-1, -1, -1, null, -1, false);
+        Map<TxnId, List<Long>> jobLoserEntity2LSNsMap = new HashMap<>();
+        TxnId tempKeyTxnId = new TxnId(-1, -1, -1, null, -1, false);
         int updateLogCount = 0;
         int entityCommitLogCount = 0;
-        long logTxnId;
+        int logJobId = -1;
         long currentLSN = -1;
-        TxnEntityId loserEntity;
+        TxnId loserEntity = null;
         List<Long> undoLSNSet = null;
         //get active partitions on this node
-        Set<Integer> activePartitions = appCtx.getReplicaManager().getPartitions();
+        Set<Integer> activePartitions = localResourceRepository.getActivePartitions();
         ILogReader logReader = logMgr.getLogReader(false);
         try {
-            logReader.setPosition(firstLSN);
+            logReader.initializeScan(firstLSN);
             ILogRecord logRecord = null;
             while (currentLSN < lastLSN) {
                 logRecord = logReader.next();
@@ -571,20 +571,19 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
                         LOGGER.info(logRecord.getLogRecordForDisplay());
                     }
                 }
-                logTxnId = logRecord.getTxnId();
-                if (logTxnId != abortedTxnId) {
+                logJobId = logRecord.getJobId();
+                if (logJobId != abortedJobId) {
                     continue;
                 }
-                tempKeyTxnEntityId.setTxnId(logTxnId, logRecord.getDatasetId(), logRecord.getPKHashValue(),
+                tempKeyTxnId.setTxnId(logJobId, logRecord.getDatasetId(), logRecord.getPKHashValue(),
                         logRecord.getPKValue(), logRecord.getPKValueSize());
                 switch (logRecord.getLogType()) {
                     case LogType.UPDATE:
                         if (activePartitions.contains(logRecord.getResourcePartition())) {
-                            undoLSNSet = jobLoserEntity2LSNsMap.get(tempKeyTxnEntityId);
+                            undoLSNSet = jobLoserEntity2LSNsMap.get(tempKeyTxnId);
                             if (undoLSNSet == null) {
-                                loserEntity =
-                                        new TxnEntityId(logTxnId, logRecord.getDatasetId(), logRecord.getPKHashValue(),
-                                                logRecord.getPKValue(), logRecord.getPKValueSize(), true);
+                                loserEntity = new TxnId(logJobId, logRecord.getDatasetId(), logRecord.getPKHashValue(),
+                                        logRecord.getPKValue(), logRecord.getPKValueSize(), true);
                                 undoLSNSet = new LinkedList<>();
                                 jobLoserEntity2LSNsMap.put(loserEntity, undoLSNSet);
                             }
@@ -592,17 +591,17 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
                             updateLogCount++;
                             if (IS_DEBUG_MODE) {
                                 LOGGER.info(Thread.currentThread().getId() + "======> update[" + currentLSN + "]:"
-                                        + tempKeyTxnEntityId);
+                                        + tempKeyTxnId);
                             }
                         }
                         break;
                     case LogType.ENTITY_COMMIT:
                         if (activePartitions.contains(logRecord.getResourcePartition())) {
-                            jobLoserEntity2LSNsMap.remove(tempKeyTxnEntityId);
+                            jobLoserEntity2LSNsMap.remove(tempKeyTxnId);
                             entityCommitLogCount++;
                             if (IS_DEBUG_MODE) {
                                 LOGGER.info(Thread.currentThread().getId() + "======> entity_commit[" + currentLSN + "]"
-                                        + tempKeyTxnEntityId);
+                                        + tempKeyTxnId);
                             }
                         }
                         break;
@@ -621,18 +620,19 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
 
             if (currentLSN != lastLSN) {
                 throw new ACIDException("LastLSN mismatch: lastLSN(" + lastLSN + ") vs currentLSN(" + currentLSN
-                        + ") during abort( " + txnContext.getTxnId() + ")");
+                        + ") during abort( " + txnContext.getJobId() + ")");
             }
 
             //undo loserTxn's effect
             LOGGER.log(Level.INFO, "undoing loser transaction's effect");
 
-            final IDatasetLifecycleManager datasetLifecycleManager = appCtx.getDatasetLifecycleManager();
+            IDatasetLifecycleManager datasetLifecycleManager =
+                    txnSubsystem.getAsterixAppRuntimeContextProvider().getDatasetLifecycleManager();
             //TODO sort loser entities by smallest LSN to undo in one pass.
-            Iterator<Entry<TxnEntityId, List<Long>>> iter = jobLoserEntity2LSNsMap.entrySet().iterator();
+            Iterator<Entry<TxnId, List<Long>>> iter = jobLoserEntity2LSNsMap.entrySet().iterator();
             int undoCount = 0;
             while (iter.hasNext()) {
-                Map.Entry<TxnEntityId, List<Long>> loserEntity2LSNsMap = iter.next();
+                Map.Entry<TxnId, List<Long>> loserEntity2LSNsMap = iter.next();
                 undoLSNSet = loserEntity2LSNsMap.getValue();
                 // The step below is important since the upsert operations must be done in reverse order.
                 Collections.reverse(undoLSNSet);
@@ -641,7 +641,7 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
                     //read the corresponding log record to be undone.
                     logRecord = logReader.read(undoLSN);
                     if (logRecord == null) {
-                        throw new ACIDException("IllegalState exception during abort( " + txnContext.getTxnId() + ")");
+                        throw new ACIDException("IllegalState exception during abort( " + txnContext.getJobId() + ")");
                     }
                     if (IS_DEBUG_MODE) {
                         LOGGER.info(logRecord.getLogRecordForDisplay());
@@ -651,7 +651,7 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
                 }
             }
 
-            if (LOGGER.isInfoEnabled()) {
+            if (LOGGER.isLoggable(Level.INFO)) {
                 LOGGER.info("undone loser transaction's effect");
                 LOGGER.info("[RecoveryManager's rollback log count] update/entityCommit/undo:" + updateLogCount + "/"
                         + entityCommitLogCount + "/" + undoCount);
@@ -668,6 +668,8 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
 
     @Override
     public void stop(boolean dumpState, OutputStream os) throws IOException {
+        // Shutdown checkpoint
+        checkpointManager.doSharpCheckpoint();
     }
 
     @Override
@@ -679,48 +681,34 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
         try {
             ILSMIndex index =
                     (ILSMIndex) datasetLifecycleManager.getIndex(logRecord.getDatasetId(), logRecord.getResourceId());
-            ILSMIndexAccessor indexAccessor = index.createAccessor(NoOpIndexAccessParameters.INSTANCE);
-            try {
-                if (logRecord.getNewOp() == AbstractIndexModificationOperationCallback.INSERT_BYTE) {
-                    indexAccessor.forceDelete(logRecord.getNewValue());
-                } else if (logRecord.getNewOp() == AbstractIndexModificationOperationCallback.DELETE_BYTE) {
-                    indexAccessor.forceInsert(logRecord.getOldValue());
-                } else if (logRecord.getNewOp() == AbstractIndexModificationOperationCallback.UPSERT_BYTE) {
-                    // undo, upsert the old value if found, otherwise, physical delete
-                    undoUpsert(indexAccessor, logRecord);
+            ILSMIndexAccessor indexAccessor =
+                    index.createAccessor(NoOpOperationCallback.INSTANCE, NoOpOperationCallback.INSTANCE);
+            if (logRecord.getNewOp() == AbstractIndexModificationOperationCallback.INSERT_BYTE) {
+                indexAccessor.forceDelete(logRecord.getNewValue());
+            } else if (logRecord.getNewOp() == AbstractIndexModificationOperationCallback.DELETE_BYTE) {
+                indexAccessor.forceInsert(logRecord.getOldValue());
+            } else if (logRecord.getNewOp() == AbstractIndexModificationOperationCallback.UPSERT_BYTE) {
+                // undo, upsert the old value if found, otherwise, physical delete
+                if (logRecord.getOldValue() == null) {
+                    indexAccessor.forcePhysicalDelete(logRecord.getNewValue());
                 } else {
-                    throw new IllegalStateException("Unsupported OperationType: " + logRecord.getNewOp());
+                    indexAccessor.forceUpsert(logRecord.getOldValue());
                 }
-            } finally {
-                indexAccessor.destroy();
+            } else {
+                throw new IllegalStateException("Unsupported OperationType: " + logRecord.getNewOp());
             }
         } catch (Exception e) {
             throw new IllegalStateException("Failed to undo", e);
         }
     }
 
-    private static void undoUpsert(ILSMIndexAccessor indexAccessor, ILogRecord logRecord) throws HyracksDataException {
-        if (logRecord.getOldValue() == null) {
-            try {
-                indexAccessor.forcePhysicalDelete(logRecord.getNewValue());
-            } catch (HyracksDataException hde) {
-                // Since we're undoing according the write-ahead log, the actual upserting tuple
-                // might not have been written to memory yet.
-                if (hde.getErrorCode() != ErrorCode.UPDATE_OR_DELETE_NON_EXISTENT_KEY) {
-                    throw hde;
-                }
-            }
-        } else {
-            indexAccessor.forceUpsert(logRecord.getOldValue());
-        }
-    }
-
     private static void redo(ILogRecord logRecord, IDatasetLifecycleManager datasetLifecycleManager) {
         try {
             int datasetId = logRecord.getDatasetId();
             long resourceId = logRecord.getResourceId();
             ILSMIndex index = (ILSMIndex) datasetLifecycleManager.getIndex(datasetId, resourceId);
-            ILSMIndexAccessor indexAccessor = index.createAccessor(NoOpIndexAccessParameters.INSTANCE);
+            ILSMIndexAccessor indexAccessor =
+                    index.createAccessor(NoOpOperationCallback.INSTANCE, NoOpOperationCallback.INSTANCE);
             if (logRecord.getNewOp() == AbstractIndexModificationOperationCallback.INSERT_BYTE) {
                 indexAccessor.forceInsert(logRecord.getNewValue());
             } else if (logRecord.getNewOp() == AbstractIndexModificationOperationCallback.DELETE_BYTE) {
@@ -738,25 +726,25 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
 
     private class JobEntityCommits {
         private static final String PARTITION_FILE_NAME_SEPARATOR = "_";
-        private final long txnId;
-        private final Set<TxnEntityId> cachedEntityCommitTxns = new HashSet<>();
+        private final int jobId;
+        private final Set<TxnId> cachedEntityCommitTxns = new HashSet<>();
         private final List<File> jobEntitCommitOnDiskPartitionsFiles = new ArrayList<>();
         //a flag indicating whether all the the commits for this jobs have been added.
         private boolean preparedForSearch = false;
-        private TxnEntityId winnerEntity = null;
+        private TxnId winnerEntity = null;
         private int currentPartitionSize = 0;
         private long partitionMaxLSN = 0;
         private String currentPartitonName;
 
-        public JobEntityCommits(long txnId) {
-            this.txnId = txnId;
+        public JobEntityCommits(int jobId) {
+            this.jobId = jobId;
         }
 
         public void add(ILogRecord logRecord) throws IOException {
             if (preparedForSearch) {
                 throw new IOException("Cannot add new entity commits after preparing for search.");
             }
-            winnerEntity = new TxnEntityId(logRecord.getTxnId(), logRecord.getDatasetId(), logRecord.getPKHashValue(),
+            winnerEntity = new TxnId(logRecord.getJobId(), logRecord.getDatasetId(), logRecord.getPKHashValue(),
                     logRecord.getPKValue(), logRecord.getPKValueSize(), true);
             cachedEntityCommitTxns.add(winnerEntity);
             //since log file is read sequentially, LSNs are always increasing
@@ -797,15 +785,15 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
             preparedForSearch = true;
         }
 
-        public boolean containsEntityCommitForTxnId(long logLSN, TxnEntityId txnEntityId) throws IOException {
+        public boolean containsEntityCommitForTxnId(long logLSN, TxnId txnId) throws IOException {
             //if we don't have any partitions on disk, search only from memory
             if (jobEntitCommitOnDiskPartitionsFiles.size() == 0) {
-                return cachedEntityCommitTxns.contains(txnEntityId);
+                return cachedEntityCommitTxns.contains(txnId);
             } else {
                 //get candidate partitions from disk
                 ArrayList<File> candidatePartitions = getCandidiatePartitions(logLSN);
                 for (File partition : candidatePartitions) {
-                    if (serachPartition(partition, txnEntityId)) {
+                    if (serachPartition(partition, txnId)) {
                         return true;
                     }
                 }
@@ -839,17 +827,17 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
             jobEntitCommitOnDiskPartitionsFiles.clear();
         }
 
-        private boolean serachPartition(File partition, TxnEntityId txnEntityId) throws IOException {
+        private boolean serachPartition(File partition, TxnId txnId) throws IOException {
             //load partition from disk if it is not  already in memory
             if (!partition.getName().equals(currentPartitonName)) {
                 loadPartitionToMemory(partition, cachedEntityCommitTxns);
                 currentPartitonName = partition.getName();
             }
-            return cachedEntityCommitTxns.contains(txnEntityId);
+            return cachedEntityCommitTxns.contains(txnId);
         }
 
         private String getPartitionName(long maxLSN) {
-            return txnId + PARTITION_FILE_NAME_SEPARATOR + maxLSN;
+            return jobId + PARTITION_FILE_NAME_SEPARATOR + maxLSN;
         }
 
         private long getPartitionMaxLSNFromName(String partitionName) {
@@ -860,18 +848,18 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
             //if we don't have enough memory to allocate for this partition,
             // we will ask recovery manager to free memory
             if (needToFreeMemory()) {
-                freeJobsCachedEntities(txnId);
+                freeJobsCachedEntities(jobId);
             }
             //allocate a buffer that can hold the current partition
             ByteBuffer buffer = ByteBuffer.allocate(currentPartitionSize);
-            for (Iterator<TxnEntityId> iterator = cachedEntityCommitTxns.iterator(); iterator.hasNext();) {
-                TxnEntityId txnEntityId = iterator.next();
+            for (Iterator<TxnId> iterator = cachedEntityCommitTxns.iterator(); iterator.hasNext();) {
+                TxnId txnId = iterator.next();
                 //serialize the object and remove it from memory
-                txnEntityId.serialize(buffer);
+                txnId.serialize(buffer);
                 iterator.remove();
             }
             //name partition file based on job id and max lsn
-            File partitionFile = createJobRecoveryFile(txnId, getPartitionName(partitionMaxLSN));
+            File partitionFile = createJobRecoveryFile(jobId, getPartitionName(partitionMaxLSN));
             //write file to disk
             try (FileOutputStream fileOutputstream = new FileOutputStream(partitionFile, false);
                     FileChannel fileChannel = fileOutputstream.getChannel()) {
@@ -883,11 +871,11 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
             jobEntitCommitOnDiskPartitionsFiles.add(partitionFile);
         }
 
-        private void loadPartitionToMemory(File partition, Set<TxnEntityId> partitionTxn) throws IOException {
+        private void loadPartitionToMemory(File partition, Set<TxnId> partitionTxn) throws IOException {
             partitionTxn.clear();
             //if we don't have enough memory to a load partition, we will ask recovery manager to free memory
             if (needToFreeMemory()) {
-                freeJobsCachedEntities(txnId);
+                freeJobsCachedEntities(jobId);
             }
             ByteBuffer buffer = ByteBuffer.allocateDirect((int) partition.length());
             //load partition to memory
@@ -898,9 +886,9 @@ public class RecoveryManager implements IRecoveryManager, ILifeCycleComponent {
                 }
             }
             buffer.flip();
-            TxnEntityId temp;
+            TxnId temp = null;
             while (buffer.remaining() != 0) {
-                temp = TxnEntityId.deserialize(buffer);
+                temp = TxnId.deserialize(buffer);
                 partitionTxn.add(temp);
             }
         }