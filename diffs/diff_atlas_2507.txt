diff --git a/addons/hive-bridge/src/test/java/org/apache/atlas/hive/hook/HiveHookIT.java b/addons/hive-bridge/src/test/java/org/apache/atlas/hive/hook/HiveHookIT.java
index 8ca47d960..f5904d607 100755
--- a/addons/hive-bridge/src/test/java/org/apache/atlas/hive/hook/HiveHookIT.java
+++ b/addons/hive-bridge/src/test/java/org/apache/atlas/hive/hook/HiveHookIT.java
@@ -28,6 +28,7 @@ import org.apache.atlas.fs.model.FSDataTypes;
 import org.apache.atlas.hive.bridge.HiveMetaStoreBridge;
 import org.apache.atlas.hive.model.HiveDataModelGenerator;
 import org.apache.atlas.hive.model.HiveDataTypes;
+import org.apache.atlas.hive.rewrite.HiveASTRewriter;
 import org.apache.atlas.typesystem.Referenceable;
 import org.apache.atlas.typesystem.Struct;
 import org.apache.atlas.typesystem.persistence.Id;
@@ -43,10 +44,7 @@ import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.CommandNeedRetryException;
 import org.apache.hadoop.hive.ql.Driver;
 import org.apache.hadoop.hive.ql.hooks.Entity;
-import org.apache.hadoop.hive.ql.hooks.ReadEntity;
-import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.security.UserGroupInformation;
@@ -62,22 +60,12 @@ import java.text.ParseException;
 import java.util.ArrayList;
 import java.util.Date;
 import java.util.HashMap;
-import java.util.Iterator;
-import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.SortedSet;
-import java.util.TreeMap;
-import java.util.TreeSet;
-
-import static org.apache.atlas.AtlasClient.NAME;
-import static org.apache.atlas.hive.hook.HiveHook.entityComparator;
-import static org.apache.atlas.hive.hook.HiveHook.getProcessQualifiedName;
+
 import static org.apache.atlas.hive.hook.HiveHook.lower;
-import static org.apache.atlas.hive.hook.HiveHook.IO_SEP;
-import static org.apache.atlas.hive.hook.HiveHook.SEP;
+import static org.apache.atlas.hive.hook.HiveHook.normalize;
+import static org.apache.atlas.hive.model.HiveDataModelGenerator.NAME;
 import static org.testng.Assert.assertEquals;
 import static org.testng.Assert.assertNotNull;
 import static org.testng.Assert.assertTrue;
@@ -89,8 +77,6 @@ public class HiveHookIT {
     private static final String DGI_URL = "http://localhost:21000/";
     private static final String CLUSTER_NAME = "test";
     public static final String DEFAULT_DB = "default";
-    
-    private static final String PART_FILE = "2015-01-01";
     private Driver driver;
     private AtlasClient atlasClient;
     private HiveMetaStoreBridge hiveMetaStoreBridge;
@@ -109,7 +95,6 @@ public class HiveHookIT {
         driver = new Driver(conf);
         ss = new SessionState(conf);
         ss = SessionState.start(ss);
-
         SessionState.setCurrentSessionState(ss);
 
         Configuration configuration = ApplicationProperties.get();
@@ -207,18 +192,13 @@ public class HiveHookIT {
         Assert.assertNotNull(colEntity.get(HiveDataModelGenerator.TABLE));
         Assert.assertEquals(((Id) colEntity.get(HiveDataModelGenerator.TABLE))._getId(), tableId);
 
-        //assert that column.owner = table.owner
-        Referenceable tableRef = atlasClient.getEntity(tableId);
-        assertEquals(tableRef.get(AtlasClient.OWNER), colEntity.get(AtlasClient.OWNER));
-
-        //create table where db is not registered
         tableName = createTable();
         tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
-        tableRef = atlasClient.getEntity(tableId);
+        Referenceable tableRef = atlasClient.getEntity(tableId);
         Assert.assertEquals(tableRef.get(HiveDataModelGenerator.TABLE_TYPE_ATTR), TableType.MANAGED_TABLE.name());
         Assert.assertEquals(tableRef.get(HiveDataModelGenerator.COMMENT), "table comment");
         String entityName = HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, DEFAULT_DB, tableName);
-        Assert.assertEquals(tableRef.get(AtlasClient.NAME), tableName.toLowerCase());
+        Assert.assertEquals(tableRef.get(HiveDataModelGenerator.NAME), tableName.toLowerCase());
         Assert.assertEquals(tableRef.get(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME), entityName);
 
         Table t = hiveMetaStoreBridge.hiveClient.getTable(DEFAULT_DB, tableName);
@@ -271,54 +251,19 @@ public class HiveHookIT {
         validateHDFSPaths(processReference, INPUTS, pFile);
     }
 
-    private Set<ReadEntity> getInputs(String inputName, Entity.Type entityType) {
-        final ReadEntity entity = new ReadEntity();
-
-        if ( Entity.Type.DFS_DIR.equals(entityType)) {
-            entity.setName(lower(new Path(inputName).toString()));
-            entity.setTyp(Entity.Type.DFS_DIR);
-        } else {
-            entity.setName(getQualifiedTblName(inputName));
-            entity.setTyp(entityType);
-        }
-
-        return new LinkedHashSet<ReadEntity>() {{ add(entity); }};
+    private void validateOutputTables(Referenceable processReference, String... expectedTableNames) throws Exception {
+       validateTables(processReference, OUTPUTS, expectedTableNames);
     }
 
-    private Set<WriteEntity> getOutputs(String inputName, Entity.Type entityType) {
-        final WriteEntity entity = new WriteEntity();
-
-        if ( Entity.Type.DFS_DIR.equals(entityType) || Entity.Type.LOCAL_DIR.equals(entityType)) {
-            entity.setName(lower(new Path(inputName).toString()));
-            entity.setTyp(entityType);
-        } else {
-            entity.setName(getQualifiedTblName(inputName));
-            entity.setTyp(entityType);
-        }
-
-        return new LinkedHashSet<WriteEntity>() {{ add(entity); }};
-    }
-
-    private void validateOutputTables(Referenceable processReference, Set<WriteEntity> expectedTables) throws Exception {
-       validateTables(processReference, OUTPUTS, expectedTables);
+    private void validateInputTables(Referenceable processReference, String... expectedTableNames) throws Exception {
+        validateTables(processReference, INPUTS, expectedTableNames);
     }
 
-    private void validateInputTables(Referenceable processReference, Set<ReadEntity> expectedTables) throws Exception {
-        validateTables(processReference, INPUTS, expectedTables);
-    }
-
-    private void validateTables(Referenceable processReference, String attrName, Set<? extends Entity> expectedTables) throws Exception {
+    private void validateTables(Referenceable processReference, String attrName, String... expectedTableNames) throws Exception {
         List<Id> tableRef = (List<Id>) processReference.get(attrName);
-
-        Iterator<? extends Entity> iterator = expectedTables.iterator();
-        for(int i = 0; i < expectedTables.size(); i++) {
-            Entity hiveEntity = iterator.next();
-            if (Entity.Type.TABLE.equals(hiveEntity.getType()) ||
-                Entity.Type.DFS_DIR.equals(hiveEntity.getType())) {
-                Referenceable entity = atlasClient.getEntity(tableRef.get(i)._getId());
-                LOG.debug("Validating output {} {} ", i, entity);
-                Assert.assertEquals(entity.get(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME), hiveEntity.getName());
-            }
+        for(int i = 0; i < expectedTableNames.length; i++) {
+            Referenceable entity = atlasClient.getEntity(tableRef.get(i)._getId());
+            Assert.assertEquals(entity.get(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME), expectedTableNames[i]);
         }
     }
 
@@ -351,22 +296,10 @@ public class HiveHookIT {
         String query = "create table " + ctasTableName + " as select * from " + tableName;
         runCommand(query);
 
-        final Set<ReadEntity> readEntities = getInputs(tableName, Entity.Type.TABLE);
-        final Set<WriteEntity> writeEntities = getOutputs(ctasTableName, Entity.Type.TABLE);
-
-        assertProcessIsRegistered(constructEvent(query, HiveOperation.CREATETABLE_AS_SELECT, readEntities, writeEntities));
+        assertProcessIsRegistered(query);
         assertTableIsRegistered(DEFAULT_DB, ctasTableName);
     }
 
-    private HiveHook.HiveEventContext constructEvent(String query, HiveOperation op, Set<ReadEntity> inputs, Set<WriteEntity> outputs) {
-        HiveHook.HiveEventContext event = new HiveHook.HiveEventContext();
-        event.setQueryStr(query);
-        event.setOperation(op);
-        event.setInputs(inputs);
-        event.setOutputs(outputs);
-        return event;
-    }
-
     @Test
     public void testDropAndRecreateCTASOutput() throws Exception {
         String tableName = createTable();
@@ -375,12 +308,7 @@ public class HiveHookIT {
         runCommand(query);
 
         assertTableIsRegistered(DEFAULT_DB, ctasTableName);
-
-        Set<ReadEntity> inputs = getInputs(tableName, Entity.Type.TABLE);
-        Set<WriteEntity> outputs =  getOutputs(ctasTableName, Entity.Type.TABLE);
-
-        final HiveHook.HiveEventContext hiveEventContext = constructEvent(query, HiveOperation.CREATETABLE_AS_SELECT, inputs, outputs);
-        String processId = assertProcessIsRegistered(hiveEventContext);
+        String processId = assertProcessIsRegistered(query);
 
         final String drpquery = String.format("drop table %s ", ctasTableName);
         runCommand(drpquery);
@@ -389,14 +317,16 @@ public class HiveHookIT {
         //Fix after ATLAS-876
         runCommand(query);
         assertTableIsRegistered(DEFAULT_DB, ctasTableName);
-        String process2Id = assertProcessIsRegistered(hiveEventContext, inputs, outputs);
+        String process2Id = assertProcessIsRegistered(query);
 
         Assert.assertEquals(process2Id, processId);
 
         Referenceable processRef = atlasClient.getEntity(processId);
+        String tblQlfdname = getQualifiedTblName(tableName);
+        String ctasQlfdname = getQualifiedTblName(ctasTableName);
 
-        outputs.add(outputs.iterator().next());
-        validateOutputTables(processRef, outputs);
+        validateInputTables(processRef, tblQlfdname);
+        validateOutputTables(processRef, ctasQlfdname, ctasQlfdname);
     }
 
     @Test
@@ -406,7 +336,7 @@ public class HiveHookIT {
         String query = "create view " + viewName + " as select * from " + tableName;
         runCommand(query);
 
-        assertProcessIsRegistered(constructEvent(query, HiveOperation.CREATEVIEW, getInputs(tableName, Entity.Type.TABLE), getOutputs(viewName, Entity.Type.TABLE)));
+        assertProcessIsRegistered(query);
         assertTableIsRegistered(DEFAULT_DB, viewName);
     }
 
@@ -420,7 +350,7 @@ public class HiveHookIT {
         runCommand(query);
 
         String table1Id = assertTableIsRegistered(DEFAULT_DB, table1Name);
-        assertProcessIsRegistered(constructEvent(query, HiveOperation.CREATEVIEW, getInputs(table1Name, Entity.Type.TABLE), getOutputs(viewName, Entity.Type.TABLE)));
+        assertProcessIsRegistered(query);
         String viewId = assertTableIsRegistered(DEFAULT_DB, viewName);
 
         //Check lineage which includes table1
@@ -436,7 +366,7 @@ public class HiveHookIT {
         runCommand(query);
 
         //Check if alter view process is reqistered
-        assertProcessIsRegistered(constructEvent(query, HiveOperation.CREATEVIEW, getInputs(table2Name, Entity.Type.TABLE), getOutputs(viewName, Entity.Type.TABLE)));
+        assertProcessIsRegistered(query);
         String table2Id = assertTableIsRegistered(DEFAULT_DB, table2Name);
         Assert.assertEquals(assertTableIsRegistered(DEFAULT_DB, viewName), viewId);
 
@@ -473,7 +403,7 @@ public class HiveHookIT {
         String query = "load data local inpath 'file://" + loadFile + "' into table " + tableName;
         runCommand(query);
 
-        assertProcessIsRegistered(constructEvent(query, HiveOperation.LOAD, null, getOutputs(tableName, Entity.Type.TABLE)));
+        assertProcessIsRegistered(query, null, getQualifiedTblName(tableName));
     }
 
     @Test
@@ -481,115 +411,78 @@ public class HiveHookIT {
         String tableName = createTable(true);
 
         String loadFile = file("load");
-        String query = "load data local inpath 'file://" + loadFile + "' into table " + tableName +  " partition(dt = '"+ PART_FILE + "')";
+        String query = "load data local inpath 'file://" + loadFile + "' into table " + tableName +  " partition(dt = '2015-01-01')";
         runCommand(query);
 
-        assertProcessIsRegistered(constructEvent(query, HiveOperation.LOAD, null, getOutputs(tableName, Entity.Type.TABLE)));
+        validateProcess(query, null, getQualifiedTblName(tableName));
     }
 
     @Test
-    public void testLoadDFSPathPartitioned() throws Exception {
+    public void testLoadDFSPath() throws Exception {
         String tableName = createTable(true, true, false);
 
-        assertTableIsRegistered(DEFAULT_DB, tableName);
+        String tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
 
-        final String loadFile = createTestDFSFile("loadDFSFile");
-        String query = "load data inpath '" + loadFile + "' into table " + tableName + " partition(dt = '"+ PART_FILE + "')";
+        String loadFile = createTestDFSFile("loadDFSFile");
+        final String testPathNormed = lower(new Path(loadFile).toString());
+        String query = "load data inpath '" + loadFile + "' into table " + tableName + " partition(dt = '2015-01-01')";
         runCommand(query);
 
-        final Set<WriteEntity> outputs = getOutputs(tableName, Entity.Type.TABLE);
-        final Set<ReadEntity> inputs = getInputs(loadFile, Entity.Type.DFS_DIR);
-
-        final Set<WriteEntity> partitionOps = new LinkedHashSet<>(outputs);
-        partitionOps.addAll(getOutputs(DEFAULT_DB + "@" + tableName + "@dt=" + PART_FILE, Entity.Type.PARTITION));
+        final String tblQlfdName = getQualifiedTblName(tableName);
+        Referenceable processReference = validateProcess(query, testPathNormed, tblQlfdName);
 
-        Referenceable processReference = validateProcess(constructEvent(query, HiveOperation.LOAD, inputs, partitionOps), inputs, outputs);
         validateHDFSPaths(processReference, INPUTS, loadFile);
-        validateOutputTables(processReference, outputs);
-
-        final String loadFile2 = createTestDFSFile("loadDFSFile1");
-        query = "load data inpath '" + loadFile2 + "' into table " + tableName + " partition(dt = '"+ PART_FILE + "')";
-        runCommand(query);
-
-        Set<ReadEntity> process2Inputs = getInputs(loadFile2, Entity.Type.DFS_DIR);
-        Set<ReadEntity> expectedInputs = new LinkedHashSet<>();
-        expectedInputs.addAll(process2Inputs);
-        expectedInputs.addAll(inputs);
-
-        validateProcess(constructEvent(query, HiveOperation.LOAD, expectedInputs, partitionOps), expectedInputs, outputs);
 
+        validateOutputTables(processReference, tblQlfdName);
     }
 
     private String getQualifiedTblName(String inputTable) {
         String inputtblQlfdName = inputTable;
 
-        if (inputTable != null && !inputTable.contains("@")) {
+        if (inputTable != null && !inputTable.contains(".")) {
             inputtblQlfdName = HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, DEFAULT_DB, inputTable);
         }
         return inputtblQlfdName;
     }
 
-    private Referenceable validateProcess(HiveHook.HiveEventContext event, Set<ReadEntity> inputTables, Set<WriteEntity> outputTables) throws Exception {
-        String processId = assertProcessIsRegistered(event, inputTables, outputTables);
+    private Referenceable validateProcess(String query, String inputTable, String... outputTables) throws Exception {
+        String processId = assertProcessIsRegistered(query, inputTable, outputTables);
         Referenceable process = atlasClient.getEntity(processId);
-        if (inputTables == null) {
+        if (inputTable == null) {
             Assert.assertNull(process.get(INPUTS));
         } else {
-            Assert.assertEquals(((List<Referenceable>) process.get(INPUTS)).size(), inputTables.size());
-            validateInputTables(process, inputTables);
+            Assert.assertEquals(((List<Referenceable>) process.get(INPUTS)).size(), 1);
+            validateInputTables(process, inputTable);
         }
 
         if (outputTables == null) {
             Assert.assertNull(process.get(OUTPUTS));
         } else {
-            Assert.assertEquals(((List<Id>) process.get(OUTPUTS)).size(), outputTables.size());
+            Assert.assertEquals(((List<Id>) process.get(OUTPUTS)).size(), 1);
             validateOutputTables(process, outputTables);
         }
 
         return process;
     }
 
-    private Referenceable validateProcess(HiveHook.HiveEventContext event) throws Exception {
-       return validateProcess(event, event.getInputs(), event.getOutputs());
-    }
-
     @Test
     public void testInsertIntoTable() throws Exception {
-        String inputTable1Name = createTable();
-        String inputTable2Name = createTable();
+        String tableName = createTable();
         String insertTableName = createTable();
-        assertTableIsRegistered(DEFAULT_DB, inputTable1Name);
-        assertTableIsRegistered(DEFAULT_DB, insertTableName);
-
-        String query = "insert into " + insertTableName + " select t1.id, t1.name from " + inputTable2Name + " as t2, " + inputTable1Name + " as t1 where t1.id=t2.id";
+        String query =
+                "insert into " + insertTableName + " select id, name from " + tableName;
 
         runCommand(query);
-        final Set<ReadEntity> inputs = getInputs(inputTable1Name, Entity.Type.TABLE);
-        inputs.addAll(getInputs(inputTable2Name, Entity.Type.TABLE));
-
-        Set<WriteEntity> outputs = getOutputs(insertTableName, Entity.Type.TABLE);
-        (outputs.iterator().next()).setWriteType(WriteEntity.WriteType.INSERT);
 
-        HiveHook.HiveEventContext event = constructEvent(query, HiveOperation.QUERY, inputs, outputs);
+        String inputTableId = assertTableIsRegistered(DEFAULT_DB, tableName);
+        String opTableId = assertTableIsRegistered(DEFAULT_DB, insertTableName);
 
-        Set<ReadEntity> expectedInputs = new TreeSet<ReadEntity>(entityComparator) {{
-            addAll(inputs);
-        }};
-        assertTableIsRegistered(DEFAULT_DB, insertTableName);
-        Referenceable processRef1 = validateProcess(event, expectedInputs, outputs);
-
-        //Test sorting of tbl names
-        SortedSet<String> sortedTblNames = new TreeSet<>();
-        sortedTblNames.add(getQualifiedTblName(inputTable1Name));
-        sortedTblNames.add(getQualifiedTblName(inputTable2Name));
-
-        //Verify sorted orer of inputs in qualified name
-        Assert.assertEquals(Joiner.on(SEP).join("QUERY", sortedTblNames.first(), sortedTblNames.last()) + IO_SEP + SEP + Joiner.on(SEP).join(WriteEntity.WriteType.INSERT.name(), getQualifiedTblName(insertTableName))
-            , processRef1.get(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME));
+        Referenceable processRef1 = validateProcess(query, getQualifiedTblName(tableName), getQualifiedTblName(insertTableName));
 
         //Rerun same query. Should result in same process
-        runCommandWithDelay(query, 1000);
-        Referenceable processRef2 = validateProcess(event, expectedInputs, outputs);
+        runCommand(query);
+
+        Referenceable processRef2 = validateProcess(query, getQualifiedTblName(tableName), getQualifiedTblName(insertTableName));
         Assert.assertEquals(processRef1.getId()._getId(), processRef2.getId()._getId());
 
     }
@@ -602,7 +495,7 @@ public class HiveHookIT {
             "insert overwrite LOCAL DIRECTORY '" + randomLocalPath.getAbsolutePath() + "' select id, name from " + tableName;
 
         runCommand(query);
-        validateProcess(constructEvent(query, HiveOperation.QUERY, getInputs(tableName, Entity.Type.TABLE), null));
+        validateProcess(query, getQualifiedTblName(tableName), null);
 
         assertTableIsRegistered(DEFAULT_DB, tableName);
     }
@@ -611,86 +504,69 @@ public class HiveHookIT {
     public void testUpdateProcess() throws Exception {
         String tableName = createTable();
         String pFile1 = createTestDFSPath("somedfspath1");
+        String testPathNormed = lower(new Path(pFile1).toString());
         String query =
             "insert overwrite DIRECTORY '" + pFile1  + "' select id, name from " + tableName;
 
         runCommand(query);
-
-        Set<ReadEntity> inputs = getInputs(tableName, Entity.Type.TABLE);
-        final Set<WriteEntity> outputs = getOutputs(pFile1, Entity.Type.DFS_DIR);
-        ((WriteEntity)outputs.iterator().next()).setWriteType(WriteEntity.WriteType.PATH_WRITE);
-
-        final HiveHook.HiveEventContext hiveEventContext = constructEvent(query, HiveOperation.QUERY, inputs, outputs);
-        Referenceable processReference = validateProcess(hiveEventContext);
+        String tblQlfdname = getQualifiedTblName(tableName);
+        Referenceable processReference = validateProcess(query, tblQlfdname, testPathNormed);
         validateHDFSPaths(processReference, OUTPUTS, pFile1);
 
         String tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
-        validateInputTables(processReference, inputs);
+
+        validateInputTables(processReference, tblQlfdname);
 
         //Rerun same query with same HDFS path
-        runCommandWithDelay(query, 1000);
-        assertTableIsRegistered(DEFAULT_DB, tableName);
-        Referenceable process2Reference = validateProcess(hiveEventContext);
+
+        runCommand(query);
+        Referenceable process2Reference = validateProcess(query, tblQlfdname, testPathNormed);
         validateHDFSPaths(process2Reference, OUTPUTS, pFile1);
 
         Assert.assertEquals(process2Reference.getId()._getId(), processReference.getId()._getId());
 
-        //Rerun same query with a new HDFS path. Will result in same process since HDFS paths is not part of qualified name for QUERY operations
-        final String pFile2 = createTestDFSPath("somedfspath2");
+        //Rerun same query with a new HDFS path. Should create a new process
+        String pFile2 = createTestDFSPath("somedfspath2");
         query = "insert overwrite DIRECTORY '" + pFile2  + "' select id, name from " + tableName;
-        runCommandWithDelay(query, 1000);
-        assertTableIsRegistered(DEFAULT_DB, tableName);
-        Set<WriteEntity> p3Outputs = new LinkedHashSet<WriteEntity>() {{
-            addAll(getOutputs(pFile2, Entity.Type.DFS_DIR));
-            addAll(outputs);
-        }};
+        final String testPathNormed2 = lower(new Path(pFile2).toString());
+        runCommand(query);
 
-        Referenceable process3Reference = validateProcess(constructEvent(query,  HiveOperation.QUERY, inputs, p3Outputs));
+        Referenceable process3Reference = validateProcess(query, tblQlfdname, testPathNormed2);
         validateHDFSPaths(process3Reference, OUTPUTS, pFile2);
 
-        Assert.assertEquals(process3Reference.getId()._getId(), processReference.getId()._getId());
+        Assert.assertNotEquals(process3Reference.getId()._getId(), processReference.getId()._getId());
     }
 
     @Test
-    public void testInsertIntoDFSDirPartitioned() throws Exception {
-
-        //Test with partitioned table
-        String tableName = createTable(true);
+    public void testInsertIntoDFSDir() throws Exception {
+        String tableName = createTable();
         String pFile1 = createTestDFSPath("somedfspath1");
+        String testPathNormed = lower(new Path(pFile1).toString());
         String query =
-            "insert overwrite DIRECTORY '" + pFile1  + "' select id, name from " + tableName + " where dt = '" + PART_FILE + "'";
+            "insert overwrite DIRECTORY '" + pFile1  + "' select id, name from " + tableName;
 
         runCommand(query);
+        String tblQlfdname = getQualifiedTblName(tableName);
+        Referenceable processReference = validateProcess(query, tblQlfdname, testPathNormed);
+        validateHDFSPaths(processReference, OUTPUTS, pFile1);
 
-        Set<ReadEntity> inputs = getInputs(tableName, Entity.Type.TABLE);
-        final Set<WriteEntity> outputs = getOutputs(pFile1, Entity.Type.DFS_DIR);
-        ((WriteEntity)outputs.iterator().next()).setWriteType(WriteEntity.WriteType.PATH_WRITE);
-
-        final Set<ReadEntity> partitionIps = new LinkedHashSet<>(inputs);
-        partitionIps.addAll(getInputs(DEFAULT_DB + "@" + tableName + "@dt='" + PART_FILE + "'", Entity.Type.PARTITION));
+        String tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
 
-        Referenceable processReference = validateProcess(constructEvent(query,  HiveOperation.QUERY, partitionIps, outputs), inputs, outputs);
+        validateInputTables(processReference, tblQlfdname);
 
-        //Rerun same query with different HDFS path. Should not create another process and should update it.
+        //Rerun same query with different HDFS path
 
-        final String pFile2 = createTestDFSPath("somedfspath2");
+        String pFile2 = createTestDFSPath("somedfspath2");
+        testPathNormed = lower(new Path(pFile2).toString());
         query =
-            "insert overwrite DIRECTORY '" + pFile2  + "' select id, name from " + tableName + " where dt = '" + PART_FILE + "'";
+            "insert overwrite DIRECTORY '" + pFile2  + "' select id, name from " + tableName;
 
         runCommand(query);
-
-        final Set<WriteEntity> pFile2Outputs = getOutputs(pFile2, Entity.Type.DFS_DIR);
-        ((WriteEntity)pFile2Outputs.iterator().next()).setWriteType(WriteEntity.WriteType.PATH_WRITE);
-        //Now the process has 2 paths - one older with deleted reference to partition and another with the the latest partition
-        Set<WriteEntity> p2Outputs = new LinkedHashSet<WriteEntity>() {{
-            addAll(pFile2Outputs);
-            addAll(outputs);
-        }};
-
-        Referenceable process2Reference = validateProcess(constructEvent(query, HiveOperation.QUERY, partitionIps, pFile2Outputs), inputs, p2Outputs);
+        tblQlfdname = getQualifiedTblName(tableName);
+        Referenceable process2Reference = validateProcess(query, tblQlfdname, testPathNormed);
         validateHDFSPaths(process2Reference, OUTPUTS, pFile2);
 
-        Assert.assertEquals(process2Reference.getId()._getId(), processReference.getId()._getId());
+        Assert.assertNotEquals(process2Reference.getId()._getId(), processReference.getId()._getId());
     }
 
     @Test
@@ -704,13 +580,7 @@ public class HiveHookIT {
             "insert into " + insertTableName + " select id, name from " + tableName;
 
         runCommand(query);
-
-        Set<ReadEntity> inputs = getInputs(tableName, Entity.Type.TABLE);
-        Set<WriteEntity> outputs = getOutputs(insertTableName, Entity.Type.TABLE);
-        outputs.iterator().next().setName(getQualifiedTblName(insertTableName + HiveMetaStoreBridge.TEMP_TABLE_PREFIX + SessionState.get().getSessionId()));
-        ((WriteEntity)outputs.iterator().next()).setWriteType(WriteEntity.WriteType.INSERT);
-
-        validateProcess(constructEvent(query,  HiveOperation.QUERY, inputs, outputs));
+        validateProcess(query, getQualifiedTblName(tableName), getQualifiedTblName(insertTableName + HiveMetaStoreBridge.TEMP_TABLE_PREFIX + SessionState.get().getSessionId()));
 
         assertTableIsRegistered(DEFAULT_DB, tableName);
         assertTableIsRegistered(DEFAULT_DB, insertTableName, null, true);
@@ -718,40 +588,16 @@ public class HiveHookIT {
 
     @Test
     public void testInsertIntoPartition() throws Exception {
-        final boolean isPartitionedTable = true;
-        String tableName = createTable(isPartitionedTable);
-        String insertTableName = createTable(isPartitionedTable);
+        String tableName = createTable(true);
+        String insertTableName = createTable(true);
         String query =
-            "insert into " + insertTableName + " partition(dt = '"+ PART_FILE + "') select id, name from " + tableName
-                + " where dt = '"+ PART_FILE + "'";
+            "insert into " + insertTableName + " partition(dt = '2015-01-01') select id, name from " + tableName
+                + " where dt = '2015-01-01'";
         runCommand(query);
-
-        final Set<ReadEntity> inputs = getInputs(tableName, Entity.Type.TABLE);
-        final Set<WriteEntity> outputs = getOutputs(insertTableName, Entity.Type.TABLE);
-        ((WriteEntity)outputs.iterator().next()).setWriteType(WriteEntity.WriteType.INSERT);
-
-        final Set<ReadEntity> partitionIps = new LinkedHashSet<ReadEntity>() {
-            {
-                addAll(inputs);
-                add(getPartitionInput());
-
-            }
-        };
-
-        final Set<WriteEntity> partitionOps = new LinkedHashSet<WriteEntity>() {
-            {
-                addAll(outputs);
-                add(getPartitionOutput());
-
-            }
-        };
-
-        validateProcess(constructEvent(query,  HiveOperation.QUERY, partitionIps, partitionOps), inputs, outputs);
+        validateProcess(query, getQualifiedTblName(tableName) , getQualifiedTblName(insertTableName));
 
         assertTableIsRegistered(DEFAULT_DB, tableName);
         assertTableIsRegistered(DEFAULT_DB, insertTableName);
-
-        //TODO - update
     }
 
     private String random() {
@@ -776,113 +622,61 @@ public class HiveHookIT {
     public void testExportImportUnPartitionedTable() throws Exception {
         String tableName = createTable(false);
 
-        assertTableIsRegistered(DEFAULT_DB, tableName);
+        String tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
 
-        String filename = "pfile://" + mkdir("exportUnPartitioned");
+        String filename = "pfile://" + mkdir("export");
         String query = "export table " + tableName + " to \"" + filename + "\"";
+        final String testPathNormed = lower(new Path(filename).toString());
         runCommand(query);
-
-        Set<ReadEntity> inputs = getInputs(tableName, Entity.Type.TABLE);
-        Set<WriteEntity> outputs = getOutputs(filename, Entity.Type.DFS_DIR);
-
-        Referenceable processReference = validateProcess(constructEvent(query, HiveOperation.EXPORT, inputs, outputs));
-
+        String tblQlfName = getQualifiedTblName(tableName);
+        Referenceable processReference = validateProcess(query, tblQlfName, testPathNormed);
         validateHDFSPaths(processReference, OUTPUTS, filename);
-        validateInputTables(processReference, inputs);
+        validateInputTables(processReference, tblQlfName);
 
         //Import
-        String importTableName = createTable(false);
-        assertTableIsRegistered(DEFAULT_DB, importTableName);
+        tableName = createTable(false);
+        tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
 
-        query = "import table " + importTableName + " from '" + filename + "'";
+        query = "import table " + tableName + " from '" + filename + "'";
         runCommand(query);
-        outputs = getOutputs(importTableName, Entity.Type.TABLE);
-        validateProcess(constructEvent(query, HiveOperation.IMPORT, getInputs(filename, Entity.Type.DFS_DIR), outputs));
-
-        //Should create another process
-        filename = "pfile://" + mkdir("export2UnPartitioned");
-        query = "export table " + tableName + " to \"" + filename + "\"";
-        runCommand(query);
-
-        inputs = getInputs(tableName, Entity.Type.TABLE);
-        outputs = getOutputs(filename, Entity.Type.DFS_DIR);
+        tblQlfName = getQualifiedTblName(tableName);
+        processReference = validateProcess(query, testPathNormed, tblQlfName);
+        validateHDFSPaths(processReference, INPUTS, filename);
 
-        validateProcess(constructEvent(query, HiveOperation.EXPORT, inputs, outputs));
-
-        //import again shouyld create another process
-        query = "import table " + importTableName + " from '" + filename + "'";
-        runCommand(query);
-        outputs = getOutputs(importTableName, Entity.Type.TABLE);
-        validateProcess(constructEvent(query, HiveOperation.IMPORT, getInputs(filename, Entity.Type.DFS_DIR), outputs));
+        validateOutputTables(processReference, tblQlfName);
     }
 
     @Test
     public void testExportImportPartitionedTable() throws Exception {
-        boolean isPartitionedTable = true;
-        final String tableName = createTable(isPartitionedTable);
-        assertTableIsRegistered(DEFAULT_DB, tableName);
+        String tableName = createTable(true);
+        String tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
 
         //Add a partition
         String partFile = "pfile://" + mkdir("partition");
-        String query = "alter table " + tableName + " add partition (dt='"+ PART_FILE + "') location '" + partFile + "'";
+        String query = "alter table " + tableName + " add partition (dt='2015-01-01') location '" + partFile + "'";
         runCommand(query);
 
         String filename = "pfile://" + mkdir("export");
+        final String testPathNormed = lower(new Path(filename).toString());
         query = "export table " + tableName + " to \"" + filename + "\"";
         runCommand(query);
-
-        final Set<ReadEntity> expectedExportInputs = getInputs(tableName, Entity.Type.TABLE);
-        final Set<WriteEntity> outputs = getOutputs(filename, Entity.Type.DFS_DIR);
-
-        //Note that export has only partition as input in this case
-        final Set<ReadEntity> partitionIps = getInputs(DEFAULT_DB + "@" + tableName + "@dt=" + PART_FILE, Entity.Type.PARTITION);
-        partitionIps.addAll(expectedExportInputs);
-
-        Referenceable processReference = validateProcess(constructEvent(query, HiveOperation.EXPORT, partitionIps, outputs), expectedExportInputs, outputs);
+        String tblQlfdName = getQualifiedTblName(tableName);
+        Referenceable processReference = validateProcess(query, tblQlfdName, testPathNormed);
         validateHDFSPaths(processReference, OUTPUTS, filename);
 
-        //Import
-        String importTableName = createTable(true);
-        assertTableIsRegistered(DEFAULT_DB, tableName);
-
-        query = "import table " + importTableName + " from '" + filename + "'";
-        runCommand(query);
-
-        final Set<ReadEntity> expectedImportInputs = getInputs(filename, Entity.Type.DFS_DIR);
-        final Set<WriteEntity> importOutputs = getOutputs(importTableName, Entity.Type.TABLE);
-
-        final Set<WriteEntity> partitionOps = getOutputs(DEFAULT_DB + "@" + importTableName + "@dt=" + PART_FILE, Entity.Type.PARTITION);
-        partitionOps.addAll(importOutputs);
-
-        validateProcess(constructEvent(query, HiveOperation.IMPORT, expectedImportInputs , partitionOps), expectedImportInputs, importOutputs);
-
-        //Export should update same process
-        filename = "pfile://" + mkdir("export2");
-        query = "export table " + tableName + " to \"" + filename + "\"";
-        runCommand(query);
-
-        final Set<WriteEntity> outputs2 = getOutputs(filename, Entity.Type.DFS_DIR);
-        Set<WriteEntity> p3Outputs = new LinkedHashSet<WriteEntity>() {{
-            addAll(outputs2);
-            addAll(outputs);
-        }};
+        validateInputTables(processReference, tblQlfdName);
 
-        validateProcess(constructEvent(query, HiveOperation.EXPORT, partitionIps, outputs2), expectedExportInputs, p3Outputs);
+        //Import
+        tableName = createTable(true);
+        tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
 
-        query = "alter table " + importTableName + " drop partition (dt='"+ PART_FILE + "')";
+        query = "import table " + tableName + " from '" + filename + "'";
         runCommand(query);
+        tblQlfdName = getQualifiedTblName(tableName);
+        processReference = validateProcess(query, testPathNormed, tblQlfdName);
+        validateHDFSPaths(processReference, INPUTS, filename);
 
-        //Import should update same process
-        query = "import table " + importTableName + " from '" + filename + "'";
-        runCommandWithDelay(query, 1000);
-
-        final Set<ReadEntity> importInputs = getInputs(filename, Entity.Type.DFS_DIR);
-        final Set<ReadEntity> expectedImport2Inputs  = new LinkedHashSet<ReadEntity>() {{
-            addAll(importInputs);
-            addAll(expectedImportInputs);
-        }};
-
-        validateProcess(constructEvent(query, HiveOperation.IMPORT, importInputs, partitionOps), expectedImport2Inputs, importOutputs);
+        validateOutputTables(processReference, tblQlfdName);
     }
 
     @Test
@@ -890,14 +684,12 @@ public class HiveHookIT {
         String tableName = createTable();
         String query = "select * from " + tableName;
         runCommand(query);
-        Set<ReadEntity> inputs = getInputs(tableName, Entity.Type.TABLE);
-        HiveHook.HiveEventContext hiveEventContext = constructEvent(query, HiveOperation.QUERY, inputs, null);
-        assertProcessIsNotRegistered(hiveEventContext);
+        assertProcessIsNotRegistered(query);
 
         //check with uppercase table name
         query = "SELECT * from " + tableName.toUpperCase();
         runCommand(query);
-        assertProcessIsNotRegistered(hiveEventContext);
+        assertProcessIsNotRegistered(query);
     }
 
     @Test
@@ -919,11 +711,7 @@ public class HiveHookIT {
         String tableName = createTable(true);
         final String newDBName = createDatabase();
 
-        String tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
-        Referenceable tableEntity = atlasClient.getEntity(tableId);
-        final String createTime = (String)tableEntity.get(HiveDataModelGenerator.CREATE_TIME);
-        Assert.assertNotNull(createTime);
-
+        assertTableIsRegistered(DEFAULT_DB, tableName);
         String columnGuid = assertColumnIsRegistered(HiveMetaStoreBridge.getColumnQualifiedName(HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, DEFAULT_DB, tableName), NAME));
         String sdGuid = assertSDIsRegistered(HiveMetaStoreBridge.getStorageDescQFName(HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, DEFAULT_DB, tableName)), null);
         assertDatabaseIsRegistered(newDBName);
@@ -940,7 +728,7 @@ public class HiveHookIT {
 
         final String newTableName = tableName();
         String query = String.format("alter table %s rename to %s", DEFAULT_DB + "." + tableName, newDBName + "." + newTableName);
-        runCommandWithDelay(query, 1000);
+        runCommand(query);
 
         String newColGuid = assertColumnIsRegistered(HiveMetaStoreBridge.getColumnQualifiedName(HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, newDBName, newTableName), NAME));
         Assert.assertEquals(newColGuid, columnGuid);
@@ -962,7 +750,6 @@ public class HiveHookIT {
                 Referenceable sd = ((Referenceable) entity.get(HiveDataModelGenerator.STORAGE_DESC));
                 String location = (String) sd.get(HiveDataModelGenerator.LOCATION);
                 assertTrue(location.contains(newTableName));
-                Assert.assertEquals(entity.get(HiveDataModelGenerator.CREATE_TIME), createTime);
             }
         });
     }
@@ -1166,10 +953,9 @@ public class HiveHookIT {
         String query = String.format("truncate table %s", tableName);
         runCommand(query);
 
-        Set<WriteEntity> outputs = getOutputs(tableName, Entity.Type.TABLE);
 
         String tableId = assertTableIsRegistered(DEFAULT_DB, tableName);
-        validateProcess(constructEvent(query, HiveOperation.TRUNCATETABLE, null, outputs));
+        validateProcess(query, null, getQualifiedTblName(tableName));
 
         //Check lineage
         String datasetName = HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, DEFAULT_DB, tableName);
@@ -1227,16 +1013,13 @@ public class HiveHookIT {
 
     @Test
     public void testTraitsPreservedOnColumnRename() throws Exception {
-        String dbName = createDatabase();
-        String tableName = tableName();
-        String createQuery = String.format("create table %s.%s (id int, name string)", dbName, tableName);
-        runCommand(createQuery);
-        String tbqn = HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, dbName, tableName);
+        String tableName = createTable();
+        String tbqn = HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, DEFAULT_DB, tableName);
         String guid = assertColumnIsRegistered(HiveMetaStoreBridge.getColumnQualifiedName(tbqn, "id"));
         String trait = createTrait(guid);
         String oldColName = "id";
         String newColName = "id_new";
-        String query = String.format("alter table %s.%s change %s %s string", dbName, tableName, oldColName, newColName);
+        String query = String.format("alter table %s change %s %s string", tableName, oldColName, newColName);
         runCommand(query);
 
         String guid2 = assertColumnIsRegistered(HiveMetaStoreBridge.getColumnQualifiedName(tbqn, "id_new"));
@@ -1268,7 +1051,7 @@ public class HiveHookIT {
         String query = "alter table " + tableName + " set location '" + testPath + "'";
         runCommand(query);
 
-        assertTableIsRegistered(DEFAULT_DB, tableName, new AssertPredicate() {
+        String tableId = assertTableIsRegistered(DEFAULT_DB, tableName, new AssertPredicate() {
             @Override
             public void assertOnEntity(Referenceable tableRef) throws Exception {
                 Referenceable sdRef = (Referenceable) tableRef.get(HiveDataModelGenerator.STORAGE_DESC);
@@ -1276,11 +1059,10 @@ public class HiveHookIT {
             }
         });
 
-        String processId = assertEntityIsRegistered(HiveDataTypes.HIVE_PROCESS.getName(), AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME,
-            HiveMetaStoreBridge.getTableQualifiedName(CLUSTER_NAME, DEFAULT_DB, tableName, false), null);
-
-        Referenceable processReference = atlasClient.getEntity(processId);
+        final String tblQlfdName = getQualifiedTblName(tableName);
 
+        final String testPathNormed = lower(new Path(testPath).toString());
+        Referenceable processReference = validateProcess(query, testPathNormed, tblQlfdName);
         validateHDFSPaths(processReference, INPUTS, testPath);
     }
 
@@ -1427,20 +1209,6 @@ public class HiveHookIT {
         assertTableIsNotRegistered(DEFAULT_DB, tableName);
     }
 
-    private WriteEntity getPartitionOutput() {
-        WriteEntity partEntity = new WriteEntity();
-        partEntity.setName(PART_FILE);
-        partEntity.setTyp(Entity.Type.PARTITION);
-        return partEntity;
-    }
-
-    private ReadEntity getPartitionInput() {
-        ReadEntity partEntity = new ReadEntity();
-        partEntity.setName(PART_FILE);
-        partEntity.setTyp(Entity.Type.PARTITION);
-        return partEntity;
-    }
-
     @Test
     public void testDropDatabaseWithCascade() throws Exception {
         //Test Deletion of database and its corresponding tables
@@ -1500,6 +1268,7 @@ public class HiveHookIT {
 
         //Should have no effect
         assertDBIsNotRegistered(dbName);
+        assertProcessIsNotRegistered(query);
     }
 
     @Test
@@ -1512,6 +1281,7 @@ public class HiveHookIT {
 
         //Should have no effect
         assertTableIsNotRegistered(DEFAULT_DB, tableName);
+        assertProcessIsNotRegistered(query);
     }
 
     @Test
@@ -1576,7 +1346,7 @@ public class HiveHookIT {
         assertDatabaseIsRegistered(dbName, new AssertPredicate() {
             @Override
             public void assertOnEntity(Referenceable entity) {
-                assertEquals(entity.get(AtlasClient.OWNER), owner);
+                assertEquals(entity.get(HiveDataModelGenerator.OWNER), owner);
             }
         });
     }
@@ -1689,91 +1459,56 @@ public class HiveHookIT {
         }
     }
 
-    private String assertProcessIsRegistered(final HiveHook.HiveEventContext event) throws Exception {
-        try {
-            SortedSet<ReadEntity> sortedHiveInputs = event.getInputs() == null ? null : new TreeSet<ReadEntity>(entityComparator);
-            SortedSet<WriteEntity> sortedHiveOutputs = event.getOutputs() == null ? null : new TreeSet<WriteEntity>(entityComparator);
+    private String assertProcessIsRegistered(final String queryStr, final String inputTblName, final String... outputTblNames) throws Exception {
 
-            if ( event.getInputs() != null) {
-                sortedHiveInputs.addAll(event.getInputs());
-            }
-            if ( event.getOutputs() != null) {
-                sortedHiveOutputs.addAll(event.getOutputs());
-            }
+        HiveASTRewriter astRewriter = new HiveASTRewriter(conf);
+        String normalizedQuery = normalize(astRewriter.rewrite(queryStr));
 
-            String processQFName = getProcessQualifiedName(event, sortedHiveInputs, sortedHiveOutputs, getSortedProcessDataSets(event.getInputs()), getSortedProcessDataSets(event.getOutputs()));
-            LOG.debug("Searching for process with query {}", processQFName);
-            return assertEntityIsRegistered(HiveDataTypes.HIVE_PROCESS.getName(), AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, processQFName, new AssertPredicate() {
-                @Override
-                public void assertOnEntity(final Referenceable entity) throws Exception {
-                    List<String> recentQueries = (List<String>) entity.get("recentQueries");
-                    Assert.assertEquals(recentQueries.get(0), lower(event.getQueryStr()));
-                }
-            });
-        } catch (Exception e) {
-            LOG.error("Exception : ", e);
-            throw e;
+        List<Referenceable> inputs = null;
+        if (inputTblName != null) {
+            Referenceable inputTableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.name(), new HashMap<String, Object>() {{
+                put(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, inputTblName);
+            }});
+            inputs = new ArrayList<Referenceable>();
+            inputs.add(inputTableRef);
         }
-    }
+        List<Referenceable> outputs = new ArrayList<Referenceable>();
+        if (outputTblNames != null) {
+            for(int i = 0; i < outputTblNames.length; i++) {
+                final String outputTblName = outputTblNames[i];
+                Referenceable outputTableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.name(), new HashMap<String, Object>() {{
+                    put(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, outputTblName);
+                }});
 
-    private String assertProcessIsRegistered(final HiveHook.HiveEventContext event, final Set<ReadEntity> inputTbls, final Set<WriteEntity> outputTbls) throws Exception {
-        try {
-            SortedSet<ReadEntity> sortedHiveInputs = event.getInputs() == null ? null : new TreeSet<ReadEntity>(entityComparator);
-            SortedSet<WriteEntity> sortedHiveOutputs = event.getOutputs() == null ? null : new TreeSet<WriteEntity>(entityComparator);
-            if ( event.getInputs() != null) {
-                sortedHiveInputs.addAll(event.getInputs());
-            }
-            if ( event.getOutputs() != null) {
-                sortedHiveOutputs.addAll(event.getOutputs());
+                outputs.add(outputTableRef);
             }
-            String processQFName = getProcessQualifiedName(event, sortedHiveInputs, sortedHiveOutputs, getSortedProcessDataSets(inputTbls), getSortedProcessDataSets(outputTbls));
-            LOG.debug("Searching for process with query {}", processQFName);
-            return assertEntityIsRegistered(HiveDataTypes.HIVE_PROCESS.getName(), AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, processQFName, new AssertPredicate() {
-                @Override
-                public void assertOnEntity(final Referenceable entity) throws Exception {
-                    List<String> recentQueries = (List<String>) entity.get("recentQueries");
-                    Assert.assertEquals(recentQueries.get(0), lower(event.getQueryStr()));
-                }
-            });
-        } catch(Exception e) {
-            LOG.error("Exception : ", e);
-            throw e;
         }
+        String processQFName = HiveHook.getProcessQualifiedName(normalizedQuery, inputs, outputs);
+        LOG.debug("Searching for process with query {}", processQFName);
+        return assertEntityIsRegistered(HiveDataTypes.HIVE_PROCESS.getName(), AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, processQFName, new AssertPredicate() {
+            @Override
+            public void assertOnEntity(final Referenceable entity) throws Exception {
+                List<String> recentQueries = (List<String>) entity.get("recentQueries");
+                Assert.assertEquals(recentQueries.get(0), queryStr);
+            }
+        });
     }
 
-    private String getDSTypeName(Entity entity) {
-        return Entity.Type.TABLE.equals(entity.getType()) ? HiveDataTypes.HIVE_TABLE.name() : FSDataTypes.HDFS_PATH().toString();
-    }
-
-    private <T extends Entity> SortedMap<T, Referenceable> getSortedProcessDataSets(Set<T> inputTbls) {
-        SortedMap<T, Referenceable> inputs = new TreeMap<T, Referenceable>(entityComparator);
-        if (inputTbls != null) {
-            for (final T tbl : inputTbls) {
-                Referenceable inputTableRef = new Referenceable(getDSTypeName(tbl), new HashMap<String, Object>() {{
-                    put(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, tbl.getName());
-                }});
-                inputs.put(tbl, inputTableRef);
+    private String assertProcessIsRegistered(final String queryStr) throws Exception {
+        String lowerQryStr = lower(queryStr);
+        LOG.debug("Searching for process with query {}", lowerQryStr);
+        return assertEntityIsRegistered(HiveDataTypes.HIVE_PROCESS.getName(), AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, lowerQryStr, new AssertPredicate() {
+            @Override
+            public void assertOnEntity(final Referenceable entity) throws Exception {
+                List<String> recentQueries = (List<String>) entity.get("recentQueries");
+                Assert.assertEquals(recentQueries.get(0), queryStr);
             }
-        }
-        return inputs;
+        });
     }
 
-    private void assertProcessIsNotRegistered(HiveHook.HiveEventContext event) throws Exception {
-        try {
-            SortedSet<ReadEntity> sortedHiveInputs = event.getInputs() == null ? null : new TreeSet<ReadEntity>(entityComparator);
-            SortedSet<WriteEntity> sortedHiveOutputs = event.getOutputs() == null ? null : new TreeSet<WriteEntity>(entityComparator);
-            if ( event.getInputs() != null) {
-                sortedHiveInputs.addAll(event.getInputs());
-            }
-            if ( event.getOutputs() != null) {
-                sortedHiveOutputs.addAll(event.getOutputs());
-            }
-            String processQFName = getProcessQualifiedName(event, sortedHiveInputs, sortedHiveOutputs, getSortedProcessDataSets(event.getInputs()), getSortedProcessDataSets(event.getOutputs()));
-            LOG.debug("Searching for process with query {}", processQFName);
-            assertEntityIsNotRegistered(HiveDataTypes.HIVE_PROCESS.getName(), AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, processQFName);
-        } catch( Exception e) {
-            LOG.error("Exception : ", e);
-        }
+    private void assertProcessIsNotRegistered(String queryStr) throws Exception {
+        LOG.debug("Searching for process with query {}", queryStr);
+        assertEntityIsNotRegistered(HiveDataTypes.HIVE_PROCESS.getName(), AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, normalize(queryStr));
     }
 
     private void assertTableIsNotRegistered(String dbName, String tableName, boolean isTemporaryTable) throws Exception {
@@ -1798,6 +1533,9 @@ public class HiveHookIT {
         return assertTableIsRegistered(dbName, tableName, null, false);
     }
 
+    private String assertTableIsRegistered(String dbName, String tableName, boolean isTemporary) throws Exception {
+        return assertTableIsRegistered(dbName, tableName, null, isTemporary);
+    }
 
     private String assertTableIsRegistered(String dbName, String tableName, AssertPredicate assertPredicate, boolean isTemporary) throws Exception {
         LOG.debug("Searching for table {}.{}", dbName, tableName);