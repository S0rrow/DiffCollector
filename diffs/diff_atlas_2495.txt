diff --git a/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java b/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
index c1940a670..ee5ae10ac 100755
--- a/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
+++ b/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
@@ -18,27 +18,17 @@
 
 package org.apache.atlas.hive.bridge;
 
-import com.sun.jersey.api.client.ClientResponse;
 import org.apache.atlas.ApplicationProperties;
 import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasConstants;
 import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.fs.model.FSDataModel;
-import org.apache.atlas.fs.model.FSDataTypes;
 import org.apache.atlas.hive.model.HiveDataModelGenerator;
 import org.apache.atlas.hive.model.HiveDataTypes;
-import org.apache.atlas.notification.hook.HookNotification;
 import org.apache.atlas.typesystem.Referenceable;
 import org.apache.atlas.typesystem.Struct;
 import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.json.TypesSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.atlas.utils.AuthenticationUtil;
 import org.apache.commons.configuration.Configuration;
-import org.apache.commons.lang.RandomStringUtils;
-import org.apache.hadoop.fs.Path;
+import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Order;
@@ -47,45 +37,37 @@ import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.codehaus.jettison.json.JSONArray;
 import org.codehaus.jettison.json.JSONException;
 import org.codehaus.jettison.json.JSONObject;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import java.util.ArrayList;
-import java.util.Date;
 import java.util.List;
 
 /**
  * A Bridge Utility that imports metadata from the Hive Meta Store
- * and registers them in Atlas.
+ * and registers then in Atlas.
  */
 public class HiveMetaStoreBridge {
     private static final String DEFAULT_DGI_URL = "http://localhost:21000/";
     public static final String HIVE_CLUSTER_NAME = "atlas.cluster.name";
     public static final String DEFAULT_CLUSTER_NAME = "primary";
-    public static final String DESCRIPTION_ATTR = "description";
-    public static final String SEARCH_ENTRY_GUID_ATTR = "__guid";
-
-    public static final String TEMP_TABLE_PREFIX = "_temp-";
-
     private final String clusterName;
-    public static final int MILLIS_CONVERT_FACTOR = 1000;
 
     public static final String ATLAS_ENDPOINT = "atlas.rest.address";
 
     private static final Logger LOG = LoggerFactory.getLogger(HiveMetaStoreBridge.class);
 
     public final Hive hiveClient;
-    private AtlasClient atlasClient = null;
+    private final AtlasClient atlasClient;
 
-    HiveMetaStoreBridge(String clusterName, Hive hiveClient, AtlasClient atlasClient) {
-        this.clusterName = clusterName;
-        this.hiveClient = hiveClient;
-        this.atlasClient = atlasClient;
+    public HiveMetaStoreBridge(HiveConf hiveConf, Configuration atlasConf) throws Exception {
+        this(hiveConf, atlasConf, null, null);
     }
 
     public String getClusterName() {
@@ -94,25 +76,21 @@ public class HiveMetaStoreBridge {
 
     /**
      * Construct a HiveMetaStoreBridge.
-     * @param hiveConf {@link HiveConf} for Hive component in the cluster
+     * @param hiveConf hive conf
      */
-    public HiveMetaStoreBridge(HiveConf hiveConf) throws Exception {
-        this(hiveConf.get(HIVE_CLUSTER_NAME, DEFAULT_CLUSTER_NAME), Hive.get(hiveConf), null);
-    }
+    public HiveMetaStoreBridge(HiveConf hiveConf, Configuration atlasConf, String doAsUser,
+                               UserGroupInformation ugi) throws Exception {
+        clusterName = hiveConf.get(HIVE_CLUSTER_NAME, DEFAULT_CLUSTER_NAME);
+        hiveClient = Hive.get(hiveConf);
 
-    /**
-     * Construct a HiveMetaStoreBridge.
-     * @param hiveConf {@link HiveConf} for Hive component in the cluster
-     */
-    public HiveMetaStoreBridge(HiveConf hiveConf, AtlasClient atlasClient) throws Exception {
-        this(hiveConf.get(HIVE_CLUSTER_NAME, DEFAULT_CLUSTER_NAME), Hive.get(hiveConf), atlasClient);
+        atlasClient = new AtlasClient(atlasConf.getString(ATLAS_ENDPOINT, DEFAULT_DGI_URL), ugi, doAsUser);
     }
 
-    AtlasClient getAtlasClient() {
+    public AtlasClient getAtlasClient() {
         return atlasClient;
     }
 
-    void importHiveMetadata() throws Exception {
+    public void importHiveMetadata() throws Exception {
         LOG.info("Importing hive metadata");
         importDatabases();
     }
@@ -127,13 +105,27 @@ public class HiveMetaStoreBridge {
     }
 
     /**
-     * Create a Hive Database entity
-     * @param hiveDB The Hive {@link Database} object from which to map properties
-     * @return new Hive Database entity
+     * Creates db entity
+     * @param hiveDB
+     * @return
      * @throws HiveException
      */
     public Referenceable createDBInstance(Database hiveDB) throws HiveException {
-        return createOrUpdateDBInstance(hiveDB, null);
+        LOG.info("Importing objects from databaseName : " + hiveDB.getName());
+
+        Referenceable dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
+        String dbName = hiveDB.getName().toLowerCase();
+        dbRef.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, getDBQualifiedName(clusterName, dbName));
+        dbRef.set(HiveDataModelGenerator.NAME, dbName);
+        dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
+        dbRef.set("description", hiveDB.getDescription());
+        dbRef.set("locationUri", hiveDB.getLocationUri());
+        dbRef.set("parameters", hiveDB.getParameters());
+        dbRef.set("ownerName", hiveDB.getOwnerName());
+        if (hiveDB.getOwnerType() != null) {
+            dbRef.set("ownerType", hiveDB.getOwnerType().getValue());
+        }
+        return dbRef;
     }
 
     /**
@@ -144,34 +136,12 @@ public class HiveMetaStoreBridge {
      */
     private Referenceable registerDatabase(String databaseName) throws Exception {
         Referenceable dbRef = getDatabaseReference(clusterName, databaseName);
-        Database db = hiveClient.getDatabase(databaseName);
         if (dbRef == null) {
+            Database db = hiveClient.getDatabase(databaseName);
             dbRef = createDBInstance(db);
             dbRef = registerInstance(dbRef);
         } else {
-            LOG.info("Database {} is already registered with id {}. Updating it.", databaseName, dbRef.getId().id);
-            dbRef = createOrUpdateDBInstance(db, dbRef);
-            updateInstance(dbRef);
-        }
-        return dbRef;
-    }
-
-    private Referenceable createOrUpdateDBInstance(Database hiveDB, Referenceable dbRef) {
-        LOG.info("Importing objects from databaseName : " + hiveDB.getName());
-
-        if (dbRef == null) {
-            dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
-        }
-        String dbName = hiveDB.getName().toLowerCase();
-        dbRef.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, getDBQualifiedName(clusterName, dbName));
-        dbRef.set(HiveDataModelGenerator.NAME, dbName);
-        dbRef.set(AtlasConstants.CLUSTER_NAME_ATTRIBUTE, clusterName);
-        dbRef.set(DESCRIPTION_ATTR, hiveDB.getDescription());
-        dbRef.set(HiveDataModelGenerator.LOCATION, hiveDB.getLocationUri());
-        dbRef.set(HiveDataModelGenerator.PARAMETERS, hiveDB.getParameters());
-        dbRef.set(HiveDataModelGenerator.OWNER, hiveDB.getOwnerName());
-        if (hiveDB.getOwnerType() != null) {
-            dbRef.set("ownerType", hiveDB.getOwnerType().getValue());
+            LOG.info("Database {} is already registered with id {}", databaseName, dbRef.getId().id);
         }
         return dbRef;
     }
@@ -182,16 +152,16 @@ public class HiveMetaStoreBridge {
      * @return
      * @throws Exception
      */
-    private Referenceable registerInstance(Referenceable referenceable) throws Exception {
+    public Referenceable registerInstance(Referenceable referenceable) throws Exception {
         String typeName = referenceable.getTypeName();
         LOG.debug("creating instance of type " + typeName);
 
         String entityJSON = InstanceSerialization.toJson(referenceable, true);
         LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(), entityJSON);
-        List<String> guids = getAtlasClient().createEntity(entityJSON);
+        JSONArray guids = atlasClient.createEntity(entityJSON);
         LOG.debug("created instance for type " + typeName + ", guid: " + guids);
 
-        return new Referenceable(guids.get(guids.size() - 1), referenceable.getTypeName(), null);
+        return new Referenceable(guids.getString(0), referenceable.getTypeName(), null);
     }
 
     /**
@@ -205,15 +175,11 @@ public class HiveMetaStoreBridge {
         LOG.debug("Getting reference for database {}", databaseName);
         String typeName = HiveDataTypes.HIVE_DB.getName();
 
-        String dslQuery = getDatabaseDSLQuery(clusterName, databaseName, typeName);
+        String dslQuery = String.format("%s where %s = '%s' and %s = '%s'", typeName, HiveDataModelGenerator.NAME,
+                databaseName.toLowerCase(), HiveDataModelGenerator.CLUSTER_NAME, clusterName);
         return getEntityReferenceFromDSL(typeName, dslQuery);
     }
 
-    static String getDatabaseDSLQuery(String clusterName, String databaseName, String typeName) {
-        return String.format("%s where %s = '%s' and %s = '%s'", typeName, HiveDataModelGenerator.NAME,
-                databaseName.toLowerCase(), AtlasConstants.CLUSTER_NAME_ATTRIBUTE, clusterName);
-    }
-
     private Referenceable getEntityReferenceFromDSL(String typeName, String dslQuery) throws Exception {
         AtlasClient dgiClient = getAtlasClient();
         JSONArray results = dgiClient.searchByDSL(dslQuery);
@@ -231,28 +197,10 @@ public class HiveMetaStoreBridge {
         }
     }
 
-    /**
-     * Construct the qualified name used to uniquely identify a Database instance in Atlas.
-     * @param clusterName Name of the cluster to which the Hive component belongs
-     * @param dbName Name of the Hive database
-     * @return Unique qualified name to identify the Database instance in Atlas.
-     */
     public static String getDBQualifiedName(String clusterName, String dbName) {
         return String.format("%s@%s", dbName.toLowerCase(), clusterName);
     }
 
-    private String getCreateTableString(Table table, String location){
-        String colString = "";
-        List<FieldSchema> colList = table.getAllCols();
-        for(FieldSchema col:colList){
-            colString += col.getName()  + " " + col.getType() + ",";
-        }
-        colString = colString.substring(0, colString.length() - 1);
-        String query = "create external table " + table.getTableName() + "(" + colString + ")" +
-                " location '" + location + "'";
-        return query;
-    }
-
     /**
      * Imports all tables for the given db
      * @param databaseName
@@ -261,226 +209,96 @@ public class HiveMetaStoreBridge {
      */
     private void importTables(Referenceable databaseReferenceable, String databaseName) throws Exception {
         List<String> hiveTables = hiveClient.getAllTables(databaseName);
-        LOG.info("Importing tables {} for db {}", hiveTables.toString(), databaseName);
+
         for (String tableName : hiveTables) {
             Table table = hiveClient.getTable(databaseName, tableName);
             Referenceable tableReferenceable = registerTable(databaseReferenceable, table);
-            if (table.getTableType() == TableType.EXTERNAL_TABLE){
-                String tableQualifiedName = getTableQualifiedName(clusterName, table);
-                Referenceable process = getProcessReference(tableQualifiedName);
-                if (process == null){
-                    LOG.info("Attempting to register create table process for {}", tableQualifiedName);
-                    Referenceable lineageProcess = new Referenceable(HiveDataTypes.HIVE_PROCESS.getName());
-                    ArrayList<Referenceable> sourceList = new ArrayList<>();
-                    ArrayList<Referenceable> targetList = new ArrayList<>();
-                    String tableLocation = table.getDataLocation().toString();
-                    Referenceable path = fillHDFSDataSet(tableLocation);
-                    String query = getCreateTableString(table, tableLocation);
-                    sourceList.add(path);
-                    targetList.add(tableReferenceable);
-                    lineageProcess.set("inputs", sourceList);
-                    lineageProcess.set("outputs", targetList);
-                    lineageProcess.set("userName", table.getOwner());
-                    lineageProcess.set("startTime", new Date(System.currentTimeMillis()));
-                    lineageProcess.set("endTime", new Date(System.currentTimeMillis()));
-                    lineageProcess.set("operationType", "CREATETABLE");
-                    lineageProcess.set("queryText", query);
-                    lineageProcess.set("queryId", query);
-                    lineageProcess.set("queryPlan", "{}");
-                    lineageProcess.set("clusterName", clusterName);
-                    List<String> recentQueries = new ArrayList<>(1);
-                    recentQueries.add(query);
-                    lineageProcess.set("recentQueries", recentQueries);
-                    lineageProcess.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, tableQualifiedName);
-                    lineageProcess.set(AtlasClient.NAME, query);
-                    registerInstance(lineageProcess);
-
-                }
-                else {
-                    LOG.info("Process {} is already registered", process.toString());
-                }
-            }
+
+            // Import Partitions
+            Referenceable sdReferenceable = getSDForTable(databaseName, tableName);
+            registerPartitions(tableReferenceable, sdReferenceable, table);
         }
     }
 
     /**
      * Gets reference for the table
      *
-     * @param hiveTable
+     * @param dbName database name
+     * @param tableName table name
      * @return table reference if exists, else null
      * @throws Exception
      */
-    private Referenceable getTableReference(Table hiveTable)  throws Exception {
-        LOG.debug("Getting reference for table {}.{}", hiveTable.getDbName(), hiveTable.getTableName());
+    private Referenceable getTableReference(String dbName, String tableName) throws Exception {
+        LOG.debug("Getting reference for table {}.{}", dbName, tableName);
 
         String typeName = HiveDataTypes.HIVE_TABLE.getName();
-        String dslQuery = getTableDSLQuery(getClusterName(), hiveTable.getDbName(), hiveTable.getTableName(), typeName, hiveTable.isTemporary());
-        return getEntityReferenceFromDSL(typeName, dslQuery);
-    }
-
-    private Referenceable getProcessReference(String qualifiedName) throws Exception{
-        LOG.debug("Getting reference for process {}", qualifiedName);
-        String typeName = HiveDataTypes.HIVE_PROCESS.getName();
-        String dslQuery = getProcessDSLQuery(typeName, qualifiedName);
+        String entityName = getTableQualifiedName(clusterName, dbName, tableName);
+        String dslQuery = String.format("%s as t where name = '%s'", typeName, entityName);
         return getEntityReferenceFromDSL(typeName, dslQuery);
     }
 
-    static String getProcessDSLQuery(String typeName, String qualifiedName) throws Exception{
-        String dslQuery = String.format("%s as t where qualifiedName = '%s'", typeName, qualifiedName);
-        return dslQuery;
-    }
-
-    static String getTableDSLQuery(String clusterName, String dbName, String tableName, String typeName, boolean isTemporary) {
-        String entityName = getTableQualifiedName(clusterName, dbName, tableName, isTemporary);
-        return String.format("%s as t where qualifiedName = '%s'", typeName, entityName);
-    }
-
-    /**
-     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
-     * @param clusterName Name of the cluster to which the Hive component belongs
-     * @param dbName Name of the Hive database to which the Table belongs
-     * @param tableName Name of the Hive table
-     * @return Unique qualified name to identify the Table instance in Atlas.
-     */
-    public static String getTableQualifiedName(String clusterName, String dbName, String tableName, boolean isTemporaryTable) {
-        String tableTempName = tableName;
-        if (isTemporaryTable) {
-            if (SessionState.get().getSessionId() != null) {
-                tableTempName = tableName + TEMP_TABLE_PREFIX + SessionState.get().getSessionId();
-            } else {
-                tableTempName = tableName + TEMP_TABLE_PREFIX + RandomStringUtils.random(10);
-            }
-        }
-        return String.format("%s.%s@%s", dbName.toLowerCase(), tableTempName.toLowerCase(), clusterName);
-    }
-
-
-
-    /**
-     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
-     * @param clusterName Name of the cluster to which the Hive component belongs
-     * @param table hive table for which the qualified name is needed
-     * @return Unique qualified name to identify the Table instance in Atlas.
-     */
-    public static String getTableQualifiedName(String clusterName, Table table) {
-        return getTableQualifiedName(clusterName, table.getDbName(), table.getTableName(), table.isTemporary());
-    }
-
-    /**
-     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
-     * @param clusterName Name of the cluster to which the Hive component belongs
-     * @param dbName Name of the Hive database to which the Table belongs
-     * @param tableName Name of the Hive table
-     * @return Unique qualified name to identify the Table instance in Atlas.
-     */
     public static String getTableQualifiedName(String clusterName, String dbName, String tableName) {
-         return getTableQualifiedName(clusterName, dbName, tableName, false);
+        return String.format("%s.%s@%s", dbName.toLowerCase(), tableName.toLowerCase(), clusterName);
     }
 
-    /**
-     * Create a new table instance in Atlas
-     * @param dbReference reference to a created Hive database {@link Referenceable} to which this table belongs
-     * @param hiveTable reference to the Hive {@link Table} from which to map properties
-     * @return Newly created Hive reference
-     * @throws Exception
-     */
     public Referenceable createTableInstance(Referenceable dbReference, Table hiveTable)
             throws Exception {
-        return createOrUpdateTableInstance(dbReference, null, hiveTable);
-    }
-
-    private Referenceable createOrUpdateTableInstance(Referenceable dbReference, Referenceable tableReference,
-                                                      final Table hiveTable) throws Exception {
         LOG.info("Importing objects from {}.{}", hiveTable.getDbName(), hiveTable.getTableName());
 
-        if (tableReference == null) {
-            tableReference = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
-        }
-
-        String tableQualifiedName = getTableQualifiedName(clusterName, hiveTable);
-        tableReference.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, tableQualifiedName);
-        tableReference.set(HiveDataModelGenerator.NAME, hiveTable.getTableName().toLowerCase());
-        tableReference.set(HiveDataModelGenerator.OWNER, hiveTable.getOwner());
-
-        Date createDate = new Date();
-        if (hiveTable.getMetadata().getProperty(hive_metastoreConstants.DDL_TIME) != null){
-            try {
-                createDate = new Date(Long.parseLong(hiveTable.getMetadata().getProperty(hive_metastoreConstants.DDL_TIME)) * MILLIS_CONVERT_FACTOR);
-                tableReference.set(HiveDataModelGenerator.CREATE_TIME, createDate);
-            } catch(NumberFormatException ne) {
-                LOG.error("Error while updating createTime for the table {} ", hiveTable.getCompleteName(), ne);
-            }
-        }
+        Referenceable tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
+        String tableQualifiedName = getTableQualifiedName(clusterName, hiveTable.getDbName(), hiveTable.getTableName());
+        tableRef.set(HiveDataModelGenerator.NAME, tableQualifiedName);
+        tableRef.set(HiveDataModelGenerator.TABLE_NAME, hiveTable.getTableName().toLowerCase());
+        tableRef.set("owner", hiveTable.getOwner());
 
-        Date lastAccessTime = createDate;
-        if ( hiveTable.getLastAccessTime() > 0) {
-            lastAccessTime = new Date(hiveTable.getLastAccessTime() * MILLIS_CONVERT_FACTOR);
-        }
-        tableReference.set(HiveDataModelGenerator.LAST_ACCESS_TIME, lastAccessTime);
-        tableReference.set("retention", hiveTable.getRetention());
+        tableRef.set("createTime", hiveTable.getMetadata().getProperty(hive_metastoreConstants.DDL_TIME));
+        tableRef.set("lastAccessTime", hiveTable.getLastAccessTime());
+        tableRef.set("retention", hiveTable.getRetention());
 
-        tableReference.set(HiveDataModelGenerator.COMMENT, hiveTable.getParameters().get(HiveDataModelGenerator.COMMENT));
+        tableRef.set(HiveDataModelGenerator.COMMENT, hiveTable.getParameters().get(HiveDataModelGenerator.COMMENT));
 
         // add reference to the database
-        tableReference.set(HiveDataModelGenerator.DB, dbReference);
+        tableRef.set(HiveDataModelGenerator.DB, dbReference);
+
+        tableRef.set("columns", getColumns(hiveTable.getCols(), tableQualifiedName));
 
         // add reference to the StorageDescriptor
-        Referenceable sdReferenceable = fillStorageDesc(hiveTable.getSd(), tableQualifiedName, getStorageDescQFName(tableQualifiedName), tableReference.getId());
-        tableReference.set(HiveDataModelGenerator.STORAGE_DESC, sdReferenceable);
+        Referenceable sdReferenceable = fillStorageDescStruct(hiveTable.getSd(), tableQualifiedName, tableQualifiedName);
+        tableRef.set("sd", sdReferenceable);
 
-        tableReference.set(HiveDataModelGenerator.PARAMETERS, hiveTable.getParameters());
+        // add reference to the Partition Keys
+        List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys(), tableQualifiedName);
+        tableRef.set("partitionKeys", partKeys);
+
+        tableRef.set("parameters", hiveTable.getParameters());
 
         if (hiveTable.getViewOriginalText() != null) {
-            tableReference.set("viewOriginalText", hiveTable.getViewOriginalText());
+            tableRef.set("viewOriginalText", hiveTable.getViewOriginalText());
         }
 
         if (hiveTable.getViewExpandedText() != null) {
-            tableReference.set("viewExpandedText", hiveTable.getViewExpandedText());
+            tableRef.set("viewExpandedText", hiveTable.getViewExpandedText());
         }
 
-        tableReference.set(HiveDataModelGenerator.TABLE_TYPE_ATTR, hiveTable.getTableType().name());
-        tableReference.set("temporary", hiveTable.isTemporary());
-
-        // add reference to the Partition Keys
-        List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys(), tableQualifiedName, tableReference.getId());
-        tableReference.set("partitionKeys", partKeys);
-
-        tableReference.set(HiveDataModelGenerator.COLUMNS, getColumns(hiveTable.getCols(), tableQualifiedName, tableReference.getId()));
-
-        return tableReference;
-    }
-
-    public static String getStorageDescQFName(String entityQualifiedName) {
-        return entityQualifiedName + "_storage";
+        tableRef.set("tableType", hiveTable.getTableType().name());
+        tableRef.set("temporary", hiveTable.isTemporary());
+        return tableRef;
     }
 
     private Referenceable registerTable(Referenceable dbReference, Table table) throws Exception {
         String dbName = table.getDbName();
         String tableName = table.getTableName();
         LOG.info("Attempting to register table [" + tableName + "]");
-        Referenceable tableReference = getTableReference(table);
-        LOG.info("Found result " + tableReference);
-        if (tableReference == null) {
-            tableReference = createTableInstance(dbReference, table);
-            tableReference = registerInstance(tableReference);
+        Referenceable tableRef = getTableReference(dbName, tableName);
+        if (tableRef == null) {
+            tableRef = createTableInstance(dbReference, table);
+            tableRef = registerInstance(tableRef);
         } else {
-            LOG.info("Table {}.{} is already registered with id {}. Updating entity.", dbName, tableName,
-                    tableReference.getId().id);
-            tableReference = createOrUpdateTableInstance(dbReference, tableReference, table);
-            updateInstance(tableReference);
+            LOG.info("Table {}.{} is already registered with id {}", dbName, tableName, tableRef.getId().id);
         }
-        return tableReference;
+        return tableRef;
     }
 
-    private void updateInstance(Referenceable referenceable) throws AtlasServiceException {
-        String typeName = referenceable.getTypeName();
-        LOG.debug("updating instance of type " + typeName);
-
-        String entityJSON = InstanceSerialization.toJson(referenceable, true);
-        LOG.debug("Updating entity {} = {}", referenceable.getTypeName(), entityJSON);
-
-        atlasClient.updateEntity(referenceable.getId().id, referenceable);
-    }
 
     private Referenceable getEntityReferenceFromGremlin(String typeName, String gremlinQuery)
     throws AtlasServiceException, JSONException {
@@ -489,12 +307,101 @@ public class HiveMetaStoreBridge {
         if (results.length() == 0) {
             return null;
         }
-        String guid = results.getJSONObject(0).getString(SEARCH_ENTRY_GUID_ATTR);
+        String guid = results.getJSONObject(0).getString("__guid");
         return new Referenceable(guid, typeName, null);
     }
 
-    public Referenceable fillStorageDesc(StorageDescriptor storageDesc, String tableQualifiedName,
-        String sdQualifiedName, Id tableId) throws Exception {
+    private Referenceable getPartitionReference(String dbName, String tableName, List<String> values) throws Exception {
+        String valuesStr = "['" + StringUtils.join(values, "', '") + "']";
+        LOG.debug("Getting reference for partition for {}.{} with values {}", dbName, tableName, valuesStr);
+        String typeName = HiveDataTypes.HIVE_PARTITION.getName();
+
+        //todo replace gremlin with DSL
+        //        String dslQuery = String.format("%s as p where values = %s, tableName where name = '%s', "
+        //                        + "dbName where name = '%s' and clusterName = '%s' select p", typeName, valuesStr,
+        // tableName,
+        //                dbName, clusterName);
+
+        String datasetType = AtlasClient.DATA_SET_SUPER_TYPE;
+        String tableEntityName = getTableQualifiedName(clusterName, dbName, tableName);
+
+        String gremlinQuery = String.format("g.V.has('__typeName', '%s').has('%s.values', %s).as('p')."
+                        + "out('__%s.table').has('%s.name', '%s').back('p').toList()", typeName, typeName, valuesStr,
+                typeName, datasetType, tableEntityName);
+
+        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
+    }
+
+    private Referenceable getSDForTable(String dbName, String tableName) throws Exception {
+        Referenceable tableRef = getTableReference(dbName, tableName);
+        if (tableRef == null) {
+            throw new IllegalArgumentException("Table " + dbName + "." + tableName + " doesn't exist");
+        }
+
+        AtlasClient dgiClient = getAtlasClient();
+        Referenceable tableInstance = dgiClient.getEntity(tableRef.getId().id);
+        Referenceable sd = (Referenceable) tableInstance.get("sd");
+        return new Referenceable(sd.getId().id, sd.getTypeName(), null);
+    }
+
+    private void registerPartitions(Referenceable tableReferenceable, Referenceable sdReferenceable,
+                                    Table table) throws Exception {
+        String dbName = table.getDbName();
+        String tableName = table.getTableName();
+        LOG.info("Registering partitions for {}.{}", dbName, tableName);
+        List<Partition> tableParts = hiveClient.getPartitions(table);
+
+        for (Partition hivePart : tableParts) {
+            registerPartition(tableReferenceable, sdReferenceable, hivePart);
+        }
+    }
+
+    private Referenceable registerPartition(Referenceable tableReferenceable, Referenceable sdReferenceable,
+                                            Partition hivePart) throws Exception {
+        LOG.info("Registering partition for {} with values {}", tableReferenceable,
+                StringUtils.join(hivePart.getValues(), ","));
+        String dbName = hivePart.getTable().getDbName();
+        String tableName = hivePart.getTable().getTableName();
+
+        Referenceable partRef = getPartitionReference(dbName, tableName, hivePart.getValues());
+        if (partRef == null) {
+            partRef = createPartitionReferenceable(tableReferenceable, sdReferenceable, hivePart);
+            partRef = registerInstance(partRef);
+        } else {
+            LOG.info("Partition {}.{} with values {} is already registered with id {}", dbName, tableName,
+                    StringUtils.join(hivePart.getValues(), ","), partRef.getId().id);
+        }
+        return partRef;
+    }
+
+    public Referenceable createPartitionReferenceable(Referenceable tableReferenceable, Referenceable sdReferenceable,
+                                                      Partition hivePart) {
+        Referenceable partRef = new Referenceable(HiveDataTypes.HIVE_PARTITION.getName());
+        partRef.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, getPartitionQualifiedName(hivePart));
+        partRef.set("values", hivePart.getValues());
+
+        partRef.set(HiveDataModelGenerator.TABLE, tableReferenceable);
+
+        //todo fix
+        partRef.set("createTime", hivePart.getLastAccessTime());
+        partRef.set("lastAccessTime", hivePart.getLastAccessTime());
+
+        // sdStruct = fillStorageDescStruct(hivePart.getSd());
+        // Instead of creating copies of the sdstruct for partitions we are reusing existing
+        // ones will fix to identify partitions with differing schema.
+        partRef.set("sd", sdReferenceable);
+
+        partRef.set("parameters", hivePart.getParameters());
+        return partRef;
+    }
+
+    private String getPartitionQualifiedName(Partition partition) {
+        return String.format("%s.%s.%s@%s", partition.getTable().getDbName(),
+                partition.getTable().getTableName(), StringUtils.join(partition.getValues(), "-"), clusterName);
+    }
+
+    private Referenceable fillStorageDescStruct(StorageDescriptor storageDesc, String tableQualifiedName,
+                                                String sdQualifiedName) throws Exception {
         LOG.debug("Filling storage descriptor information for " + storageDesc);
 
         Referenceable sdReferenceable = new Referenceable(HiveDataTypes.HIVE_STORAGEDESC.getName());
@@ -509,13 +416,19 @@ public class HiveMetaStoreBridge {
 
         serdeInfoStruct.set(HiveDataModelGenerator.NAME, serdeInfo.getName());
         serdeInfoStruct.set("serializationLib", serdeInfo.getSerializationLib());
-        serdeInfoStruct.set(HiveDataModelGenerator.PARAMETERS, serdeInfo.getParameters());
+        serdeInfoStruct.set("parameters", serdeInfo.getParameters());
 
         sdReferenceable.set("serdeInfo", serdeInfoStruct);
         sdReferenceable.set(HiveDataModelGenerator.STORAGE_NUM_BUCKETS, storageDesc.getNumBuckets());
         sdReferenceable
                 .set(HiveDataModelGenerator.STORAGE_IS_STORED_AS_SUB_DIRS, storageDesc.isStoredAsSubDirectories());
 
+        //Use the passed column list if not null, ex: use same references for table and SD
+        List<FieldSchema> columns = storageDesc.getCols();
+        if (columns != null && !columns.isEmpty()) {
+            sdReferenceable.set("cols", getColumns(columns, tableQualifiedName));
+        }
+
         List<Struct> sortColsStruct = new ArrayList<>();
         for (Order sortcol : storageDesc.getSortCols()) {
             String hiveOrderName = HiveDataTypes.HIVE_ORDER.getName();
@@ -529,7 +442,7 @@ public class HiveMetaStoreBridge {
             sdReferenceable.set("sortCols", sortColsStruct);
         }
 
-        sdReferenceable.set(HiveDataModelGenerator.LOCATION, storageDesc.getLocation());
+        sdReferenceable.set("location", storageDesc.getLocation());
         sdReferenceable.set("inputFormat", storageDesc.getInputFormat());
         sdReferenceable.set("outputFormat", storageDesc.getOutputFormat());
         sdReferenceable.set("compressed", storageDesc.isCompressed());
@@ -538,32 +451,20 @@ public class HiveMetaStoreBridge {
             sdReferenceable.set("bucketCols", storageDesc.getBucketCols());
         }
 
-        sdReferenceable.set(HiveDataModelGenerator.PARAMETERS, storageDesc.getParameters());
+        sdReferenceable.set("parameters", storageDesc.getParameters());
         sdReferenceable.set("storedAsSubDirectories", storageDesc.isStoredAsSubDirectories());
-        sdReferenceable.set(HiveDataModelGenerator.TABLE, tableId);
 
         return sdReferenceable;
     }
 
-    public Referenceable fillHDFSDataSet(String pathUri) {
-        Referenceable ref = new Referenceable(FSDataTypes.HDFS_PATH().toString());
-        ref.set("path", pathUri);
-        Path path = new Path(pathUri);
-        ref.set(AtlasClient.NAME, path.getName());
-        ref.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, pathUri);
-        return ref;
-    }
-
-    public static String getColumnQualifiedName(final String tableQualifiedName, final String colName) {
-        final String[] parts = tableQualifiedName.split("@");
-        final String tableName = parts[0];
-        final String clusterName = parts[1];
-        return String.format("%s.%s@%s", tableName, colName.toLowerCase(), clusterName);
+    private String getColumnQualifiedName(String tableQualifiedName, String colName) {
+        String[] parts = tableQualifiedName.split("@");
+        String tableName = parts[0];
+        return String.format("%s.%s@%s", tableName, colName, clusterName);
     }
 
-    public List<Referenceable> getColumns(List<FieldSchema> schemaList, String tableQualifiedName, Id tableReference) throws Exception {
+    private List<Referenceable> getColumns(List<FieldSchema> schemaList, String tableQualifiedName) throws Exception {
         List<Referenceable> colList = new ArrayList<>();
-
         for (FieldSchema fs : schemaList) {
             LOG.debug("Processing field " + fs);
             Referenceable colReferenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName());
@@ -572,66 +473,29 @@ public class HiveMetaStoreBridge {
             colReferenceable.set(HiveDataModelGenerator.NAME, fs.getName());
             colReferenceable.set("type", fs.getType());
             colReferenceable.set(HiveDataModelGenerator.COMMENT, fs.getComment());
-            colReferenceable.set(HiveDataModelGenerator.TABLE, tableReference);
 
             colList.add(colReferenceable);
         }
         return colList;
     }
 
-    /**
-     * Register the Hive DataModel in Atlas, if not already defined.
-     *
-     * The method checks for the presence of the type {@link HiveDataTypes#HIVE_PROCESS} with the Atlas server.
-     * If this type is defined, then we assume the Hive DataModel is registered.
-     * @throws Exception
-     */
     public synchronized void registerHiveDataModel() throws Exception {
         HiveDataModelGenerator dataModelGenerator = new HiveDataModelGenerator();
         AtlasClient dgiClient = getAtlasClient();
 
-        try {
-            dgiClient.getType(FSDataTypes.HDFS_PATH().toString());
-            LOG.info("HDFS data model is already registered!");
-        } catch(AtlasServiceException ase) {
-            if (ase.getStatus() == ClientResponse.Status.NOT_FOUND) {
-                //Trigger val definition
-                FSDataModel.main(null);
-
-                final String hdfsModelJson = TypesSerialization.toJson(FSDataModel.typesDef());
-                //Expected in case types do not exist
-                LOG.info("Registering HDFS data model : " + hdfsModelJson);
-                dgiClient.createType(hdfsModelJson);
-            }
-        }
-
         try {
             dgiClient.getType(HiveDataTypes.HIVE_PROCESS.getName());
             LOG.info("Hive data model is already registered!");
         } catch(AtlasServiceException ase) {
-            if (ase.getStatus() == ClientResponse.Status.NOT_FOUND) {
-                //Expected in case types do not exist
-                LOG.info("Registering Hive data model");
-                dgiClient.createType(dataModelGenerator.getModelAsJson());
-            }
+            //Expected in case types do not exist
+            LOG.info("Registering Hive data model");
+            dgiClient.createType(dataModelGenerator.getModelAsJson());
         }
     }
 
     public static void main(String[] argv) throws Exception {
-
-        Configuration atlasConf = ApplicationProperties.get();
-        String atlasEndpoint = atlasConf.getString(ATLAS_ENDPOINT, DEFAULT_DGI_URL);
-        AtlasClient atlasClient;
-
-        if (!AuthenticationUtil.isKerberosAuthenticationEnabled()) {
-            String[] basicAuthUsernamePassword = AuthenticationUtil.getBasicAuthenticationInput();
-            atlasClient = new AtlasClient(new String[]{atlasEndpoint}, basicAuthUsernamePassword);
-        } else {
-            UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
-            atlasClient = new AtlasClient(ugi, ugi.getShortUserName(), atlasEndpoint);
-        }
-
-        HiveMetaStoreBridge hiveMetaStoreBridge = new HiveMetaStoreBridge(new HiveConf(), atlasClient);
+        Configuration atlasConf = ApplicationProperties.get(ApplicationProperties.CLIENT_PROPERTIES);
+        HiveMetaStoreBridge hiveMetaStoreBridge = new HiveMetaStoreBridge(new HiveConf(), atlasConf);
         hiveMetaStoreBridge.registerHiveDataModel();
         hiveMetaStoreBridge.importHiveMetadata();
     }