diff --git a/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java b/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
index a4d53303a..d7ffa2bd7 100755
--- a/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
+++ b/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
@@ -18,7 +18,6 @@
 
 package org.apache.atlas.hive.bridge;
 
-import org.apache.atlas.ApplicationProperties;
 import org.apache.atlas.AtlasClient;
 import org.apache.atlas.AtlasServiceException;
 import org.apache.atlas.hive.model.HiveDataModelGenerator;
@@ -26,20 +25,20 @@ import org.apache.atlas.hive.model.HiveDataTypes;
 import org.apache.atlas.typesystem.Referenceable;
 import org.apache.atlas.typesystem.Struct;
 import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.commons.configuration.Configuration;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.commons.lang.StringEscapeUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.Database;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Index;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.metastore.api.SerDeInfo;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.security.UserGroupInformation;
 import org.codehaus.jettison.json.JSONArray;
 import org.codehaus.jettison.json.JSONException;
 import org.codehaus.jettison.json.JSONObject;
@@ -48,38 +47,33 @@ import org.slf4j.LoggerFactory;
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Set;
 
 /**
  * A Bridge Utility that imports metadata from the Hive Meta Store
- * and registers then in Atlas.
+ * and registers then in DGI.
  */
 public class HiveMetaStoreBridge {
     private static final String DEFAULT_DGI_URL = "http://localhost:21000/";
-    public static final String HIVE_CLUSTER_NAME = "atlas.cluster.name";
+    public static final String HIVE_CLUSTER_NAME = "hive.cluster.name";
     public static final String DEFAULT_CLUSTER_NAME = "primary";
     private final String clusterName;
 
-    public static final String ATLAS_ENDPOINT = "atlas.rest.address";
+    public static final String DGI_URL_PROPERTY = "hive.hook.dgi.url";
 
     private static final Logger LOG = LoggerFactory.getLogger(HiveMetaStoreBridge.class);
 
-    public final Hive hiveClient;
+    private final Hive hiveClient;
     private final AtlasClient atlasClient;
 
-    public HiveMetaStoreBridge(HiveConf hiveConf, Configuration atlasConf) throws Exception {
-        this(hiveConf, atlasConf, null, null);
-    }
-
     /**
      * Construct a HiveMetaStoreBridge.
      * @param hiveConf hive conf
      */
-    public HiveMetaStoreBridge(HiveConf hiveConf, Configuration atlasConf, String doAsUser,
-                               UserGroupInformation ugi) throws Exception {
+    public HiveMetaStoreBridge(HiveConf hiveConf) throws Exception {
         clusterName = hiveConf.get(HIVE_CLUSTER_NAME, DEFAULT_CLUSTER_NAME);
         hiveClient = Hive.get(hiveConf);
-
-        atlasClient = new AtlasClient(atlasConf.getString(ATLAS_ENDPOINT, DEFAULT_DGI_URL), ugi, doAsUser);
+        atlasClient = new AtlasClient(hiveConf.get(DGI_URL_PROPERTY, DEFAULT_DGI_URL));
     }
 
     public AtlasClient getAtlasClient() {
@@ -96,86 +90,94 @@ public class HiveMetaStoreBridge {
         for (String databaseName : databases) {
             Referenceable dbReference = registerDatabase(databaseName);
 
-            importTables(dbReference, databaseName);
-        }
-    }
-
-    /**
-     * Creates db entity
-     * @param hiveDB
-     * @return
-     * @throws HiveException
-     */
-    public Referenceable createDBInstance(Database hiveDB) throws HiveException {
-        LOG.info("Importing objects from databaseName : " + hiveDB.getName());
-
-        Referenceable dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
-        String dbName = hiveDB.getName().toLowerCase();
-        dbRef.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, getDBQualifiedName(clusterName, dbName));
-        dbRef.set(HiveDataModelGenerator.NAME, dbName);
-        dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
-        dbRef.set("description", hiveDB.getDescription());
-        dbRef.set("locationUri", hiveDB.getLocationUri());
-        dbRef.set("parameters", hiveDB.getParameters());
-        dbRef.set("ownerName", hiveDB.getOwnerName());
-        if (hiveDB.getOwnerType() != null) {
-            dbRef.set("ownerType", hiveDB.getOwnerType().getValue());
+            importTables(databaseName, dbReference);
         }
-        return dbRef;
     }
 
-    /**
-     * Checks if db is already registered, else creates and registers db entity
-     * @param databaseName
-     * @return
-     * @throws Exception
-     */
-    private Referenceable registerDatabase(String databaseName) throws Exception {
-        Referenceable dbRef = getDatabaseReference(clusterName, databaseName);
+    public Referenceable registerDatabase(String databaseName) throws Exception {
+        Referenceable dbRef = getDatabaseReference(databaseName, clusterName);
         if (dbRef == null) {
-            Database db = hiveClient.getDatabase(databaseName);
-            dbRef = createDBInstance(db);
-            dbRef = registerInstance(dbRef);
+            LOG.info("Importing objects from databaseName : " + databaseName);
+            Database hiveDB = hiveClient.getDatabase(databaseName);
+
+            dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
+            dbRef.set(HiveDataModelGenerator.NAME, hiveDB.getName().toLowerCase());
+            dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
+            dbRef.set("description", hiveDB.getDescription());
+            dbRef.set("locationUri", hiveDB.getLocationUri());
+            dbRef.set("parameters", hiveDB.getParameters());
+            dbRef.set("ownerName", hiveDB.getOwnerName());
+            if (hiveDB.getOwnerType() != null) {
+                dbRef.set("ownerType", hiveDB.getOwnerType().getValue());
+            }
+
+            dbRef = createInstance(dbRef);
         } else {
             LOG.info("Database {} is already registered with id {}", databaseName, dbRef.getId().id);
         }
         return dbRef;
     }
 
-    /**
-     * Registers an entity in atlas
-     * @param referenceable
-     * @return
-     * @throws Exception
-     */
-    public Referenceable registerInstance(Referenceable referenceable) throws Exception {
+    public Referenceable createInstance(Referenceable referenceable) throws Exception {
         String typeName = referenceable.getTypeName();
         LOG.debug("creating instance of type " + typeName);
 
         String entityJSON = InstanceSerialization.toJson(referenceable, true);
         LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(), entityJSON);
-        JSONArray guids = atlasClient.createEntity(entityJSON);
-        LOG.debug("created instance for type " + typeName + ", guid: " + guids);
+        JSONObject jsonObject = atlasClient.createEntity(entityJSON);
+        String guid = jsonObject.getString(AtlasClient.GUID);
+        LOG.debug("created instance for type " + typeName + ", guid: " + guid);
 
-        return new Referenceable(guids.getString(0), referenceable.getTypeName(), null);
+        return new Referenceable(guid, referenceable.getTypeName(), null);
+    }
+
+    private void importTables(String databaseName, Referenceable databaseReferenceable) throws Exception {
+        List<String> hiveTables = hiveClient.getAllTables(databaseName);
+
+        for (String tableName : hiveTables) {
+            Referenceable tableReferenceable = registerTable(databaseReferenceable, databaseName, tableName);
+
+            // Import Partitions
+            Referenceable sdReferenceable = getSDForTable(databaseName, tableName);
+            registerPartitions(databaseName, tableName, tableReferenceable, sdReferenceable);
+
+            // Import Indexes
+            importIndexes(databaseName, tableName, databaseReferenceable, tableReferenceable);
+        }
     }
 
     /**
-     * Gets reference to the atlas entity for the database
+     * Gets reference for the database
+     *
+     *
      * @param databaseName  database Name
      * @param clusterName    cluster name
      * @return Reference for database if exists, else null
      * @throws Exception
      */
-    private Referenceable getDatabaseReference(String clusterName, String databaseName) throws Exception {
+    private Referenceable getDatabaseReference(String databaseName, String clusterName) throws Exception {
         LOG.debug("Getting reference for database {}", databaseName);
         String typeName = HiveDataTypes.HIVE_DB.getName();
 
-        String dslQuery = String.format("%s where %s = '%s' and %s = '%s'", typeName, HiveDataModelGenerator.NAME,
-                databaseName.toLowerCase(), HiveDataModelGenerator.CLUSTER_NAME, clusterName);
+        String dslQuery = String.format("%s where %s = '%s' and %s = '%s'", typeName,
+                HiveDataModelGenerator.NAME, databaseName.toLowerCase(), HiveDataModelGenerator.CLUSTER_NAME,
+                clusterName);
         return getEntityReferenceFromDSL(typeName, dslQuery);
     }
 
+    public Referenceable getProcessReference(String queryStr) throws Exception {
+        LOG.debug("Getting reference for process with query {}", queryStr);
+        String typeName = HiveDataTypes.HIVE_PROCESS.getName();
+
+        //todo enable DSL
+//        String dslQuery = String.format("%s where queryText = \"%s\"", typeName, queryStr);
+//        return getEntityReferenceFromDSL(typeName, dslQuery);
+
+        String gremlinQuery = String.format("g.V.has('__typeName', '%s').has('%s.queryText', \"%s\").toList()",
+                typeName, typeName, StringEscapeUtils.escapeJava(queryStr));
+        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
+    }
+
     private Referenceable getEntityReferenceFromDSL(String typeName, String dslQuery) throws Exception {
         AtlasClient dgiClient = getAtlasClient();
         JSONArray results = dgiClient.searchByDSL(dslQuery);
@@ -193,27 +195,8 @@ public class HiveMetaStoreBridge {
         }
     }
 
-    public static String getDBQualifiedName(String clusterName, String dbName) {
-        return String.format("%s.%s", clusterName, dbName.toLowerCase());
-    }
-
-    /**
-     * Imports all tables for the given db
-     * @param databaseName
-     * @param databaseReferenceable
-     * @throws Exception
-     */
-    private void importTables(Referenceable databaseReferenceable, String databaseName) throws Exception {
-        List<String> hiveTables = hiveClient.getAllTables(databaseName);
-
-        for (String tableName : hiveTables) {
-            Table table = hiveClient.getTable(databaseName, tableName);
-            Referenceable tableReferenceable = registerTable(databaseReferenceable, table);
-
-            // Import Partitions
-            Referenceable sdReferenceable = getSDForTable(databaseName, tableName);
-            registerPartitions(tableReferenceable, sdReferenceable, table);
-        }
+    public static String getTableName(String clusterName, String dbName, String tableName) {
+        return String.format("%s.%s@%s", dbName.toLowerCase(), tableName.toLowerCase(), clusterName);
     }
 
     /**
@@ -228,78 +211,17 @@ public class HiveMetaStoreBridge {
         LOG.debug("Getting reference for table {}.{}", dbName, tableName);
 
         String typeName = HiveDataTypes.HIVE_TABLE.getName();
-        String entityName = getTableQualifiedName(clusterName, dbName, tableName);
+        String entityName = getTableName(clusterName, dbName, tableName);
         String dslQuery = String.format("%s as t where name = '%s'", typeName, entityName);
         return getEntityReferenceFromDSL(typeName, dslQuery);
     }
 
-    public static String getTableQualifiedName(String clusterName, String dbName, String tableName) {
-        return String.format("%s.%s.%s", clusterName, dbName.toLowerCase(), tableName.toLowerCase());
-    }
-
-    public Referenceable createTableInstance(Referenceable dbReference, Table hiveTable)
-            throws Exception {
-        LOG.info("Importing objects from {}.{}", hiveTable.getDbName(), hiveTable.getTableName());
-
-        Referenceable tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
-        String tableQualifiedName = getTableQualifiedName(clusterName, hiveTable.getDbName(), hiveTable.getTableName());
-        tableRef.set(HiveDataModelGenerator.NAME, tableQualifiedName);
-        tableRef.set(HiveDataModelGenerator.TABLE_NAME, hiveTable.getTableName().toLowerCase());
-        tableRef.set("owner", hiveTable.getOwner());
-
-        tableRef.set("createTime", hiveTable.getMetadata().getProperty(hive_metastoreConstants.DDL_TIME));
-        tableRef.set("lastAccessTime", hiveTable.getLastAccessTime());
-        tableRef.set("retention", hiveTable.getRetention());
-
-        tableRef.set(HiveDataModelGenerator.COMMENT, hiveTable.getParameters().get(HiveDataModelGenerator.COMMENT));
-
-        // add reference to the database
-        tableRef.set(HiveDataModelGenerator.DB, dbReference);
-
-        tableRef.set("columns", getColumns(hiveTable.getCols(), tableQualifiedName));
-
-        // add reference to the StorageDescriptor
-        Referenceable sdReferenceable = fillStorageDescStruct(hiveTable.getSd(), tableQualifiedName, tableQualifiedName);
-        tableRef.set("sd", sdReferenceable);
-
-        // add reference to the Partition Keys
-        List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys(), tableQualifiedName);
-        tableRef.set("partitionKeys", partKeys);
-
-        tableRef.set("parameters", hiveTable.getParameters());
-
-        if (hiveTable.getViewOriginalText() != null) {
-            tableRef.set("viewOriginalText", hiveTable.getViewOriginalText());
-        }
-
-        if (hiveTable.getViewExpandedText() != null) {
-            tableRef.set("viewExpandedText", hiveTable.getViewExpandedText());
-        }
-
-        tableRef.set("tableType", hiveTable.getTableType().name());
-        tableRef.set("temporary", hiveTable.isTemporary());
-        return tableRef;
-    }
-
-    private Referenceable registerTable(Referenceable dbReference, Table table) throws Exception {
-        String dbName = table.getDbName();
-        String tableName = table.getTableName();
-        LOG.info("Attempting to register table [" + tableName + "]");
-        Referenceable tableRef = getTableReference(dbName, tableName);
-        if (tableRef == null) {
-            tableRef = createTableInstance(dbReference, table);
-            tableRef = registerInstance(tableRef);
-        } else {
-            LOG.info("Table {}.{} is already registered with id {}", dbName, tableName, tableRef.getId().id);
-        }
-        return tableRef;
-    }
-
-
-    private Referenceable getEntityReferenceFromGremlin(String typeName, String gremlinQuery)
-    throws AtlasServiceException, JSONException {
+    private Referenceable getEntityReferenceFromGremlin(String typeName, String gremlinQuery) throws
+    AtlasServiceException,
+    JSONException {
         AtlasClient client = getAtlasClient();
-        JSONArray results = client.searchByGremlin(gremlinQuery);
+        JSONObject response = client.searchByGremlin(gremlinQuery);
+        JSONArray results = response.getJSONArray(AtlasClient.RESULTS);
         if (results.length() == 0) {
             return null;
         }
@@ -314,12 +236,11 @@ public class HiveMetaStoreBridge {
 
         //todo replace gremlin with DSL
         //        String dslQuery = String.format("%s as p where values = %s, tableName where name = '%s', "
-        //                        + "dbName where name = '%s' and clusterName = '%s' select p", typeName, valuesStr,
-        // tableName,
+        //                        + "dbName where name = '%s' and clusterName = '%s' select p", typeName, valuesStr, tableName,
         //                dbName, clusterName);
 
         String datasetType = AtlasClient.DATA_SET_SUPER_TYPE;
-        String tableEntityName = getTableQualifiedName(clusterName, dbName, tableName);
+        String tableEntityName = getTableName(clusterName, dbName, tableName);
 
         String gremlinQuery = String.format("g.V.has('__typeName', '%s').has('%s.values', %s).as('p')."
                         + "out('__%s.table').has('%s.name', '%s').back('p').toList()", typeName, typeName, valuesStr,
@@ -336,24 +257,92 @@ public class HiveMetaStoreBridge {
 
         AtlasClient dgiClient = getAtlasClient();
         Referenceable tableInstance = dgiClient.getEntity(tableRef.getId().id);
-        Referenceable sd = (Referenceable) tableInstance.get("sd");
-        return new Referenceable(sd.getId().id, sd.getTypeName(), null);
+        Id sdId = (Id) tableInstance.get("sd");
+        return new Referenceable(sdId.id, sdId.getTypeName(), null);
     }
 
-    private void registerPartitions(Referenceable tableReferenceable, Referenceable sdReferenceable,
-                                    Table table) throws Exception {
-        String dbName = table.getDbName();
-        String tableName = table.getTableName();
-        LOG.info("Registering partitions for {}.{}", dbName, tableName);
-        List<Partition> tableParts = hiveClient.getPartitions(table);
+    public Referenceable registerTable(String dbName, String tableName) throws Exception {
+        Referenceable dbReferenceable = registerDatabase(dbName);
+        return registerTable(dbReferenceable, dbName, tableName);
+    }
+
+    public Referenceable registerTable(Referenceable dbReference, String dbName, String tableName) throws Exception {
+        LOG.info("Attempting to register table [" + tableName + "]");
+        Referenceable tableRef = getTableReference(dbName, tableName);
+        if (tableRef == null) {
+            LOG.info("Importing objects from " + dbName + "." + tableName);
+
+            Table hiveTable = hiveClient.getTable(dbName, tableName);
+
+            tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
+            tableRef.set(HiveDataModelGenerator.NAME,
+                    getTableName(clusterName, hiveTable.getDbName(), hiveTable.getTableName()));
+            tableRef.set(HiveDataModelGenerator.TABLE_NAME, hiveTable.getTableName().toLowerCase());
+            tableRef.set("owner", hiveTable.getOwner());
+
+            tableRef.set("createTime", hiveTable.getMetadata().getProperty(hive_metastoreConstants.DDL_TIME));
+            tableRef.set("lastAccessTime", hiveTable.getLastAccessTime());
+            tableRef.set("retention", hiveTable.getRetention());
+
+            tableRef.set(HiveDataModelGenerator.COMMENT, hiveTable.getParameters().get(HiveDataModelGenerator.COMMENT));
+
+            // add reference to the database
+            tableRef.set(HiveDataModelGenerator.DB, dbReference);
+
+            List<Referenceable> colList = getColumns(hiveTable.getCols());
+            tableRef.set("columns", colList);
+
+            // add reference to the StorageDescriptor
+            StorageDescriptor storageDesc = hiveTable.getSd();
+            Referenceable sdReferenceable = fillStorageDescStruct(storageDesc, colList);
+            tableRef.set("sd", sdReferenceable);
+
+            // add reference to the Partition Keys
+            List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys());
+            tableRef.set("partitionKeys", partKeys);
+
+            tableRef.set("parameters", hiveTable.getParameters());
+
+            if (hiveTable.getViewOriginalText() != null) {
+                tableRef.set("viewOriginalText", hiveTable.getViewOriginalText());
+            }
+
+            if (hiveTable.getViewExpandedText() != null) {
+                tableRef.set("viewExpandedText", hiveTable.getViewExpandedText());
+            }
+
+            tableRef.set("tableType", hiveTable.getTableType().name());
+            tableRef.set("temporary", hiveTable.isTemporary());
 
-        for (Partition hivePart : tableParts) {
-            registerPartition(tableReferenceable, sdReferenceable, hivePart);
+
+            tableRef = createInstance(tableRef);
+        } else {
+            LOG.info("Table {}.{} is already registered with id {}", dbName, tableName, tableRef.getId().id);
         }
+        return tableRef;
     }
 
-    private Referenceable registerPartition(Referenceable tableReferenceable, Referenceable sdReferenceable,
-                                            Partition hivePart) throws Exception {
+    private void registerPartitions(String db, String tableName, Referenceable tableReferenceable,
+            Referenceable sdReferenceable) throws Exception {
+        Set<Partition> tableParts = hiveClient.getAllPartitionsOf(new Table(Table.getEmptyTable(db, tableName)));
+
+        if (tableParts.size() > 0) {
+            for (Partition hivePart : tableParts) {
+                registerPartition(hivePart, tableReferenceable, sdReferenceable);
+            }
+        }
+    }
+
+    public Referenceable registerPartition(Partition partition) throws Exception {
+        String dbName = partition.getTable().getDbName();
+        String tableName = partition.getTable().getTableName();
+        Referenceable tableRef = registerTable(dbName, tableName);
+        Referenceable sdRef = getSDForTable(dbName, tableName);
+        return registerPartition(partition, tableRef, sdRef);
+    }
+
+    private Referenceable registerPartition(Partition hivePart, Referenceable tableReferenceable,
+            Referenceable sdReferenceable) throws Exception {
         LOG.info("Registering partition for {} with values {}", tableReferenceable,
                 StringUtils.join(hivePart.getValues(), ","));
         String dbName = hivePart.getTable().getDbName();
@@ -361,8 +350,22 @@ public class HiveMetaStoreBridge {
 
         Referenceable partRef = getPartitionReference(dbName, tableName, hivePart.getValues());
         if (partRef == null) {
-            partRef = createPartitionReferenceable(tableReferenceable, sdReferenceable, hivePart);
-            partRef = registerInstance(partRef);
+            partRef = new Referenceable(HiveDataTypes.HIVE_PARTITION.getName());
+            partRef.set("values", hivePart.getValues());
+
+            partRef.set(HiveDataModelGenerator.TABLE, tableReferenceable);
+
+            //todo fix
+            partRef.set("createTime", hivePart.getLastAccessTime());
+            partRef.set("lastAccessTime", hivePart.getLastAccessTime());
+
+            // sdStruct = fillStorageDescStruct(hivePart.getSd());
+            // Instead of creating copies of the sdstruct for partitions we are reusing existing
+            // ones will fix to identify partitions with differing schema.
+            partRef.set("sd", sdReferenceable);
+
+            partRef.set("parameters", hivePart.getParameters());
+            partRef = createInstance(partRef);
         } else {
             LOG.info("Partition {}.{} with values {} is already registered with id {}", dbName, tableName,
                     StringUtils.join(hivePart.getValues(), ","), partRef.getId().id);
@@ -370,38 +373,48 @@ public class HiveMetaStoreBridge {
         return partRef;
     }
 
-    public Referenceable createPartitionReferenceable(Referenceable tableReferenceable, Referenceable sdReferenceable,
-                                                      Partition hivePart) {
-        Referenceable partRef = new Referenceable(HiveDataTypes.HIVE_PARTITION.getName());
-        partRef.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, getPartitionQualifiedName(hivePart));
-        partRef.set("values", hivePart.getValues());
+    private void importIndexes(String db, String table,
+                               Referenceable dbReferenceable,
+                               Referenceable tableReferenceable) throws Exception {
+        List<Index> indexes = hiveClient.getIndexes(db, table, Short.MAX_VALUE);
+        if (indexes.size() > 0) {
+            for (Index index : indexes) {
+                importIndex(index, dbReferenceable, tableReferenceable);
+            }
+        }
+    }
 
-        partRef.set(HiveDataModelGenerator.TABLE, tableReferenceable);
+    //todo should be idempotent
+    private void importIndex(Index index,
+                             Referenceable dbReferenceable,
+                             Referenceable tableReferenceable) throws Exception {
+        LOG.info("Importing index {} for {}.{}", index.getIndexName(), dbReferenceable, tableReferenceable);
+        Referenceable indexRef = new Referenceable(HiveDataTypes.HIVE_INDEX.getName());
 
-        //todo fix
-        partRef.set("createTime", hivePart.getLastAccessTime());
-        partRef.set("lastAccessTime", hivePart.getLastAccessTime());
+        indexRef.set(HiveDataModelGenerator.NAME, index.getIndexName());
+        indexRef.set("indexHandlerClass", index.getIndexHandlerClass());
 
-        // sdStruct = fillStorageDescStruct(hivePart.getSd());
-        // Instead of creating copies of the sdstruct for partitions we are reusing existing
-        // ones will fix to identify partitions with differing schema.
-        partRef.set("sd", sdReferenceable);
+        indexRef.set(HiveDataModelGenerator.DB, dbReferenceable);
 
-        partRef.set("parameters", hivePart.getParameters());
-        return partRef;
-    }
+        indexRef.set("createTime", index.getCreateTime());
+        indexRef.set("lastAccessTime", index.getLastAccessTime());
+        indexRef.set("origTable", index.getOrigTableName());
+        indexRef.set("indexTable", index.getIndexTableName());
+
+        Referenceable sdReferenceable = fillStorageDescStruct(index.getSd(), null);
+        indexRef.set("sd", sdReferenceable);
+
+        indexRef.set("parameters", index.getParameters());
+
+        tableReferenceable.set("deferredRebuild", index.isDeferredRebuild());
 
-    private String getPartitionQualifiedName(Partition partition) {
-        return String.format("%s.%s.%s.%s", clusterName, partition.getTable().getDbName(),
-                partition.getTable().getTableName(), StringUtils.join(partition.getValues(), "/"));
+        createInstance(indexRef);
     }
 
-    private Referenceable fillStorageDescStruct(StorageDescriptor storageDesc, String tableQualifiedName,
-                                                String sdQualifiedName) throws Exception {
+    private Referenceable fillStorageDescStruct(StorageDescriptor storageDesc, List<Referenceable> colList) throws Exception {
         LOG.debug("Filling storage descriptor information for " + storageDesc);
 
         Referenceable sdReferenceable = new Referenceable(HiveDataTypes.HIVE_STORAGEDESC.getName());
-        sdReferenceable.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, sdQualifiedName);
 
         SerDeInfo serdeInfo = storageDesc.getSerdeInfo();
         LOG.debug("serdeInfo = " + serdeInfo);
@@ -416,13 +429,16 @@ public class HiveMetaStoreBridge {
 
         sdReferenceable.set("serdeInfo", serdeInfoStruct);
         sdReferenceable.set(HiveDataModelGenerator.STORAGE_NUM_BUCKETS, storageDesc.getNumBuckets());
-        sdReferenceable
-                .set(HiveDataModelGenerator.STORAGE_IS_STORED_AS_SUB_DIRS, storageDesc.isStoredAsSubDirectories());
+        sdReferenceable.set(HiveDataModelGenerator.STORAGE_IS_STORED_AS_SUB_DIRS, storageDesc.isStoredAsSubDirectories());
 
         //Use the passed column list if not null, ex: use same references for table and SD
         List<FieldSchema> columns = storageDesc.getCols();
         if (columns != null && !columns.isEmpty()) {
-            sdReferenceable.set("cols", getColumns(columns, tableQualifiedName));
+            if (colList != null) {
+                sdReferenceable.set("cols", colList);
+            } else {
+                sdReferenceable.set("cols", getColumns(columns));
+            }
         }
 
         List<Struct> sortColsStruct = new ArrayList<>();
@@ -450,25 +466,20 @@ public class HiveMetaStoreBridge {
         sdReferenceable.set("parameters", storageDesc.getParameters());
         sdReferenceable.set("storedAsSubDirectories", storageDesc.isStoredAsSubDirectories());
 
-        return sdReferenceable;
-    }
-
-    private String getColumnQualifiedName(String tableQualifiedName, String colName) {
-        return String.format("%s.%s", tableQualifiedName, colName);
+        return createInstance(sdReferenceable);
     }
 
-    private List<Referenceable> getColumns(List<FieldSchema> schemaList, String tableQualifiedName) throws Exception {
+    private List<Referenceable> getColumns(List<FieldSchema> schemaList) throws Exception
+    {
         List<Referenceable> colList = new ArrayList<>();
         for (FieldSchema fs : schemaList) {
             LOG.debug("Processing field " + fs);
             Referenceable colReferenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName());
-            colReferenceable.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME,
-                    getColumnQualifiedName(tableQualifiedName, fs.getName()));
             colReferenceable.set(HiveDataModelGenerator.NAME, fs.getName());
             colReferenceable.set("type", fs.getType());
             colReferenceable.set(HiveDataModelGenerator.COMMENT, fs.getComment());
 
-            colList.add(colReferenceable);
+            colList.add(createInstance(colReferenceable));
         }
         return colList;
     }
@@ -478,7 +489,7 @@ public class HiveMetaStoreBridge {
         AtlasClient dgiClient = getAtlasClient();
 
         //Register hive data model if its not already registered
-        if (dgiClient.getType(HiveDataTypes.HIVE_PROCESS.getName()) == null) {
+        if (dgiClient.getType(HiveDataTypes.HIVE_PROCESS.getName()) == null ) {
             LOG.info("Registering Hive data model");
             dgiClient.createType(dataModelGenerator.getModelAsJson());
         } else {
@@ -487,9 +498,16 @@ public class HiveMetaStoreBridge {
     }
 
     public static void main(String[] argv) throws Exception {
-        Configuration atlasConf = ApplicationProperties.get(ApplicationProperties.CLIENT_PROPERTIES);
-        HiveMetaStoreBridge hiveMetaStoreBridge = new HiveMetaStoreBridge(new HiveConf(), atlasConf);
+        HiveMetaStoreBridge hiveMetaStoreBridge = new HiveMetaStoreBridge(new HiveConf());
         hiveMetaStoreBridge.registerHiveDataModel();
         hiveMetaStoreBridge.importHiveMetadata();
     }
+
+    public void updateTable(Referenceable tableReferenceable, Table newTable) throws AtlasServiceException {
+        AtlasClient client = getAtlasClient();
+        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.TABLE_NAME,
+                newTable.getTableName().toLowerCase());
+        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.NAME,
+                getTableName(clusterName, newTable.getDbName(), newTable.getTableName()));
+    }
 }