diff --git a/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java b/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
index 51df8d265..0045780e3 100755
--- a/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
+++ b/addons/hive-bridge/src/main/java/org/apache/atlas/hive/bridge/HiveMetaStoreBridge.java
@@ -18,34 +18,27 @@
 
 package org.apache.atlas.hive.bridge;
 
-import com.google.common.annotations.VisibleForTesting;
 import com.sun.jersey.api.client.ClientResponse;
 import org.apache.atlas.ApplicationProperties;
-import org.apache.atlas.AtlasClientV2;
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasConstants;
 import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.hive.hook.events.BaseHiveEvent;
+import org.apache.atlas.fs.model.FSDataModel;
+import org.apache.atlas.fs.model.FSDataTypes;
+import org.apache.atlas.hive.model.HiveDataModelGenerator;
 import org.apache.atlas.hive.model.HiveDataTypes;
-import org.apache.atlas.hook.AtlasHookException;
-import org.apache.atlas.model.instance.AtlasEntityHeader;
-import org.apache.atlas.model.instance.EntityMutationResponse;
-import org.apache.atlas.model.instance.EntityMutations;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.Struct;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.json.TypesSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
 import org.apache.atlas.utils.AuthenticationUtil;
-import org.apache.atlas.utils.HdfsNameServiceResolver;
-import org.apache.atlas.model.instance.AtlasEntity;
-import org.apache.atlas.model.instance.AtlasEntity.AtlasEntityWithExtInfo;
-import org.apache.atlas.model.instance.AtlasEntity.AtlasEntitiesWithExtInfo;
-import org.apache.atlas.model.instance.AtlasObjectId;
-import org.apache.atlas.model.instance.AtlasStruct;
-import org.apache.commons.collections.CollectionUtils;
-
 import org.apache.commons.cli.BasicParser;
 import org.apache.commons.cli.CommandLine;
 import org.apache.commons.cli.CommandLineParser;
 import org.apache.commons.cli.Options;
-import org.apache.commons.collections.MapUtils;
 import org.apache.commons.configuration.Configuration;
 import org.apache.commons.lang.RandomStringUtils;
-import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.TableType;
@@ -59,733 +52,616 @@ import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.util.ArrayList;
-import java.util.Collections;
+import java.util.Date;
 import java.util.List;
-import java.util.Map;
-
-import static org.apache.atlas.hive.hook.events.BaseHiveEvent.*;
 
 /**
  * A Bridge Utility that imports metadata from the Hive Meta Store
  * and registers them in Atlas.
  */
-
 public class HiveMetaStoreBridge {
-    private static final Logger LOG = LoggerFactory.getLogger(HiveMetaStoreBridge.class);
-
-    public static final String CONF_PREFIX                     = "atlas.hook.hive.";
-    public static final String HIVE_CLUSTER_NAME               = "atlas.cluster.name";
-    public static final String HDFS_PATH_CONVERT_TO_LOWER_CASE = CONF_PREFIX + "hdfs_path.convert_to_lowercase";
-    public static final String DEFAULT_CLUSTER_NAME            = "primary";
-    public static final String TEMP_TABLE_PREFIX               = "_temp-";
-    public static final String ATLAS_ENDPOINT                  = "atlas.rest.address";
-    public static final String SEP                             = ":".intern();
-    public static final String HDFS_PATH                       = "hdfs_path";
+    private static final String DEFAULT_DGI_URL = "http://localhost:21000/";
+    public static final String HIVE_CLUSTER_NAME = "atlas.cluster.name";
+    public static final String DEFAULT_CLUSTER_NAME = "primary";
+    public static final String DESCRIPTION_ATTR = "description";
+    public static final String SEARCH_ENTRY_GUID_ATTR = "__guid";
 
-    private static final String DEFAULT_ATLAS_URL            = "http://localhost:21000/";
+    public static final String TEMP_TABLE_PREFIX = "_temp-";
 
-    private final HdfsNameServiceResolver hdfsNameServiceResolver = HdfsNameServiceResolver.getInstance();
-    private final String                  clusterName;
-    private final Hive                    hiveClient;
-    private final AtlasClientV2           atlasClientV2;
-    private final boolean                 convertHdfsPathToLowerCase;
-
-
-    public static void main(String[] args) throws AtlasHookException {
-        try {
-            Configuration atlasConf     = ApplicationProperties.get();
-            String[]      atlasEndpoint = atlasConf.getStringArray(ATLAS_ENDPOINT);
-
-            if (atlasEndpoint == null || atlasEndpoint.length == 0){
-                atlasEndpoint = new String[] { DEFAULT_ATLAS_URL };
-            }
+    private final String clusterName;
+    public static final long MILLIS_CONVERT_FACTOR = 1000;
 
-            AtlasClientV2 atlasClientV2;
+    public static final String ATLAS_ENDPOINT = "atlas.rest.address";
 
-            if (!AuthenticationUtil.isKerberosAuthenticationEnabled()) {
-                String[] basicAuthUsernamePassword = AuthenticationUtil.getBasicAuthenticationInput();
-
-                atlasClientV2 = new AtlasClientV2(atlasEndpoint, basicAuthUsernamePassword);
-            } else {
-                UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
-
-                atlasClientV2 = new AtlasClientV2(ugi, ugi.getShortUserName(), atlasEndpoint);
-            }
+    private static final Logger LOG = LoggerFactory.getLogger(HiveMetaStoreBridge.class);
 
-            Options           options     = new Options();
-            CommandLineParser parser      = new BasicParser();
-            CommandLine       cmd         = parser.parse(options, args);
-            boolean           failOnError = cmd.hasOption("failOnError");
+    public final Hive hiveClient;
+    private AtlasClient atlasClient = null;
 
-            HiveMetaStoreBridge hiveMetaStoreBridge = new HiveMetaStoreBridge(atlasConf, new HiveConf(), atlasClientV2);
+    HiveMetaStoreBridge(String clusterName, Hive hiveClient, AtlasClient atlasClient) {
+        this.clusterName = clusterName;
+        this.hiveClient = hiveClient;
+        this.atlasClient = atlasClient;
+    }
 
-            hiveMetaStoreBridge.importHiveMetadata(failOnError);
-        } catch(Exception e) {
-            throw new AtlasHookException("HiveMetaStoreBridge.main() failed.", e);
-        }
+    public String getClusterName() {
+        return clusterName;
     }
 
     /**
      * Construct a HiveMetaStoreBridge.
      * @param hiveConf {@link HiveConf} for Hive component in the cluster
      */
-    public HiveMetaStoreBridge(Configuration atlasProperties, HiveConf hiveConf, AtlasClientV2 atlasClientV2) throws Exception {
-        this(atlasProperties.getString(HIVE_CLUSTER_NAME, DEFAULT_CLUSTER_NAME), Hive.get(hiveConf), atlasClientV2, atlasProperties.getBoolean(HDFS_PATH_CONVERT_TO_LOWER_CASE, true));
+    public HiveMetaStoreBridge(HiveConf hiveConf) throws Exception {
+        this(hiveConf.get(HIVE_CLUSTER_NAME, DEFAULT_CLUSTER_NAME), Hive.get(hiveConf), null);
     }
 
     /**
      * Construct a HiveMetaStoreBridge.
      * @param hiveConf {@link HiveConf} for Hive component in the cluster
      */
-    public HiveMetaStoreBridge(Configuration atlasProperties, HiveConf hiveConf) throws Exception {
-        this(atlasProperties, hiveConf, null);
+    public HiveMetaStoreBridge(HiveConf hiveConf, AtlasClient atlasClient) throws Exception {
+        this(hiveConf.get(HIVE_CLUSTER_NAME, DEFAULT_CLUSTER_NAME), Hive.get(hiveConf), atlasClient);
     }
 
-    HiveMetaStoreBridge(String clusterName, Hive hiveClient, AtlasClientV2 atlasClientV2) {
-        this(clusterName, hiveClient, atlasClientV2, true);
+    AtlasClient getAtlasClient() {
+        return atlasClient;
     }
 
-    HiveMetaStoreBridge(String clusterName, Hive hiveClient, AtlasClientV2 atlasClientV2, boolean convertHdfsPathToLowerCase) {
-        this.clusterName                = clusterName;
-        this.hiveClient                 = hiveClient;
-        this.atlasClientV2              = atlasClientV2;
-        this.convertHdfsPathToLowerCase = convertHdfsPathToLowerCase;
-    }
-
-    public String getClusterName() {
-        return clusterName;
-    }
-
-    public Hive getHiveClient() {
-        return hiveClient;
-    }
-
-    public AtlasClientV2 getAtlasClient() {
-        return atlasClientV2;
-    }
-
-    public boolean isConvertHdfsPathToLowerCase() {
-        return convertHdfsPathToLowerCase;
-    }
-
-
-    @VisibleForTesting
-    public void importHiveMetadata(boolean failOnError) throws Exception {
-        LOG.info("Importing Hive metadata");
-
+    void importHiveMetadata(boolean failOnError) throws Exception {
+        LOG.info("Importing hive metadata");
         importDatabases(failOnError);
     }
 
     private void importDatabases(boolean failOnError) throws Exception {
         List<String> databases = hiveClient.getAllDatabases();
-
-        LOG.info("Found {} databases", databases.size());
-
         for (String databaseName : databases) {
-            AtlasEntityWithExtInfo dbEntity = registerDatabase(databaseName);
-
-            if (dbEntity != null) {
-                importTables(dbEntity.getEntity(), databaseName, failOnError);
-            }
+            Referenceable dbReference = registerDatabase(databaseName);
+            importTables(dbReference, databaseName, failOnError);
         }
     }
 
     /**
-     * Imports all tables for the given db
-     * @param dbEntity
+     * Create a Hive Database entity
+     * @param hiveDB The Hive {@link Database} object from which to map properties
+     * @return new Hive Database entity
+     * @throws HiveException
+     */
+    public Referenceable createDBInstance(Database hiveDB) throws HiveException {
+        return createOrUpdateDBInstance(hiveDB, null);
+    }
+
+    /**
+     * Checks if db is already registered, else creates and registers db entity
      * @param databaseName
-     * @param failOnError
+     * @return
      * @throws Exception
      */
-    private int importTables(AtlasEntity dbEntity, String databaseName, final boolean failOnError) throws Exception {
-        List<String> hiveTables = hiveClient.getAllTables(databaseName);
-
-        LOG.info("Found {} tables in database {}", hiveTables.size(), databaseName);
-
-        int tablesImported = 0;
-
-        try {
-            for (String tableName : hiveTables) {
-                int imported = importTable(dbEntity, databaseName, tableName, failOnError);
-
-                tablesImported += imported;
-            }
-        } finally {
-            if (tablesImported == hiveTables.size()) {
-                LOG.info("Successfully imported all {} tables from database {}", tablesImported, databaseName);
-            } else {
-                LOG.error("Imported {} of {} tables from database {}. Please check logs for errors during import", tablesImported, hiveTables.size(), databaseName);
-            }
+    private Referenceable registerDatabase(String databaseName) throws Exception {
+        Referenceable dbRef = getDatabaseReference(clusterName, databaseName);
+        Database db = hiveClient.getDatabase(databaseName);
+        if (dbRef == null) {
+            dbRef = createDBInstance(db);
+            dbRef = registerInstance(dbRef);
+        } else {
+            LOG.info("Database {} is already registered with id {}. Updating it.", databaseName, dbRef.getId().id);
+            dbRef = createOrUpdateDBInstance(db, dbRef);
+            updateInstance(dbRef);
         }
-
-        return tablesImported;
+        return dbRef;
     }
 
-    @VisibleForTesting
-    public int importTable(AtlasEntity dbEntity, String databaseName, String tableName, final boolean failOnError) throws Exception {
-        try {
-            Table                  table       = hiveClient.getTable(databaseName, tableName);
-            AtlasEntityWithExtInfo tableEntity = registerTable(dbEntity, table);
-
-            if (table.getTableType() == TableType.EXTERNAL_TABLE) {
-                String                 processQualifiedName = getTableProcessQualifiedName(clusterName, table);
-                AtlasEntityWithExtInfo processEntity        = findProcessEntity(processQualifiedName);
-
-                if (processEntity == null) {
-                    String      tableLocation = isConvertHdfsPathToLowerCase() ? lower(table.getDataLocation().toString()) : table.getDataLocation().toString();
-                    String      query         = getCreateTableString(table, tableLocation);
-                    AtlasEntity pathInst      = toHdfsPathEntity(tableLocation);
-                    AtlasEntity tableInst     = tableEntity.getEntity();
-                    AtlasEntity processInst   = new AtlasEntity(HiveDataTypes.HIVE_PROCESS.getName());
-                    long        now           = System.currentTimeMillis();
-
-                    processInst.setAttribute(ATTRIBUTE_QUALIFIED_NAME, processQualifiedName);
-                    processInst.setAttribute(ATTRIBUTE_NAME, query);
-                    processInst.setAttribute(ATTRIBUTE_CLUSTER_NAME, clusterName);
-                    processInst.setAttribute(ATTRIBUTE_INPUTS, Collections.singletonList(BaseHiveEvent.getObjectId(pathInst)));
-                    processInst.setAttribute(ATTRIBUTE_OUTPUTS, Collections.singletonList(BaseHiveEvent.getObjectId(tableInst)));
-                    processInst.setAttribute(ATTRIBUTE_USER_NAME, table.getOwner());
-                    processInst.setAttribute(ATTRIBUTE_START_TIME, now);
-                    processInst.setAttribute(ATTRIBUTE_END_TIME, now);
-                    processInst.setAttribute(ATTRIBUTE_OPERATION_TYPE, "CREATETABLE");
-                    processInst.setAttribute(ATTRIBUTE_QUERY_TEXT, query);
-                    processInst.setAttribute(ATTRIBUTE_QUERY_ID, query);
-                    processInst.setAttribute(ATTRIBUTE_QUERY_PLAN, "{}");
-                    processInst.setAttribute(ATTRIBUTE_RECENT_QUERIES, Collections.singletonList(query));
-
-                    AtlasEntitiesWithExtInfo createTableProcess = new AtlasEntitiesWithExtInfo();
-
-                    createTableProcess.addEntity(processInst);
-                    createTableProcess.addEntity(pathInst);
-
-                    registerInstances(createTableProcess);
-                } else {
-                    LOG.info("Process {} is already registered", processQualifiedName);
-                }
-            }
-
-            return 1;
-        } catch (Exception e) {
-            LOG.error("Import failed for hive_table {}", tableName, e);
-
-            if (failOnError) {
-                throw e;
-            }
+    private Referenceable createOrUpdateDBInstance(Database hiveDB, Referenceable dbRef) {
+        LOG.info("Importing objects from databaseName : " + hiveDB.getName());
 
-            return 0;
+        if (dbRef == null) {
+            dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
         }
+        String dbName = hiveDB.getName().toLowerCase();
+        dbRef.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, getDBQualifiedName(clusterName, dbName));
+        dbRef.set(AtlasClient.NAME, dbName);
+        dbRef.set(AtlasConstants.CLUSTER_NAME_ATTRIBUTE, clusterName);
+        dbRef.set(DESCRIPTION_ATTR, hiveDB.getDescription());
+        dbRef.set(HiveDataModelGenerator.LOCATION, hiveDB.getLocationUri());
+        dbRef.set(HiveDataModelGenerator.PARAMETERS, hiveDB.getParameters());
+        dbRef.set(AtlasClient.OWNER, hiveDB.getOwnerName());
+        if (hiveDB.getOwnerType() != null) {
+            dbRef.set("ownerType", hiveDB.getOwnerType().getValue());
+        }
+        return dbRef;
     }
 
     /**
-     * Checks if db is already registered, else creates and registers db entity
-     * @param databaseName
+     * Registers an entity in atlas
+     * @param referenceable
      * @return
      * @throws Exception
      */
-    private AtlasEntityWithExtInfo registerDatabase(String databaseName) throws Exception {
-        AtlasEntityWithExtInfo ret = null;
-        Database               db  = hiveClient.getDatabase(databaseName);
-
-        if (db != null) {
-            ret = findDatabase(clusterName, databaseName);
+    private Referenceable registerInstance(Referenceable referenceable) throws Exception {
+        String typeName = referenceable.getTypeName();
+        LOG.debug("creating instance of type " + typeName);
 
-            if (ret == null) {
-                ret = registerInstance(new AtlasEntityWithExtInfo(toDbEntity(db)));
-            } else {
-                LOG.info("Database {} is already registered - id={}. Updating it.", databaseName, ret.getEntity().getGuid());
+        String entityJSON = InstanceSerialization.toJson(referenceable, true);
+        LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(), entityJSON);
+        List<String> guids = getAtlasClient().createEntity(entityJSON);
+        LOG.debug("created instance for type " + typeName + ", guid: " + guids);
 
-                ret.setEntity(toDbEntity(db, ret.getEntity()));
+        return new Referenceable(guids.get(guids.size() - 1), referenceable.getTypeName(), null);
+    }
 
-                updateInstance(ret);
-            }
-        }
+    /**
+     * Gets reference to the atlas entity for the database
+     * @param databaseName  database Name
+     * @param clusterName    cluster name
+     * @return Reference for database if exists, else null
+     * @throws Exception
+     */
+    private Referenceable getDatabaseReference(String clusterName, String databaseName) throws Exception {
+        LOG.debug("Getting reference for database {}", databaseName);
+        String typeName = HiveDataTypes.HIVE_DB.getName();
 
-        return ret;
+        String dslQuery = getDatabaseDSLQuery(clusterName, databaseName, typeName);
+        return getEntityReferenceFromDSL(typeName, dslQuery);
     }
 
-    private AtlasEntityWithExtInfo registerTable(AtlasEntity dbEntity, Table table) throws AtlasHookException {
-        try {
-            AtlasEntityWithExtInfo ret;
-            AtlasEntityWithExtInfo tableEntity = findTableEntity(table);
-
-            if (tableEntity == null) {
-                tableEntity = toTableEntity(dbEntity, table);
+    static String getDatabaseDSLQuery(String clusterName, String databaseName, String typeName) {
+        return String.format("%s where %s = '%s' and %s = '%s'", typeName, AtlasClient.NAME,
+                databaseName.toLowerCase(), AtlasConstants.CLUSTER_NAME_ATTRIBUTE, clusterName);
+    }
 
-                ret = registerInstance(tableEntity);
+    private Referenceable getEntityReferenceFromDSL(String typeName, String dslQuery) throws Exception {
+        AtlasClient dgiClient = getAtlasClient();
+        JSONArray results = dgiClient.searchByDSL(dslQuery);
+        if (results.length() == 0) {
+            return null;
+        } else {
+            String guid;
+            JSONObject row = results.getJSONObject(0);
+            if (row.has("$id$")) {
+                guid = row.getJSONObject("$id$").getString("id");
             } else {
-                LOG.info("Table {}.{} is already registered with id {}. Updating entity.", table.getDbName(), table.getTableName(), tableEntity.getEntity().getGuid());
-
-                ret = toTableEntity(dbEntity, table, tableEntity);
-
-                updateInstance(ret);
+                guid = row.getJSONObject("_col_0").getString("id");
             }
-
-            return ret;
-        } catch (Exception e) {
-            throw new AtlasHookException("HiveMetaStoreBridge.registerTable() failed.", e);
+            return new Referenceable(guid, typeName, null);
         }
     }
 
     /**
-     * Registers an entity in atlas
-     * @param entity
-     * @return
-     * @throws Exception
+     * Construct the qualified name used to uniquely identify a Database instance in Atlas.
+     * @param clusterName Name of the cluster to which the Hive component belongs
+     * @param dbName Name of the Hive database
+     * @return Unique qualified name to identify the Database instance in Atlas.
      */
-    private AtlasEntityWithExtInfo registerInstance(AtlasEntityWithExtInfo entity) throws Exception {
-        if (LOG.isDebugEnabled()) {
-            LOG.debug("creating {} entity: {}", entity.getEntity().getTypeName(), entity);
-        }
-
-        AtlasEntityWithExtInfo  ret             = null;
-        EntityMutationResponse  response        = atlasClientV2.createEntity(entity);
-        List<AtlasEntityHeader> createdEntities = response.getEntitiesByOperation(EntityMutations.EntityOperation.CREATE);
-
-        if (CollectionUtils.isNotEmpty(createdEntities)) {
-            for (AtlasEntityHeader createdEntity : createdEntities) {
-                if (ret == null) {
-                    ret = atlasClientV2.getEntityByGuid(createdEntity.getGuid());
-
-                    LOG.info("Created {} entity: name={}, guid={}", ret.getEntity().getTypeName(), ret.getEntity().getAttribute(ATTRIBUTE_QUALIFIED_NAME), ret.getEntity().getGuid());
-                } else if (ret.getEntity(createdEntity.getGuid()) == null) {
-                    AtlasEntityWithExtInfo newEntity = atlasClientV2.getEntityByGuid(createdEntity.getGuid());
+    public static String getDBQualifiedName(String clusterName, String dbName) {
+        return String.format("%s@%s", dbName.toLowerCase(), clusterName);
+    }
 
-                    ret.addReferredEntity(newEntity.getEntity());
+    private String getCreateTableString(Table table, String location){
+        String colString = "";
+        List<FieldSchema> colList = table.getAllCols();
+        for(FieldSchema col:colList){
+            colString += col.getName()  + " " + col.getType() + ",";
+        }
+        colString = colString.substring(0, colString.length() - 1);
+        String query = "create external table " + table.getTableName() + "(" + colString + ")" +
+                " location '" + location + "'";
+        return query;
+    }
 
-                    if (MapUtils.isNotEmpty(newEntity.getReferredEntities())) {
-                        for (Map.Entry<String, AtlasEntity> entry : newEntity.getReferredEntities().entrySet()) {
-                            ret.addReferredEntity(entry.getKey(), entry.getValue());
-                        }
+    /**
+     * Imports all tables for the given db
+     * @param databaseReferenceable
+     * @param databaseName
+     * @param failOnError
+     * @throws Exception
+     */
+    private int importTables(Referenceable databaseReferenceable, String databaseName, final boolean failOnError) throws Exception {
+        int tablesImported = 0;
+        List<String> hiveTables = hiveClient.getAllTables(databaseName);
+        LOG.info("Importing tables {} for db {}", hiveTables.toString(), databaseName);
+        for (String tableName : hiveTables) {
+            try {
+                Table table = hiveClient.getTable(databaseName, tableName);
+                Referenceable tableReferenceable = registerTable(databaseReferenceable, table);
+                tablesImported++;
+                if (table.getTableType() == TableType.EXTERNAL_TABLE) {
+                    String tableQualifiedName = getTableQualifiedName(clusterName, table);
+                    Referenceable process = getProcessReference(tableQualifiedName);
+                    if (process == null) {
+                        LOG.info("Attempting to register create table process for {}", tableQualifiedName);
+                        Referenceable lineageProcess = new Referenceable(HiveDataTypes.HIVE_PROCESS.getName());
+                        ArrayList<Referenceable> sourceList = new ArrayList<>();
+                        ArrayList<Referenceable> targetList = new ArrayList<>();
+                        String tableLocation = table.getDataLocation().toString();
+                        Referenceable path = fillHDFSDataSet(tableLocation);
+                        String query = getCreateTableString(table, tableLocation);
+                        sourceList.add(path);
+                        targetList.add(tableReferenceable);
+                        lineageProcess.set("inputs", sourceList);
+                        lineageProcess.set("outputs", targetList);
+                        lineageProcess.set("userName", table.getOwner());
+                        lineageProcess.set("startTime", new Date(System.currentTimeMillis()));
+                        lineageProcess.set("endTime", new Date(System.currentTimeMillis()));
+                        lineageProcess.set("operationType", "CREATETABLE");
+                        lineageProcess.set("queryText", query);
+                        lineageProcess.set("queryId", query);
+                        lineageProcess.set("queryPlan", "{}");
+                        lineageProcess.set("clusterName", clusterName);
+                        List<String> recentQueries = new ArrayList<>(1);
+                        recentQueries.add(query);
+                        lineageProcess.set("recentQueries", recentQueries);
+                        lineageProcess.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, tableQualifiedName);
+                        lineageProcess.set(AtlasClient.NAME, query);
+                        registerInstance(lineageProcess);
+                    } else {
+                        LOG.info("Process {} is already registered", process.toString());
                     }
-
-                    LOG.info("Created {} entity: name={}, guid={}", newEntity.getEntity().getTypeName(), newEntity.getEntity().getAttribute(ATTRIBUTE_QUALIFIED_NAME), newEntity.getEntity().getGuid());
+                }
+            } catch (Exception e) {
+                LOG.error("Import failed for hive_table {} ", tableName, e);
+                if (failOnError) {
+                    throw e;
                 }
             }
         }
 
-        return ret;
+        if ( tablesImported == hiveTables.size()) {
+            LOG.info("Successfully imported all {} tables from {} ", tablesImported, databaseName);
+        } else {
+            LOG.error("Unable to import {} tables out of {} tables from {}", tablesImported, hiveTables.size(), databaseName);
+        }
+
+        return tablesImported;
     }
 
     /**
-     * Registers an entity in atlas
-     * @param entities
-     * @return
+     * Gets reference for the table
+     *
+     * @param hiveTable
+     * @return table reference if exists, else null
      * @throws Exception
      */
-    private AtlasEntitiesWithExtInfo registerInstances(AtlasEntitiesWithExtInfo entities) throws Exception {
-        if (LOG.isDebugEnabled()) {
-            LOG.debug("creating {} entities: {}", entities.getEntities().size(), entities);
-        }
-
-        AtlasEntitiesWithExtInfo ret = null;
-        EntityMutationResponse   response        = atlasClientV2.createEntities(entities);
-        List<AtlasEntityHeader>  createdEntities = response.getEntitiesByOperation(EntityMutations.EntityOperation.CREATE);
+    private Referenceable getTableReference(Table hiveTable)  throws Exception {
+        LOG.debug("Getting reference for table {}.{}", hiveTable.getDbName(), hiveTable.getTableName());
 
-        if (CollectionUtils.isNotEmpty(createdEntities)) {
-            ret = new AtlasEntitiesWithExtInfo();
+        String typeName = HiveDataTypes.HIVE_TABLE.getName();
+        String dslQuery = getTableDSLQuery(getClusterName(), hiveTable.getDbName(), hiveTable.getTableName(), typeName, hiveTable.isTemporary());
+        return getEntityReferenceFromDSL(typeName, dslQuery);
+    }
 
-            for (AtlasEntityHeader createdEntity : createdEntities) {
-                AtlasEntityWithExtInfo entity = atlasClientV2.getEntityByGuid(createdEntity.getGuid());
+    private Referenceable getProcessReference(String qualifiedName) throws Exception{
+        LOG.debug("Getting reference for process {}", qualifiedName);
+        String typeName = HiveDataTypes.HIVE_PROCESS.getName();
+        String dslQuery = getProcessDSLQuery(typeName, qualifiedName);
+        return getEntityReferenceFromDSL(typeName, dslQuery);
+    }
 
-                ret.addEntity(entity.getEntity());
+    static String getProcessDSLQuery(String typeName, String qualifiedName) throws Exception{
+        String dslQuery = String.format("%s as t where qualifiedName = '%s'", typeName, qualifiedName);
+        return dslQuery;
+    }
 
-                if (MapUtils.isNotEmpty(entity.getReferredEntities())) {
-                    for (Map.Entry<String, AtlasEntity> entry : entity.getReferredEntities().entrySet()) {
-                        ret.addReferredEntity(entry.getKey(), entry.getValue());
-                    }
-                }
+    static String getTableDSLQuery(String clusterName, String dbName, String tableName, String typeName, boolean isTemporary) {
+        String entityName = getTableQualifiedName(clusterName, dbName, tableName, isTemporary);
+        return String.format("%s as t where qualifiedName = '%s'", typeName, entityName);
+    }
 
-                LOG.info("Created {} entity: name={}, guid={}", entity.getEntity().getTypeName(), entity.getEntity().getAttribute(ATTRIBUTE_QUALIFIED_NAME), entity.getEntity().getGuid());
+    /**
+     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
+     * @param clusterName Name of the cluster to which the Hive component belongs
+     * @param dbName Name of the Hive database to which the Table belongs
+     * @param tableName Name of the Hive table
+     * @return Unique qualified name to identify the Table instance in Atlas.
+     */
+    public static String getTableQualifiedName(String clusterName, String dbName, String tableName, boolean isTemporaryTable) {
+        String tableTempName = tableName;
+        if (isTemporaryTable) {
+            if (SessionState.get().getSessionId() != null) {
+                tableTempName = tableName + TEMP_TABLE_PREFIX + SessionState.get().getSessionId();
+            } else {
+                tableTempName = tableName + TEMP_TABLE_PREFIX + RandomStringUtils.random(10);
             }
         }
-
-        return ret;
+        return String.format("%s.%s@%s", dbName.toLowerCase(), tableTempName.toLowerCase(), clusterName);
     }
 
-    private void updateInstance(AtlasEntityWithExtInfo entity) throws AtlasServiceException {
-        if (LOG.isDebugEnabled()) {
-            LOG.debug("updating {} entity: {}", entity.getEntity().getTypeName(), entity);
-        }
 
-        atlasClientV2.updateEntity(entity);
 
-        LOG.info("Updated {} entity: name={}, guid={}", entity.getEntity().getTypeName(), entity.getEntity().getAttribute(ATTRIBUTE_QUALIFIED_NAME), entity.getEntity().getGuid());
+    /**
+     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
+     * @param clusterName Name of the cluster to which the Hive component belongs
+     * @param table hive table for which the qualified name is needed
+     * @return Unique qualified name to identify the Table instance in Atlas.
+     */
+    public static String getTableQualifiedName(String clusterName, Table table) {
+        return getTableQualifiedName(clusterName, table.getDbName(), table.getTableName(), table.isTemporary());
     }
 
     /**
-     * Create a Hive Database entity
-     * @param hiveDB The Hive {@link Database} object from which to map properties
-     * @return new Hive Database AtlasEntity
-     * @throws HiveException
+     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
+     * @param clusterName Name of the cluster to which the Hive component belongs
+     * @param dbName Name of the Hive database to which the Table belongs
+     * @param tableName Name of the Hive table
+     * @return Unique qualified name to identify the Table instance in Atlas.
      */
-    private AtlasEntity toDbEntity(Database hiveDB) throws HiveException {
-        return toDbEntity(hiveDB, null);
+    public static String getTableQualifiedName(String clusterName, String dbName, String tableName) {
+         return getTableQualifiedName(clusterName, dbName, tableName, false);
     }
 
-    private AtlasEntity toDbEntity(Database hiveDB, AtlasEntity dbEntity) {
-        if (dbEntity == null) {
-            dbEntity = new AtlasEntity(HiveDataTypes.HIVE_DB.getName());
-        }
-
-        String dbName = hiveDB.getName().toLowerCase();
-
-        dbEntity.setAttribute(ATTRIBUTE_QUALIFIED_NAME, getDBQualifiedName(clusterName, dbName));
-        dbEntity.setAttribute(ATTRIBUTE_NAME, dbName);
-        dbEntity.setAttribute(ATTRIBUTE_DESCRIPTION, hiveDB.getDescription());
-        dbEntity.setAttribute(ATTRIBUTE_OWNER, hiveDB.getOwnerName());
-
-        dbEntity.setAttribute(ATTRIBUTE_CLUSTER_NAME, clusterName);
-        dbEntity.setAttribute(ATTRIBUTE_LOCATION, hdfsNameServiceResolver.getPathWithNameServiceID(hiveDB.getLocationUri()));
-        dbEntity.setAttribute(ATTRIBUTE_PARAMETERS, hiveDB.getParameters());
-
-        if (hiveDB.getOwnerType() != null) {
-            dbEntity.setAttribute(ATTRIBUTE_OWNER_TYPE, OWNER_TYPE_TO_ENUM_VALUE.get(hiveDB.getOwnerType().getValue()));
-        }
-
-        return dbEntity;
-    }
     /**
      * Create a new table instance in Atlas
-     * @param  database AtlasEntity for Hive  {@link AtlasEntity} to which this table belongs
+     * @param dbReference reference to a created Hive database {@link Referenceable} to which this table belongs
      * @param hiveTable reference to the Hive {@link Table} from which to map properties
-     * @return Newly created Hive AtlasEntity
+     * @return Newly created Hive reference
      * @throws Exception
      */
-    private AtlasEntityWithExtInfo toTableEntity(AtlasEntity database, Table hiveTable) throws AtlasHookException {
-        return toTableEntity(database, hiveTable, null);
+    public Referenceable createTableInstance(Referenceable dbReference, Table hiveTable)
+            throws Exception {
+        return createOrUpdateTableInstance(dbReference, null, hiveTable);
     }
 
-    private AtlasEntityWithExtInfo toTableEntity(AtlasEntity database, final Table hiveTable, AtlasEntityWithExtInfo table) throws AtlasHookException {
-        if (table == null) {
-            table = new AtlasEntityWithExtInfo(new AtlasEntity(HiveDataTypes.HIVE_TABLE.getName()));
-        }
-
-        AtlasEntity tableEntity        = table.getEntity();
-        String      tableQualifiedName = getTableQualifiedName(clusterName, hiveTable);
-        long        createTime         = BaseHiveEvent.getTableCreateTime(hiveTable);
-        long        lastAccessTime     = hiveTable.getLastAccessTime() > 0 ? hiveTable.getLastAccessTime() : createTime;
-
-        tableEntity.setAttribute(ATTRIBUTE_DB, BaseHiveEvent.getObjectId(database));
-        tableEntity.setAttribute(ATTRIBUTE_QUALIFIED_NAME, tableQualifiedName);
-        tableEntity.setAttribute(ATTRIBUTE_NAME, hiveTable.getTableName().toLowerCase());
-        tableEntity.setAttribute(ATTRIBUTE_OWNER, hiveTable.getOwner());
-
-        tableEntity.setAttribute(ATTRIBUTE_CREATE_TIME, createTime);
-        tableEntity.setAttribute(ATTRIBUTE_LAST_ACCESS_TIME, lastAccessTime);
-        tableEntity.setAttribute(ATTRIBUTE_RETENTION, hiveTable.getRetention());
-        tableEntity.setAttribute(ATTRIBUTE_PARAMETERS, hiveTable.getParameters());
-        tableEntity.setAttribute(ATTRIBUTE_COMMENT, hiveTable.getParameters().get(ATTRIBUTE_COMMENT));
-        tableEntity.setAttribute(ATTRIBUTE_TABLE_TYPE, hiveTable.getTableType().name());
-        tableEntity.setAttribute(ATTRIBUTE_TEMPORARY, hiveTable.isTemporary());
-
-        if (hiveTable.getViewOriginalText() != null) {
-            tableEntity.setAttribute(ATTRIBUTE_VIEW_ORIGINAL_TEXT, hiveTable.getViewOriginalText());
-        }
-
-        if (hiveTable.getViewExpandedText() != null) {
-            tableEntity.setAttribute(ATTRIBUTE_VIEW_EXPANDED_TEXT, hiveTable.getViewExpandedText());
-        }
-
-        AtlasEntity       sdEntity = toStroageDescEntity(hiveTable.getSd(), tableQualifiedName, getStorageDescQFName(tableQualifiedName), BaseHiveEvent.getObjectId(tableEntity));
-        List<AtlasEntity> partKeys = toColumns(hiveTable.getPartitionKeys(), tableEntity);
-        List<AtlasEntity> columns  = toColumns(hiveTable.getCols(), tableEntity);
-
-        tableEntity.setAttribute(ATTRIBUTE_STORAGEDESC, BaseHiveEvent.getObjectId(sdEntity));
-        tableEntity.setAttribute(ATTRIBUTE_PARTITION_KEYS, BaseHiveEvent.getObjectIds(partKeys));
-        tableEntity.setAttribute(ATTRIBUTE_COLUMNS, BaseHiveEvent.getObjectIds(columns));
+    private Referenceable createOrUpdateTableInstance(Referenceable dbReference, Referenceable tableReference,
+                                                      final Table hiveTable) throws Exception {
+        LOG.info("Importing objects from {}.{}", hiveTable.getDbName(), hiveTable.getTableName());
 
-        if (MapUtils.isNotEmpty(table.getReferredEntities())) {
-            table.getReferredEntities().clear();
+        if (tableReference == null) {
+            tableReference = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
         }
 
-        table.addReferredEntity(database);
-        table.addReferredEntity(sdEntity);
-
-        if (partKeys != null) {
-            for (AtlasEntity partKey : partKeys) {
-                table.addReferredEntity(partKey);
+        String tableQualifiedName = getTableQualifiedName(clusterName, hiveTable);
+        tableReference.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, tableQualifiedName);
+        tableReference.set(AtlasClient.NAME, hiveTable.getTableName().toLowerCase());
+        tableReference.set(AtlasClient.OWNER, hiveTable.getOwner());
+
+        Date createDate = new Date();
+        if (hiveTable.getTTable() != null){
+            try {
+                createDate = new Date(hiveTable.getTTable().getCreateTime() * MILLIS_CONVERT_FACTOR);
+                LOG.debug("Setting create time to {} ", createDate);
+                tableReference.set(HiveDataModelGenerator.CREATE_TIME, createDate);
+            } catch(NumberFormatException ne) {
+                LOG.error("Error while updating createTime for the table {} ", hiveTable.getCompleteName(), ne);
             }
         }
 
-        if (columns != null) {
-            for (AtlasEntity column : columns) {
-                table.addReferredEntity(column);
-            }
-        }
-
-        return table;
-    }
-
-    private AtlasEntity toStroageDescEntity(StorageDescriptor storageDesc, String tableQualifiedName, String sdQualifiedName, AtlasObjectId tableId ) throws AtlasHookException {
-        AtlasEntity ret = new AtlasEntity(HiveDataTypes.HIVE_STORAGEDESC.getName());
-
-        ret.setAttribute(ATTRIBUTE_TABLE, tableId);
-        ret.setAttribute(ATTRIBUTE_QUALIFIED_NAME, sdQualifiedName);
-        ret.setAttribute(ATTRIBUTE_PARAMETERS, storageDesc.getParameters());
-        ret.setAttribute(ATTRIBUTE_LOCATION, hdfsNameServiceResolver.getPathWithNameServiceID(storageDesc.getLocation()));
-        ret.setAttribute(ATTRIBUTE_INPUT_FORMAT, storageDesc.getInputFormat());
-        ret.setAttribute(ATTRIBUTE_OUTPUT_FORMAT, storageDesc.getOutputFormat());
-        ret.setAttribute(ATTRIBUTE_COMPRESSED, storageDesc.isCompressed());
-        ret.setAttribute(ATTRIBUTE_NUM_BUCKETS, storageDesc.getNumBuckets());
-        ret.setAttribute(ATTRIBUTE_STORED_AS_SUB_DIRECTORIES, storageDesc.isStoredAsSubDirectories());
-
-        if (storageDesc.getBucketCols().size() > 0) {
-            ret.setAttribute(ATTRIBUTE_BUCKET_COLS, storageDesc.getBucketCols());
+        Date lastAccessTime = createDate;
+        if ( hiveTable.getLastAccessTime() > 0) {
+            lastAccessTime = new Date(hiveTable.getLastAccessTime() * MILLIS_CONVERT_FACTOR);
         }
+        tableReference.set(HiveDataModelGenerator.LAST_ACCESS_TIME, lastAccessTime);
+        tableReference.set("retention", hiveTable.getRetention());
 
-        if (storageDesc.getSerdeInfo() != null) {
-            SerDeInfo serdeInfo = storageDesc.getSerdeInfo();
+        tableReference.set(HiveDataModelGenerator.COMMENT, hiveTable.getParameters().get(HiveDataModelGenerator.COMMENT));
 
-            LOG.debug("serdeInfo = {}", serdeInfo);
-            // SkewedInfo skewedInfo = storageDesc.getSkewedInfo();
+        // add reference to the database
+        tableReference.set(HiveDataModelGenerator.DB, dbReference);
 
-            AtlasStruct serdeInfoStruct = new AtlasStruct(HiveDataTypes.HIVE_SERDE.getName());
+        // add reference to the StorageDescriptor
+        Referenceable sdReferenceable = fillStorageDesc(hiveTable.getSd(), tableQualifiedName, getStorageDescQFName(tableQualifiedName), tableReference.getId());
+        tableReference.set(HiveDataModelGenerator.STORAGE_DESC, sdReferenceable);
 
-            serdeInfoStruct.setAttribute(ATTRIBUTE_NAME, serdeInfo.getName());
-            serdeInfoStruct.setAttribute(ATTRIBUTE_SERIALIZATION_LIB, serdeInfo.getSerializationLib());
-            serdeInfoStruct.setAttribute(ATTRIBUTE_PARAMETERS, serdeInfo.getParameters());
+        tableReference.set(HiveDataModelGenerator.PARAMETERS, hiveTable.getParameters());
 
-            ret.setAttribute(ATTRIBUTE_SERDE_INFO, serdeInfoStruct);
+        if (hiveTable.getViewOriginalText() != null) {
+            tableReference.set("viewOriginalText", hiveTable.getViewOriginalText());
         }
 
-        if (CollectionUtils.isNotEmpty(storageDesc.getSortCols())) {
-            List<AtlasStruct> sortColsStruct = new ArrayList<>();
-
-            for (Order sortcol : storageDesc.getSortCols()) {
-                String hiveOrderName = HiveDataTypes.HIVE_ORDER.getName();
-                AtlasStruct colStruct = new AtlasStruct(hiveOrderName);
-                colStruct.setAttribute("col", sortcol.getCol());
-                colStruct.setAttribute("order", sortcol.getOrder());
-
-                sortColsStruct.add(colStruct);
-            }
-
-            ret.setAttribute(ATTRIBUTE_SORT_COLS, sortColsStruct);
+        if (hiveTable.getViewExpandedText() != null) {
+            tableReference.set("viewExpandedText", hiveTable.getViewExpandedText());
         }
 
-        return ret;
-    }
-
-    private List<AtlasEntity> toColumns(List<FieldSchema> schemaList, AtlasEntity table) throws AtlasHookException {
-        List<AtlasEntity> ret = new ArrayList<>();
-
-        int columnPosition = 0;
-        for (FieldSchema fs : schemaList) {
-            LOG.debug("Processing field {}", fs);
+        tableReference.set(HiveDataModelGenerator.TABLE_TYPE_ATTR, hiveTable.getTableType().name());
+        tableReference.set("temporary", hiveTable.isTemporary());
 
-            AtlasEntity column = new AtlasEntity(HiveDataTypes.HIVE_COLUMN.getName());
+        // add reference to the Partition Keys
+        List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys(), tableReference);
+        tableReference.set("partitionKeys", partKeys);
 
-            column.setAttribute(ATTRIBUTE_TABLE, BaseHiveEvent.getObjectId(table));
-            column.setAttribute(ATTRIBUTE_QUALIFIED_NAME, getColumnQualifiedName((String) table.getAttribute(ATTRIBUTE_QUALIFIED_NAME), fs.getName()));
-            column.setAttribute(ATTRIBUTE_NAME, fs.getName());
-            column.setAttribute(ATTRIBUTE_OWNER, table.getAttribute(ATTRIBUTE_OWNER));
-            column.setAttribute(ATTRIBUTE_COL_TYPE, fs.getType());
-            column.setAttribute(ATTRIBUTE_COL_POSITION, columnPosition++);
-            column.setAttribute(ATTRIBUTE_COMMENT, fs.getComment());
+        tableReference.set(HiveDataModelGenerator.COLUMNS, getColumns(hiveTable.getCols(), tableReference));
 
-            ret.add(column);
-        }
-        return ret;
+        return tableReference;
     }
 
-    private AtlasEntity toHdfsPathEntity(String pathUri) {
-        AtlasEntity ret           = new AtlasEntity(HDFS_PATH);
-        String      nameServiceID = hdfsNameServiceResolver.getNameServiceIDForPath(pathUri);
-        Path        path          = new Path(pathUri);
-
-        ret.setAttribute(ATTRIBUTE_NAME, Path.getPathWithoutSchemeAndAuthority(path).toString().toLowerCase());
-        ret.setAttribute(ATTRIBUTE_CLUSTER_NAME, clusterName);
-
-        if (StringUtils.isNotEmpty(nameServiceID)) {
-            // Name service resolution is successful, now get updated HDFS path where the host port info is replaced by resolved name service
-            String updatedHdfsPath = hdfsNameServiceResolver.getPathWithNameServiceID(pathUri);
+    public static String getStorageDescQFName(String entityQualifiedName) {
+        return entityQualifiedName + "_storage";
+    }
 
-            ret.setAttribute(ATTRIBUTE_PATH, updatedHdfsPath);
-            ret.setAttribute(ATTRIBUTE_QUALIFIED_NAME, getHdfsPathQualifiedName(updatedHdfsPath));
-            ret.setAttribute(ATTRIBUTE_NAMESERVICE_ID, nameServiceID);
+    private Referenceable registerTable(Referenceable dbReference, Table table) throws Exception {
+        String dbName = table.getDbName();
+        String tableName = table.getTableName();
+        LOG.info("Attempting to register table [" + tableName + "]");
+        Referenceable tableReference = getTableReference(table);
+        LOG.info("Found result " + tableReference);
+        if (tableReference == null) {
+            tableReference = createTableInstance(dbReference, table);
+            tableReference = registerInstance(tableReference);
         } else {
-            ret.setAttribute(ATTRIBUTE_PATH, pathUri);
-
-            // Only append clusterName for the HDFS path
-            if (pathUri.startsWith(HdfsNameServiceResolver.HDFS_SCHEME)) {
-                ret.setAttribute(ATTRIBUTE_QUALIFIED_NAME, getHdfsPathQualifiedName(pathUri));
-            } else {
-                ret.setAttribute(ATTRIBUTE_QUALIFIED_NAME, pathUri);
-            }
+            LOG.info("Table {}.{} is already registered with id {}. Updating entity.", dbName, tableName,
+                    tableReference.getId().id);
+            tableReference = createOrUpdateTableInstance(dbReference, tableReference, table);
+            updateInstance(tableReference);
         }
-
-        return ret;
+        return tableReference;
     }
 
-    /**
-     * Gets the atlas entity for the database
-     * @param databaseName  database Name
-     * @param clusterName    cluster name
-     * @return AtlasEntity for database if exists, else null
-     * @throws Exception
-     */
-    private AtlasEntityWithExtInfo findDatabase(String clusterName, String databaseName) throws Exception {
-        if (LOG.isDebugEnabled()) {
-            LOG.debug("Searching Atlas for database {}", databaseName);
-        }
+    private void updateInstance(Referenceable referenceable) throws AtlasServiceException {
+        String typeName = referenceable.getTypeName();
+        LOG.debug("updating instance of type " + typeName);
 
-        String typeName = HiveDataTypes.HIVE_DB.getName();
+        String entityJSON = InstanceSerialization.toJson(referenceable, true);
+        LOG.debug("Updating entity {} = {}", referenceable.getTypeName(), entityJSON);
 
-        return findEntity(typeName, getDBQualifiedName(clusterName, databaseName));
+        atlasClient.updateEntity(referenceable.getId().id, referenceable);
     }
 
-    /**
-     * Gets Atlas Entity for the table
-     *
-     * @param hiveTable
-     * @return table entity from Atlas  if exists, else null
-     * @throws Exception
-     */
-    private AtlasEntityWithExtInfo findTableEntity(Table hiveTable)  throws Exception {
-        if (LOG.isDebugEnabled()) {
-            LOG.debug("Searching Atlas for table {}.{}", hiveTable.getDbName(), hiveTable.getTableName());
+    private Referenceable getEntityReferenceFromGremlin(String typeName, String gremlinQuery)
+    throws AtlasServiceException, JSONException {
+        AtlasClient client = getAtlasClient();
+        JSONArray results = client.searchByGremlin(gremlinQuery);
+        if (results.length() == 0) {
+            return null;
         }
+        String guid = results.getJSONObject(0).getString(SEARCH_ENTRY_GUID_ATTR);
+        return new Referenceable(guid, typeName, null);
+    }
 
-        String typeName         = HiveDataTypes.HIVE_TABLE.getName();
-        String tblQualifiedName = getTableQualifiedName(getClusterName(), hiveTable.getDbName(), hiveTable.getTableName());
+    public Referenceable fillStorageDesc(StorageDescriptor storageDesc, String tableQualifiedName,
+        String sdQualifiedName, Id tableId) throws Exception {
+        LOG.debug("Filling storage descriptor information for " + storageDesc);
 
-        return findEntity(typeName, tblQualifiedName);
-    }
+        Referenceable sdReferenceable = new Referenceable(HiveDataTypes.HIVE_STORAGEDESC.getName());
+        sdReferenceable.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, sdQualifiedName);
 
-    private AtlasEntityWithExtInfo findProcessEntity(String qualifiedName) throws Exception{
-        if (LOG.isDebugEnabled()) {
-            LOG.debug("Searching Atlas for process {}", qualifiedName);
-        }
+        SerDeInfo serdeInfo = storageDesc.getSerdeInfo();
+        LOG.debug("serdeInfo = " + serdeInfo);
+        // SkewedInfo skewedInfo = storageDesc.getSkewedInfo();
 
-        String typeName = HiveDataTypes.HIVE_PROCESS.getName();
+        String serdeInfoName = HiveDataTypes.HIVE_SERDE.getName();
+        Struct serdeInfoStruct = new Struct(serdeInfoName);
 
-        return findEntity(typeName, qualifiedName);
-    }
+        serdeInfoStruct.set(AtlasClient.NAME, serdeInfo.getName());
+        serdeInfoStruct.set("serializationLib", serdeInfo.getSerializationLib());
+        serdeInfoStruct.set(HiveDataModelGenerator.PARAMETERS, serdeInfo.getParameters());
 
-    private AtlasEntityWithExtInfo findEntity(final String typeName, final String qualifiedName) throws AtlasServiceException {
-        AtlasClientV2 atlasClientV2 = getAtlasClient();
+        sdReferenceable.set("serdeInfo", serdeInfoStruct);
+        sdReferenceable.set(HiveDataModelGenerator.STORAGE_NUM_BUCKETS, storageDesc.getNumBuckets());
+        sdReferenceable
+                .set(HiveDataModelGenerator.STORAGE_IS_STORED_AS_SUB_DIRS, storageDesc.isStoredAsSubDirectories());
 
-        try {
-            return atlasClientV2.getEntityByAttribute(typeName, Collections.singletonMap(ATTRIBUTE_QUALIFIED_NAME, qualifiedName));
-        } catch (AtlasServiceException e) {
-            if(e.getStatus() == ClientResponse.Status.NOT_FOUND) {
-                return null;
-            }
+        List<Struct> sortColsStruct = new ArrayList<>();
+        for (Order sortcol : storageDesc.getSortCols()) {
+            String hiveOrderName = HiveDataTypes.HIVE_ORDER.getName();
+            Struct colStruct = new Struct(hiveOrderName);
+            colStruct.set("col", sortcol.getCol());
+            colStruct.set("order", sortcol.getOrder());
 
-            throw e;
+            sortColsStruct.add(colStruct);
+        }
+        if (sortColsStruct.size() > 0) {
+            sdReferenceable.set("sortCols", sortColsStruct);
         }
-    }
-
-    private String getCreateTableString(Table table, String location){
-        String            colString = "";
-        List<FieldSchema> colList   = table.getAllCols();
 
-        if (colList != null) {
-            for (FieldSchema col : colList) {
-                colString += col.getName() + " " + col.getType() + ",";
-            }
+        sdReferenceable.set(HiveDataModelGenerator.LOCATION, storageDesc.getLocation());
+        sdReferenceable.set("inputFormat", storageDesc.getInputFormat());
+        sdReferenceable.set("outputFormat", storageDesc.getOutputFormat());
+        sdReferenceable.set("compressed", storageDesc.isCompressed());
 
-            if (colList.size() > 0) {
-                colString = colString.substring(0, colString.length() - 1);
-                colString = "(" + colString + ")";
-            }
+        if (storageDesc.getBucketCols().size() > 0) {
+            sdReferenceable.set("bucketCols", storageDesc.getBucketCols());
         }
 
-        String query = "create external table " + table.getTableName() +  colString + " location '" + location + "'";
+        sdReferenceable.set(HiveDataModelGenerator.PARAMETERS, storageDesc.getParameters());
+        sdReferenceable.set("storedAsSubDirectories", storageDesc.isStoredAsSubDirectories());
+        sdReferenceable.set(HiveDataModelGenerator.TABLE, tableId);
 
-        return query;
+        return sdReferenceable;
     }
 
-    private String lower(String str) {
-        if (StringUtils.isEmpty(str)) {
-            return "";
-        }
-
-        return str.toLowerCase().trim();
+    public Referenceable fillHDFSDataSet(String pathUri) {
+        Referenceable ref = new Referenceable(FSDataTypes.HDFS_PATH().toString());
+        ref.set("path", pathUri);
+        Path path = new Path(pathUri);
+        ref.set(AtlasClient.NAME, path.getName());
+        ref.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME, pathUri);
+        return ref;
     }
 
-
-    /**
-     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
-     * @param clusterName Name of the cluster to which the Hive component belongs
-     * @param table hive table for which the qualified name is needed
-     * @return Unique qualified name to identify the Table instance in Atlas.
-     */
-    private static String getTableQualifiedName(String clusterName, Table table) {
-        return getTableQualifiedName(clusterName, table.getDbName(), table.getTableName(), table.isTemporary());
+    public static String getColumnQualifiedName(final String tableQualifiedName, final String colName) {
+        final String[] parts = tableQualifiedName.split("@");
+        final String tableName = parts[0];
+        final String clusterName = parts[1];
+        return String.format("%s.%s@%s", tableName, colName.toLowerCase(), clusterName);
     }
 
-    private String getHdfsPathQualifiedName(String hdfsPath) {
-        return String.format("%s@%s", hdfsPath, clusterName);
-    }
+    public List<Referenceable> getColumns(List<FieldSchema> schemaList, Referenceable tableReference) throws Exception {
+        List<Referenceable> colList = new ArrayList<>();
 
-    /**
-     * Construct the qualified name used to uniquely identify a Database instance in Atlas.
-     * @param clusterName Name of the cluster to which the Hive component belongs
-     * @param dbName Name of the Hive database
-     * @return Unique qualified name to identify the Database instance in Atlas.
-     */
-    public static String getDBQualifiedName(String clusterName, String dbName) {
-        return String.format("%s@%s", dbName.toLowerCase(), clusterName);
+        for (FieldSchema fs : schemaList) {
+            LOG.debug("Processing field " + fs);
+            Referenceable colReferenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName());
+            colReferenceable.set(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME,
+                    getColumnQualifiedName((String) tableReference.get(AtlasClient.REFERENCEABLE_ATTRIBUTE_NAME), fs.getName()));
+            colReferenceable.set(AtlasClient.NAME, fs.getName());
+            colReferenceable.set(AtlasClient.OWNER, tableReference.get(AtlasClient.OWNER));
+            colReferenceable.set("type", fs.getType());
+            colReferenceable.set(HiveDataModelGenerator.COMMENT, fs.getComment());
+            colReferenceable.set(HiveDataModelGenerator.TABLE, tableReference.getId());
+
+            colList.add(colReferenceable);
+        }
+        return colList;
     }
 
     /**
-     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
-     * @param clusterName Name of the cluster to which the Hive component belongs
-     * @param dbName Name of the Hive database to which the Table belongs
-     * @param tableName Name of the Hive table
-     * @param isTemporaryTable is this a temporary table
-     * @return Unique qualified name to identify the Table instance in Atlas.
+     * Register the Hive DataModel in Atlas, if not already defined.
+     *
+     * The method checks for the presence of the type {@link HiveDataTypes#HIVE_PROCESS} with the Atlas server.
+     * If this type is defined, then we assume the Hive DataModel is registered.
+     * @throws Exception
      */
-    public static String getTableQualifiedName(String clusterName, String dbName, String tableName, boolean isTemporaryTable) {
-        String tableTempName = tableName;
+    public synchronized void registerHiveDataModel() throws Exception {
+        HiveDataModelGenerator dataModelGenerator = new HiveDataModelGenerator();
+        AtlasClient dgiClient = getAtlasClient();
 
-        if (isTemporaryTable) {
-            if (SessionState.get() != null && SessionState.get().getSessionId() != null) {
-                tableTempName = tableName + TEMP_TABLE_PREFIX + SessionState.get().getSessionId();
-            } else {
-                tableTempName = tableName + TEMP_TABLE_PREFIX + RandomStringUtils.random(10);
+        try {
+            dgiClient.getType(FSDataTypes.HDFS_PATH().toString());
+            LOG.info("HDFS data model is already registered!");
+        } catch(AtlasServiceException ase) {
+            if (ase.getStatus() == ClientResponse.Status.NOT_FOUND) {
+                //Trigger val definition
+                FSDataModel.main(null);
+
+                final String hdfsModelJson = TypesSerialization.toJson(FSDataModel.typesDef());
+                //Expected in case types do not exist
+                LOG.info("Registering HDFS data model : " + hdfsModelJson);
+                dgiClient.createType(hdfsModelJson);
             }
         }
 
-        return String.format("%s.%s@%s", dbName.toLowerCase(), tableTempName.toLowerCase(), clusterName);
+        try {
+            dgiClient.getType(HiveDataTypes.HIVE_PROCESS.getName());
+            LOG.info("Hive data model is already registered!");
+        } catch(AtlasServiceException ase) {
+            if (ase.getStatus() == ClientResponse.Status.NOT_FOUND) {
+                //Expected in case types do not exist
+                LOG.info("Registering Hive data model");
+                dgiClient.createType(dataModelGenerator.getModelAsJson());
+            }
+        }
     }
 
-    public static String getTableProcessQualifiedName(String clusterName, Table table) {
-        String tableQualifiedName = getTableQualifiedName(clusterName, table);
-        long   createdTime        = getTableCreatedTime(table);
-
-        return tableQualifiedName + SEP + createdTime;
-    }
+    public static void main(String[] args) throws Exception {
 
+        Configuration atlasConf = ApplicationProperties.get();
+        String atlasEndpoint = atlasConf.getString(ATLAS_ENDPOINT, DEFAULT_DGI_URL);
+        AtlasClient atlasClient;
 
-    /**
-     * Construct the qualified name used to uniquely identify a Table instance in Atlas.
-     * @param clusterName Name of the cluster to which the Hive component belongs
-     * @param dbName Name of the Hive database to which the Table belongs
-     * @param tableName Name of the Hive table
-     * @return Unique qualified name to identify the Table instance in Atlas.
-     */
-    public static String getTableQualifiedName(String clusterName, String dbName, String tableName) {
-        return getTableQualifiedName(clusterName, dbName, tableName, false);
-    }
-    public static String getStorageDescQFName(String tableQualifiedName) {
-        return tableQualifiedName + "_storage";
-    }
+        if (!AuthenticationUtil.isKerberosAuthenticationEnabled()) {
+            String[] basicAuthUsernamePassword = AuthenticationUtil.getBasicAuthenticationInput();
+            atlasClient = new AtlasClient(new String[]{atlasEndpoint}, basicAuthUsernamePassword);
+        } else {
+            UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
+            atlasClient = new AtlasClient(ugi, ugi.getShortUserName(), atlasEndpoint);
+        }
 
-    public static String getColumnQualifiedName(final String tableQualifiedName, final String colName) {
-        final String[] parts       = tableQualifiedName.split("@");
-        final String   tableName   = parts[0];
-        final String   clusterName = parts[1];
+        Options options = new Options();
+        CommandLineParser parser = new BasicParser();
+        CommandLine cmd = parser.parse( options, args);
 
-        return String.format("%s.%s@%s", tableName, colName.toLowerCase(), clusterName);
-    }
+        boolean failOnError = false;
+        if (cmd.hasOption("failOnError")) {
+            failOnError = true;
+        }
 
-    public static long getTableCreatedTime(Table table) {
-        return table.getTTable().getCreateTime() * MILLIS_CONVERT_FACTOR;
+        HiveMetaStoreBridge hiveMetaStoreBridge = new HiveMetaStoreBridge(new HiveConf(), atlasClient);
+        hiveMetaStoreBridge.registerHiveDataModel();
+        hiveMetaStoreBridge.importHiveMetadata(failOnError);
     }
 }