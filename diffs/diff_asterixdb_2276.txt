diff --git a/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/join/OptimizedHybridHashJoin.java b/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/join/OptimizedHybridHashJoin.java
index 7b6dcdb692..17f009e450 100644
--- a/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/join/OptimizedHybridHashJoin.java
+++ b/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/join/OptimizedHybridHashJoin.java
@@ -25,10 +25,10 @@ import org.apache.hyracks.api.comm.IFrame;
 import org.apache.hyracks.api.comm.IFrameWriter;
 import org.apache.hyracks.api.comm.VSizeFrame;
 import org.apache.hyracks.api.context.IHyracksTaskContext;
+import org.apache.hyracks.api.dataflow.value.IBinaryComparator;
 import org.apache.hyracks.api.dataflow.value.IMissingWriter;
 import org.apache.hyracks.api.dataflow.value.IMissingWriterFactory;
 import org.apache.hyracks.api.dataflow.value.IPredicateEvaluator;
-import org.apache.hyracks.api.dataflow.value.ITuplePairComparator;
 import org.apache.hyracks.api.dataflow.value.ITuplePartitionComputer;
 import org.apache.hyracks.api.dataflow.value.RecordDescriptor;
 import org.apache.hyracks.api.exceptions.HyracksDataException;
@@ -47,6 +47,7 @@ import org.apache.hyracks.dataflow.std.buffermanager.VPartitionTupleBufferManage
 import org.apache.hyracks.dataflow.std.structures.ISerializableTable;
 import org.apache.hyracks.dataflow.std.structures.SerializableHashTable;
 import org.apache.hyracks.dataflow.std.structures.TuplePointer;
+import org.apache.hyracks.dataflow.std.util.FrameTuplePairComparator;
 
 /**
  * This class mainly applies one level of HHJ on a pair of
@@ -57,66 +58,88 @@ public class OptimizedHybridHashJoin {
     // Used for special probe BigObject which can not be held into the Join memory
     private FrameTupleAppender bigProbeFrameAppender;
 
-    public enum SIDE {
+    enum SIDE {
         BUILD,
         PROBE
     }
 
-    private final IHyracksTaskContext ctx;
+    private IHyracksTaskContext ctx;
 
     private final String buildRelName;
     private final String probeRelName;
-    private final ITuplePairComparator comparator;
+
+    private final int[] buildKeys;
+    private final int[] probeKeys;
+
+    private final IBinaryComparator[] comparators;
+
     private final ITuplePartitionComputer buildHpc;
     private final ITuplePartitionComputer probeHpc;
+
     private final RecordDescriptor buildRd;
     private final RecordDescriptor probeRd;
-    private final RunFileWriter[] buildRFWriters; //writing spilled build partitions
-    private final RunFileWriter[] probeRFWriters; //writing spilled probe partitions
+
+    private RunFileWriter[] buildRFWriters; //writing spilled build partitions
+    private RunFileWriter[] probeRFWriters; //writing spilled probe partitions
+
     private final IPredicateEvaluator predEvaluator;
     private final boolean isLeftOuter;
     private final IMissingWriter[] nonMatchWriters;
+
     private final BitSet spilledStatus; //0=resident, 1=spilled
     private final int numOfPartitions;
     private final int memSizeInFrames;
     private InMemoryHashJoin inMemJoiner; //Used for joining resident partitions
+
     private IPartitionedTupleBufferManager bufferManager;
     private PreferToSpillFullyOccupiedFramePolicy spillPolicy;
+
     private final FrameTupleAccessor accessorBuild;
     private final FrameTupleAccessor accessorProbe;
+
+    private IDeallocatableFramePool framePool;
     private ISimpleFrameBufferManager bufferManagerForHashTable;
-    // Added for handling correct calling for predicate-evaluator upon recursive calls that cause role-reversal.
-    private boolean isReversed;
+
+    private boolean isReversed; //Added for handling correct calling for predicate-evaluator upon recursive calls that cause role-reversal
+
     // stats information
     private int[] buildPSizeInTups;
     private IFrame reloadBuffer;
-    // this is a reusable object to store the pointer,which is not used anywhere. we mainly use it to match the
-    // corresponding function signature.
-    private final TuplePointer tempPtr = new TuplePointer();
+    private TuplePointer tempPtr = new TuplePointer(); // this is a reusable object to store the pointer,which is not used anywhere.
+                                                       // we mainly use it to match the corresponding function signature.
     private int[] probePSizeInTups;
 
     public OptimizedHybridHashJoin(IHyracksTaskContext ctx, int memSizeInFrames, int numOfPartitions,
-            String probeRelName, String buildRelName, ITuplePairComparator comparator, RecordDescriptor probeRd,
-            RecordDescriptor buildRd, ITuplePartitionComputer probeHpc, ITuplePartitionComputer buildHpc,
-            IPredicateEvaluator predEval, boolean isLeftOuter, IMissingWriterFactory[] nullWriterFactories1) {
+            String probeRelName,
+            String buildRelName, int[] probeKeys, int[] buildKeys, IBinaryComparator[] comparators,
+            RecordDescriptor probeRd, RecordDescriptor buildRd, ITuplePartitionComputer probeHpc,
+            ITuplePartitionComputer buildHpc, IPredicateEvaluator predEval, boolean isLeftOuter,
+            IMissingWriterFactory[] nullWriterFactories1) {
         this.ctx = ctx;
         this.memSizeInFrames = memSizeInFrames;
         this.buildRd = buildRd;
         this.probeRd = probeRd;
         this.buildHpc = buildHpc;
         this.probeHpc = probeHpc;
-        this.comparator = comparator;
+        this.buildKeys = buildKeys;
+        this.probeKeys = probeKeys;
+        this.comparators = comparators;
         this.buildRelName = buildRelName;
         this.probeRelName = probeRelName;
+
         this.numOfPartitions = numOfPartitions;
         this.buildRFWriters = new RunFileWriter[numOfPartitions];
         this.probeRFWriters = new RunFileWriter[numOfPartitions];
+
         this.accessorBuild = new FrameTupleAccessor(buildRd);
         this.accessorProbe = new FrameTupleAccessor(probeRd);
+
         this.predEvaluator = predEval;
         this.isLeftOuter = isLeftOuter;
         this.isReversed = false;
+
         this.spilledStatus = new BitSet(numOfPartitions);
+
         this.nonMatchWriters = isLeftOuter ? new IMissingWriter[nullWriterFactories1.length] : null;
         if (isLeftOuter) {
             for (int i = 0; i < nullWriterFactories1.length; i++) {
@@ -126,8 +149,7 @@ public class OptimizedHybridHashJoin {
     }
 
     public void initBuild() throws HyracksDataException {
-        IDeallocatableFramePool framePool =
-                new DeallocatableFramePool(ctx, memSizeInFrames * ctx.getInitialFrameSize());
+        framePool = new DeallocatableFramePool(ctx, memSizeInFrames * ctx.getInitialFrameSize());
         bufferManagerForHashTable = new FramePoolBackedFrameBufferManager(framePool);
         bufferManager = new VPartitionTupleBufferManager(
                 PreferToSpillFullyOccupiedFramePolicy.createAtMostOneFrameForSpilledPartitionConstrain(spilledStatus),
@@ -140,27 +162,32 @@ public class OptimizedHybridHashJoin {
     public void build(ByteBuffer buffer) throws HyracksDataException {
         accessorBuild.reset(buffer);
         int tupleCount = accessorBuild.getTupleCount();
+
         for (int i = 0; i < tupleCount; ++i) {
             int pid = buildHpc.partition(accessorBuild, i, numOfPartitions);
-            processTupleBuildPhase(i, pid);
+            processTuple(i, pid);
             buildPSizeInTups[pid]++;
         }
 
     }
 
-    private void processTupleBuildPhase(int tid, int pid) throws HyracksDataException {
+    private void processTuple(int tid, int pid) throws HyracksDataException {
         while (!bufferManager.insertTuple(pid, accessorBuild, tid, tempPtr)) {
-            int victimPartition = spillPolicy.selectVictimPartition(pid);
-            if (victimPartition < 0) {
-                throw new HyracksDataException(
-                        "No more space left in the memory buffer, please assign more memory to hash-join.");
-            }
-            spillPartition(victimPartition);
+            selectAndSpillVictim(pid);
         }
     }
 
+    private void selectAndSpillVictim(int pid) throws HyracksDataException {
+        int victimPartition = spillPolicy.selectVictimPartition(pid);
+        if (victimPartition < 0) {
+            throw new HyracksDataException(
+                    "No more space left in the memory buffer, please assign more memory to hash-join.");
+        }
+        spillPartition(victimPartition);
+    }
+
     private void spillPartition(int pid) throws HyracksDataException {
-        RunFileWriter writer = getSpillWriterOrCreateNewOneIfNotExist(buildRFWriters, buildRelName, pid);
+        RunFileWriter writer = getSpillWriterOrCreateNewOneIfNotExist(pid, SIDE.BUILD);
         bufferManager.flushPartition(pid, writer);
         bufferManager.clearPartition(pid);
         spilledStatus.set(pid);
@@ -173,12 +200,23 @@ public class OptimizedHybridHashJoin {
         buildRFWriters[pid].close();
     }
 
-    private RunFileWriter getSpillWriterOrCreateNewOneIfNotExist(RunFileWriter[] runFileWriters, String refName,
-            int pid) throws HyracksDataException {
+    private RunFileWriter getSpillWriterOrCreateNewOneIfNotExist(int pid, SIDE whichSide) throws HyracksDataException {
+        RunFileWriter[] runFileWriters = null;
+        String refName = null;
+        switch (whichSide) {
+            case BUILD:
+                runFileWriters = buildRFWriters;
+                refName = buildRelName;
+                break;
+            case PROBE:
+                refName = probeRelName;
+                runFileWriters = probeRFWriters;
+                break;
+        }
         RunFileWriter writer = runFileWriters[pid];
         if (writer == null) {
             FileReference file = ctx.getJobletContext().createManagedWorkspaceFile(refName);
-            writer = new RunFileWriter(file, ctx.getIoManager());
+            writer = new RunFileWriter(file, ctx.getIOManager());
             writer.open();
             runFileWriters[pid] = writer;
         }
@@ -187,56 +225,50 @@ public class OptimizedHybridHashJoin {
 
     public void closeBuild() throws HyracksDataException {
         // Flushes the remaining chunks of the all spilled partitions to the disk.
-        closeAllSpilledPartitions(buildRFWriters, buildRelName);
+        closeAllSpilledPartitions(SIDE.BUILD);
 
         // Makes the space for the in-memory hash table (some partitions may need to be spilled to the disk
         // during this step in order to make the space.)
         // and tries to bring back as many spilled partitions as possible if there is free space.
         int inMemTupCount = makeSpaceForHashTableAndBringBackSpilledPartitions();
 
-        ISerializableTable table = new SerializableHashTable(inMemTupCount, ctx, bufferManagerForHashTable);
-        this.inMemJoiner = new InMemoryHashJoin(ctx, new FrameTupleAccessor(probeRd), probeHpc,
-                new FrameTupleAccessor(buildRd), buildRd, buildHpc, comparator, isLeftOuter, nonMatchWriters, table,
-                predEvaluator, isReversed, bufferManagerForHashTable);
-
-        buildHashTable();
-    }
-
-    public void clearBuildTempFiles() throws HyracksDataException {
-        clearTempFiles(buildRFWriters);
-    }
+        createInMemoryJoiner(inMemTupCount);
 
-    public void clearProbeTempFiles() throws HyracksDataException {
-        clearTempFiles(probeRFWriters);
+        loadDataInMemJoin();
     }
 
     /**
      * In case of failure happens, we need to clear up the generated temporary files.
      */
-    private void clearTempFiles(RunFileWriter[] runFileWriters) throws HyracksDataException {
-        for (int i = 0; i < runFileWriters.length; i++) {
-            if (runFileWriters[i] != null) {
-                runFileWriters[i].erase();
+    public void clearBuildTempFiles() {
+        for (int i = 0; i < buildRFWriters.length; i++) {
+            if (buildRFWriters[i] != null) {
+                buildRFWriters[i].getFileReference().delete();
             }
         }
     }
 
-    private void closeAllSpilledPartitions(RunFileWriter[] runFileWriters, String refName) throws HyracksDataException {
-        try {
-            for (int pid = spilledStatus.nextSetBit(0); pid >= 0 && pid < numOfPartitions; pid =
-                    spilledStatus.nextSetBit(pid + 1)) {
-                if (bufferManager.getNumTuples(pid) > 0) {
-                    bufferManager.flushPartition(pid,
-                            getSpillWriterOrCreateNewOneIfNotExist(runFileWriters, refName, pid));
-                    bufferManager.clearPartition(pid);
-                }
+    private void closeAllSpilledPartitions(SIDE whichSide) throws HyracksDataException {
+        RunFileWriter[] runFileWriters = null;
+        switch (whichSide) {
+            case BUILD:
+                runFileWriters = buildRFWriters;
+                break;
+            case PROBE:
+                runFileWriters = probeRFWriters;
+                break;
+        }
+
+        for (int pid = spilledStatus.nextSetBit(0); pid >= 0
+                && pid < numOfPartitions; pid = spilledStatus.nextSetBit(pid + 1)) {
+            if (bufferManager.getNumTuples(pid) > 0) {
+                bufferManager.flushPartition(pid, getSpillWriterOrCreateNewOneIfNotExist(pid, whichSide));
+                bufferManager.clearPartition(pid);
             }
-        } finally {
-            // Force to close all run file writers.
-            for (RunFileWriter runFileWriter : runFileWriters) {
-                if (runFileWriter != null) {
-                    runFileWriter.close();
-                }
+            // It doesn't matter whether a spilled partition currently holds a tuple in memory or not.
+            // The file that holds the corresponding spilled partition needs to be closed.
+            if (runFileWriters[pid] != null) {
+                runFileWriters[pid].close();
             }
         }
     }
@@ -250,68 +282,94 @@ public class OptimizedHybridHashJoin {
      * @throws HyracksDataException
      */
     private int makeSpaceForHashTableAndBringBackSpilledPartitions() throws HyracksDataException {
+        // we need number of |spilledPartitions| buffers to store the probe data
         int frameSize = ctx.getInitialFrameSize();
         long freeSpace = (long) (memSizeInFrames - spilledStatus.cardinality()) * frameSize;
 
+        // For partitions in main memory, we deduct their size from the free space.
         int inMemTupCount = 0;
-        for (int p = spilledStatus.nextClearBit(0); p >= 0 && p < numOfPartitions; p =
-                spilledStatus.nextClearBit(p + 1)) {
+        for (int p = spilledStatus.nextClearBit(0); p >= 0
+                && p < numOfPartitions; p = spilledStatus.nextClearBit(p + 1)) {
             freeSpace -= bufferManager.getPhysicalSize(p);
             inMemTupCount += buildPSizeInTups[p];
         }
-        freeSpace -= SerializableHashTable.getExpectedTableByteSize(inMemTupCount, frameSize);
 
-        return spillAndReloadPartitions(frameSize, freeSpace, inMemTupCount);
-    }
-
-    private int spillAndReloadPartitions(int frameSize, long freeSpace, int inMemTupCount) throws HyracksDataException {
-        int pidToSpill, numberOfTuplesToBeSpilled;
-        long expectedHashTableSizeDecrease;
-        long currentFreeSpace = freeSpace;
-        int currentInMemTupCount = inMemTupCount;
-        // Spill some partitions if there is no free space.
-        while (currentFreeSpace < 0) {
-            pidToSpill = selectSinglePartitionToSpill(currentFreeSpace, currentInMemTupCount, frameSize);
+        // Calculates the expected hash table size for the given number of tuples in main memory
+        // and deducts it from the free space.
+        long hashTableByteSizeForInMemTuples = SerializableHashTable.getExpectedTableByteSize(inMemTupCount,
+                frameSize);
+        freeSpace -= hashTableByteSizeForInMemTuples;
+
+        // In the case where free space is less than zero after considering the hash table size,
+        // we need to spill more partitions until we can accommodate the hash table in memory.
+        // TODO: there may be different policies (keep spilling minimum, spilling maximum, find a similar size to the
+        //                                        hash table, or keep spilling from the first partition)
+        boolean moreSpilled = false;
+
+        // No space to accommodate the hash table? Then, we spill one or more partitions to the disk.
+        if (freeSpace < 0) {
+            // Tries to find a best-fit partition not to spill many partitions.
+            int pidToSpill = selectSinglePartitionToSpill(freeSpace, inMemTupCount, frameSize);
             if (pidToSpill >= 0) {
-                numberOfTuplesToBeSpilled = buildPSizeInTups[pidToSpill];
-                expectedHashTableSizeDecrease =
-                        -SerializableHashTable.calculateByteSizeDeltaForTableSizeChange(currentInMemTupCount,
-                                -numberOfTuplesToBeSpilled, frameSize);
-                currentInMemTupCount -= numberOfTuplesToBeSpilled;
-                currentFreeSpace +=
-                        bufferManager.getPhysicalSize(pidToSpill) + expectedHashTableSizeDecrease - frameSize;
+                // There is a suitable one. We spill that partition to the disk.
+                long hashTableSizeDecrease = -SerializableHashTable.calculateByteSizeDeltaForTableSizeChange(
+                        inMemTupCount, -buildPSizeInTups[pidToSpill], frameSize);
+                freeSpace = freeSpace + bufferManager.getPhysicalSize(pidToSpill) + hashTableSizeDecrease;
+                inMemTupCount -= buildPSizeInTups[pidToSpill];
                 spillPartition(pidToSpill);
                 closeBuildPartition(pidToSpill);
+                moreSpilled = true;
             } else {
-                throw new HyracksDataException("Hash join does not have enough memory even after spilling.");
+                // There is no single suitable partition. So, we need to spill multiple partitions to the disk
+                // in order to accommodate the hash table.
+                for (int p = spilledStatus.nextClearBit(0); p >= 0
+                        && p < numOfPartitions; p = spilledStatus.nextClearBit(p + 1)) {
+                    int spaceToBeReturned = bufferManager.getPhysicalSize(p);
+                    int numberOfTuplesToBeSpilled = buildPSizeInTups[p];
+                    if (spaceToBeReturned == 0 || numberOfTuplesToBeSpilled == 0) {
+                        continue;
+                    }
+                    spillPartition(p);
+                    closeBuildPartition(p);
+                    moreSpilled = true;
+                    // Since the number of tuples in memory has been decreased,
+                    // the hash table size will be decreased, too.
+                    // We put minus since the method returns a negative value to represent a newly reclaimed space.
+                    long expectedHashTableSizeDecrease = -SerializableHashTable
+                            .calculateByteSizeDeltaForTableSizeChange(inMemTupCount, -numberOfTuplesToBeSpilled,
+                                    frameSize);
+                    freeSpace = freeSpace + spaceToBeReturned + expectedHashTableSizeDecrease;
+                    // Adjusts the hash table size
+                    inMemTupCount -= numberOfTuplesToBeSpilled;
+                    if (freeSpace >= 0) {
+                        break;
+                    }
+                }
             }
         }
-        // Bring some partitions back in if there is enough space.
-        return bringPartitionsBack(currentFreeSpace, currentInMemTupCount, frameSize);
-    }
 
-    /**
-     * Brings back some partitions if there is free memory and partitions that fit in that space.
-     *
-     * @param freeSpace current amount of free space in memory
-     * @param inMemTupCount number of in memory tuples
-     * @return number of in memory tuples after bringing some (or none) partitions in memory.
-     * @throws HyracksDataException
-     */
-    private int bringPartitionsBack(long freeSpace, int inMemTupCount, int frameSize) throws HyracksDataException {
+        // If more partitions have been spilled to the disk, calculate the expected hash table size again
+        // before bringing some partitions to main memory.
+        if (moreSpilled) {
+            hashTableByteSizeForInMemTuples = SerializableHashTable.getExpectedTableByteSize(inMemTupCount,
+                    frameSize);
+        }
+
+        // Brings back some partitions if there is enough free space.
         int pid = 0;
-        int currentMemoryTupleCount = inMemTupCount;
-        long currentFreeSpace = freeSpace;
-        while ((pid = selectAPartitionToReload(currentFreeSpace, pid, currentMemoryTupleCount)) >= 0
-                && loadSpilledPartitionToMem(pid, buildRFWriters[pid])) {
-            currentMemoryTupleCount += buildPSizeInTups[pid];
-            // Reserve space for loaded data & increase in hash table (give back one frame taken by spilled partition.)
-            currentFreeSpace = currentFreeSpace
-                    - bufferManager.getPhysicalSize(pid) - SerializableHashTable
-                            .calculateByteSizeDeltaForTableSizeChange(inMemTupCount, buildPSizeInTups[pid], frameSize)
-                    + frameSize;
+        while ((pid = selectPartitionsToReload(freeSpace, pid, inMemTupCount)) >= 0) {
+            if (!loadSpilledPartitionToMem(pid, buildRFWriters[pid])) {
+                break;
+            }
+            long expectedHashTableByteSizeIncrease = SerializableHashTable
+                    .calculateByteSizeDeltaForTableSizeChange(inMemTupCount, buildPSizeInTups[pid], frameSize);
+            freeSpace = freeSpace - bufferManager.getPhysicalSize(pid) - expectedHashTableByteSizeIncrease;
+            inMemTupCount += buildPSizeInTups[pid];
+            // Adjusts the hash table size
+            hashTableByteSizeForInMemTuples += expectedHashTableByteSizeIncrease;
         }
-        return currentMemoryTupleCount;
+
+        return inMemTupCount;
     }
 
     /**
@@ -323,18 +381,14 @@ public class OptimizedHybridHashJoin {
         long spaceAfterSpill;
         long minSpaceAfterSpill = (long) memSizeInFrames * frameSize;
         int minSpaceAfterSpillPartID = -1;
-        int nextAvailablePidToSpill = -1;
-        for (int p = spilledStatus.nextClearBit(0); p >= 0 && p < numOfPartitions; p =
-                spilledStatus.nextClearBit(p + 1)) {
+
+        for (int p = spilledStatus.nextClearBit(0); p >= 0
+                && p < numOfPartitions; p = spilledStatus.nextClearBit(p + 1)) {
             if (buildPSizeInTups[p] == 0 || bufferManager.getPhysicalSize(p) == 0) {
                 continue;
             }
-            if (nextAvailablePidToSpill < 0) {
-                nextAvailablePidToSpill = p;
-            }
             // We put minus since the method returns a negative value to represent a newly reclaimed space.
-            // One frame is deducted since a spilled partition needs one frame from free space.
-            spaceAfterSpill = currentFreeSpace + bufferManager.getPhysicalSize(p) - frameSize + (-SerializableHashTable
+            spaceAfterSpill = currentFreeSpace + bufferManager.getPhysicalSize(p) + (-SerializableHashTable
                     .calculateByteSizeDeltaForTableSizeChange(currentInMemTupCount, -buildPSizeInTups[p], frameSize));
             if (spaceAfterSpill == 0) {
                 // Found the perfect one. Just returns this partition.
@@ -345,30 +399,18 @@ public class OptimizedHybridHashJoin {
                 minSpaceAfterSpillPartID = p;
             }
         }
-
-        return minSpaceAfterSpillPartID >= 0 ? minSpaceAfterSpillPartID : nextAvailablePidToSpill;
-    }
-
-    /**
-     * Finds a partition that can fit in the left over memory.
-     * @param freeSpace current free space
-     * @param inMemTupCount number of tuples currently in memory
-     * @return partition id of selected partition to reload
-     */
-    private int selectAPartitionToReload(long freeSpace, int pid, int inMemTupCount) {
-        int frameSize = ctx.getInitialFrameSize();
-        // Add one frame to freeSpace to consider the one frame reserved for the spilled partition
-        long totalFreeSpace = freeSpace + frameSize;
-        if (totalFreeSpace > 0) {
-            for (int i = spilledStatus.nextSetBit(pid); i >= 0 && i < numOfPartitions; i =
-                    spilledStatus.nextSetBit(i + 1)) {
-                int spilledTupleCount = buildPSizeInTups[i];
-                // Expected hash table size increase after reloading this partition
-                long expectedHashTableByteSizeIncrease = SerializableHashTable
-                        .calculateByteSizeDeltaForTableSizeChange(inMemTupCount, spilledTupleCount, frameSize);
-                if (totalFreeSpace >= buildRFWriters[i].getFileSize() + expectedHashTableByteSizeIncrease) {
-                    return i;
-                }
+        return minSpaceAfterSpillPartID;
+    }
+
+    private int selectPartitionsToReload(long freeSpace, int pid, int inMemTupCount) {
+        for (int i = spilledStatus.nextSetBit(pid); i >= 0
+                && i < numOfPartitions; i = spilledStatus.nextSetBit(i + 1)) {
+            int spilledTupleCount = buildPSizeInTups[i];
+            // Expected hash table size increase after reloading this partition
+            long expectedHashTableByteSizeIncrease = SerializableHashTable.calculateByteSizeDeltaForTableSizeChange(
+                    inMemTupCount, spilledTupleCount, ctx.getInitialFrameSize());
+            if (freeSpace >= buildRFWriters[i].getFileSize() + expectedHashTableByteSizeIncrease) {
+                return i;
             }
         }
         return -1;
@@ -376,39 +418,47 @@ public class OptimizedHybridHashJoin {
 
     private boolean loadSpilledPartitionToMem(int pid, RunFileWriter wr) throws HyracksDataException {
         RunFileReader r = wr.createReader();
-        try {
-            r.open();
-            if (reloadBuffer == null) {
-                reloadBuffer = new VSizeFrame(ctx);
-            }
-            while (r.nextFrame(reloadBuffer)) {
-                accessorBuild.reset(reloadBuffer.getBuffer());
-                for (int tid = 0; tid < accessorBuild.getTupleCount(); tid++) {
-                    if (!bufferManager.insertTuple(pid, accessorBuild, tid, tempPtr)) {
-                        // for some reason (e.g. fragmentation) if inserting fails, we need to clear the occupied frames
-                        bufferManager.clearPartition(pid);
-                        return false;
-                    }
+        r.open();
+        if (reloadBuffer == null) {
+            reloadBuffer = new VSizeFrame(ctx);
+        }
+        while (r.nextFrame(reloadBuffer)) {
+            accessorBuild.reset(reloadBuffer.getBuffer());
+            for (int tid = 0; tid < accessorBuild.getTupleCount(); tid++) {
+                if (!bufferManager.insertTuple(pid, accessorBuild, tid, tempPtr)) {
+                    // for some reason (e.g. due to fragmentation) if the inserting failed,
+                    // we need to clear the occupied frames
+                    bufferManager.clearPartition(pid);
+                    r.close();
+                    return false;
                 }
             }
-            // Closes and deletes the run file if it is already loaded into memory.
-            r.setDeleteAfterClose(true);
-        } finally {
-            r.close();
         }
+
+        // Closes and deletes the run file if it is already loaded into memory.
+        r.setDeleteAfterClose(true);
+        r.close();
         spilledStatus.set(pid, false);
         buildRFWriters[pid] = null;
         return true;
     }
 
-    private void buildHashTable() throws HyracksDataException {
+    private void createInMemoryJoiner(int inMemTupCount) throws HyracksDataException {
+        ISerializableTable table = new SerializableHashTable(inMemTupCount, ctx, bufferManagerForHashTable);
+        this.inMemJoiner = new InMemoryHashJoin(ctx, inMemTupCount, new FrameTupleAccessor(probeRd), probeHpc,
+                new FrameTupleAccessor(buildRd), buildRd, buildHpc,
+                new FrameTuplePairComparator(probeKeys, buildKeys, comparators), isLeftOuter, nonMatchWriters, table,
+                predEvaluator, isReversed, bufferManagerForHashTable);
+    }
+
+    private void loadDataInMemJoin() throws HyracksDataException {
 
         for (int pid = 0; pid < numOfPartitions; pid++) {
             if (!spilledStatus.get(pid)) {
                 bufferManager.flushPartition(pid, new IFrameWriter() {
                     @Override
-                    public void open() {
-                        // Only nextFrame method is needed to pass the frame to the next operator.
+                    public void open() throws HyracksDataException {
+
                     }
 
                     @Override
@@ -417,21 +467,24 @@ public class OptimizedHybridHashJoin {
                     }
 
                     @Override
-                    public void fail() {
-                        // Only nextFrame method is needed to pass the frame to the next operator.
+                    public void fail() throws HyracksDataException {
+
                     }
 
                     @Override
-                    public void close() {
-                        // Only nextFrame method is needed to pass the frame to the next operator.
+                    public void close() throws HyracksDataException {
+
                     }
                 });
             }
         }
     }
 
-    public void initProbe() {
+    public void initProbe() throws HyracksDataException {
+
         probePSizeInTups = new int[numOfPartitions];
+        probeRFWriters = new RunFileWriter[numOfPartitions];
+
     }
 
     public void probe(ByteBuffer buffer, IFrameWriter writer) throws HyracksDataException {
@@ -448,7 +501,19 @@ public class OptimizedHybridHashJoin {
 
             if (buildPSizeInTups[pid] > 0 || isLeftOuter) { //Tuple has potential match from previous phase
                 if (spilledStatus.get(pid)) { //pid is Spilled
-                    processTupleProbePhase(i, pid);
+                    while (!bufferManager.insertTuple(pid, accessorProbe, i, tempPtr)) {
+                        int victim = pid;
+                        if (bufferManager.getNumTuples(pid) == 0) { // current pid is empty, choose the biggest one
+                            victim = spillPolicy.findSpilledPartitionWithMaxMemoryUsage();
+                        }
+                        if (victim < 0) { // current tuple is too big for all the free space
+                            flushBigProbeObjectToDisk(pid, accessorProbe, i);
+                            break;
+                        }
+                        RunFileWriter runFileWriter = getSpillWriterOrCreateNewOneIfNotExist(victim, SIDE.PROBE);
+                        bufferManager.flushPartition(victim, runFileWriter);
+                        bufferManager.clearPartition(victim);
+                    }
                 } else { //pid is Resident
                     inMemJoiner.join(i, writer);
                 }
@@ -457,44 +522,12 @@ public class OptimizedHybridHashJoin {
         }
     }
 
-    private void processTupleProbePhase(int tupleId, int pid) throws HyracksDataException {
-
-        if (!bufferManager.insertTuple(pid, accessorProbe, tupleId, tempPtr)) {
-            int recordSize =
-                    VPartitionTupleBufferManager.calculateActualSize(null, accessorProbe.getTupleLength(tupleId));
-            // If the partition is at least half-full and insertion fails, that partition is preferred to get
-            // spilled, otherwise the biggest partition gets chosen as the victim.
-            boolean modestCase = recordSize <= (ctx.getInitialFrameSize() / 2);
-            int victim = (modestCase && bufferManager.getNumTuples(pid) > 0) ? pid
-                    : spillPolicy.findSpilledPartitionWithMaxMemoryUsage();
-            // This method is called for the spilled partitions, so we know that this tuple is going to get written to
-            // disk, sooner or later. As such, we try to reduce the number of writes that happens. So if the record is
-            // larger than the size of the victim partition, we just flush it to the disk, otherwise we spill the
-            // victim and this time insertion should be successful.
-            //TODO:(More optimization) There might be a case where the leftover memory in the last frame of the
-            // current partition + free memory in buffer manager + memory from the victim would be sufficient to hold
-            // the record.
-            if (victim >= 0 && bufferManager.getPhysicalSize(victim) >= recordSize) {
-                RunFileWriter runFileWriter =
-                        getSpillWriterOrCreateNewOneIfNotExist(probeRFWriters, probeRelName, victim);
-                bufferManager.flushPartition(victim, runFileWriter);
-                bufferManager.clearPartition(victim);
-                if (!bufferManager.insertTuple(pid, accessorProbe, tupleId, tempPtr)) {
-                    // This should not happen if the size calculations are correct, just not to let the query fail.
-                    flushBigProbeObjectToDisk(pid, accessorProbe, tupleId);
-                }
-            } else {
-                flushBigProbeObjectToDisk(pid, accessorProbe, tupleId);
-            }
-        }
-    }
-
     private void flushBigProbeObjectToDisk(int pid, FrameTupleAccessor accessorProbe, int i)
             throws HyracksDataException {
         if (bigProbeFrameAppender == null) {
             bigProbeFrameAppender = new FrameTupleAppender(new VSizeFrame(ctx));
         }
-        RunFileWriter runFileWriter = getSpillWriterOrCreateNewOneIfNotExist(probeRFWriters, probeRelName, pid);
+        RunFileWriter runFileWriter = getSpillWriterOrCreateNewOneIfNotExist(pid, SIDE.PROBE);
         if (!bigProbeFrameAppender.append(accessorProbe, i)) {
             throw new HyracksDataException("The given tuple is too big");
         }
@@ -505,50 +538,60 @@ public class OptimizedHybridHashJoin {
         return spilledStatus.nextSetBit(0) < 0;
     }
 
-    public void completeProbe(IFrameWriter writer) throws HyracksDataException {
+    public void closeProbe(IFrameWriter writer) throws HyracksDataException {
         //We do NOT join the spilled partitions here, that decision is made at the descriptor level
         //(which join technique to use)
-        inMemJoiner.completeJoin(writer);
-    }
-
-    public void releaseResource() throws HyracksDataException {
+        inMemJoiner.closeJoin(writer);
         inMemJoiner.closeTable();
-        closeAllSpilledPartitions(probeRFWriters, probeRelName);
+        closeAllSpilledPartitions(SIDE.PROBE);
         bufferManager.close();
         inMemJoiner = null;
         bufferManager = null;
         bufferManagerForHashTable = null;
     }
 
+    /**
+     * In case of failure happens, we need to clear up the generated temporary files.
+     */
+    public void clearProbeTempFiles() {
+        for (int i = 0; i < probeRFWriters.length; i++) {
+            if (probeRFWriters[i] != null) {
+                probeRFWriters[i].getFileReference().delete();
+            }
+        }
+    }
+
     public RunFileReader getBuildRFReader(int pid) throws HyracksDataException {
-        return buildRFWriters[pid] == null ? null : buildRFWriters[pid].createDeleteOnCloseReader();
+        return ((buildRFWriters[pid] == null) ? null : (buildRFWriters[pid]).createDeleteOnCloseReader());
     }
 
     public int getBuildPartitionSizeInTup(int pid) {
-        return buildPSizeInTups[pid];
+        return (buildPSizeInTups[pid]);
     }
 
     public RunFileReader getProbeRFReader(int pid) throws HyracksDataException {
-        return probeRFWriters[pid] == null ? null : probeRFWriters[pid].createDeleteOnCloseReader();
+        return ((probeRFWriters[pid] == null) ? null : (probeRFWriters[pid]).createDeleteOnCloseReader());
     }
 
     public int getProbePartitionSizeInTup(int pid) {
-        return probePSizeInTups[pid];
+        return (probePSizeInTups[pid]);
     }
 
     public int getMaxBuildPartitionSize() {
-        return getMaxPartitionSize(buildPSizeInTups);
+        int max = buildPSizeInTups[0];
+        for (int i = 1; i < buildPSizeInTups.length; i++) {
+            if (buildPSizeInTups[i] > max) {
+                max = buildPSizeInTups[i];
+            }
+        }
+        return max;
     }
 
     public int getMaxProbePartitionSize() {
-        return getMaxPartitionSize(probePSizeInTups);
-    }
-
-    private int getMaxPartitionSize(int[] partitions) {
-        int max = partitions[0];
-        for (int i = 1; i < partitions.length; i++) {
-            if (partitions[i] > max) {
-                max = partitions[i];
+        int max = probePSizeInTups[0];
+        for (int i = 1; i < probePSizeInTups.length; i++) {
+            if (probePSizeInTups[i] > max) {
+                max = probePSizeInTups[i];
             }
         }
         return max;
@@ -558,11 +601,74 @@ public class OptimizedHybridHashJoin {
         return spilledStatus;
     }
 
-    public int getPartitionSize(int pid) {
-        return bufferManager.getPhysicalSize(pid);
-    }
-
     public void setIsReversed(boolean b) {
         this.isReversed = b;
     }
+
+    /**
+     * Prints out the detailed information for partitions: in-memory and spilled partitions.
+     * This method exists for a debug purpose.
+     */
+    public String printPartitionInfo(SIDE whichSide) {
+        StringBuilder buf = new StringBuilder();
+        buf.append(">>> " + this + " " + Thread.currentThread().getId() + " printInfo():" + "\n");
+        if (whichSide == SIDE.BUILD) {
+            buf.append("BUILD side" + "\n");
+        } else {
+            buf.append("PROBE side" + "\n");
+        }
+        buf.append("# of partitions:\t" + numOfPartitions + "\t#spilled:\t" + spilledStatus.cardinality()
+                + "\t#in-memory:\t" + (numOfPartitions - spilledStatus.cardinality()) + "\n");
+        buf.append("(A) Spilled partitions" + "\n");
+        int spilledTupleCount = 0;
+        int spilledPartByteSize = 0;
+        for (int pid = spilledStatus.nextSetBit(0); pid >= 0
+                && pid < numOfPartitions; pid = spilledStatus.nextSetBit(pid + 1)) {
+            if (whichSide == SIDE.BUILD) {
+                spilledTupleCount += buildPSizeInTups[pid];
+                spilledPartByteSize += buildRFWriters[pid].getFileSize();
+                buf.append("part:\t" + pid + "\t#tuple:\t" + buildPSizeInTups[pid] + "\tsize(MB):\t"
+                        + ((double) buildRFWriters[pid].getFileSize() / 1048576) + "\n");
+            } else {
+                spilledTupleCount += probePSizeInTups[pid];
+                spilledPartByteSize += probeRFWriters[pid].getFileSize();
+            }
+        }
+        if (spilledStatus.cardinality() > 0) {
+            buf.append("# of spilled tuples:\t" + spilledTupleCount + "\tsize(MB):\t"
+                    + ((double) spilledPartByteSize / 1048576) + "avg #tuples per spilled part:\t"
+                    + (spilledTupleCount / spilledStatus.cardinality()) + "\tavg size per part(MB):\t"
+                    + ((double) spilledPartByteSize / 1048576 / spilledStatus.cardinality()) + "\n");
+        }
+        buf.append("(B) In-memory partitions" + "\n");
+        int inMemoryTupleCount = 0;
+        int inMemoryPartByteSize = 0;
+        for (int pid = spilledStatus.nextClearBit(0); pid >= 0
+                && pid < numOfPartitions; pid = spilledStatus.nextClearBit(pid + 1)) {
+            if (whichSide == SIDE.BUILD) {
+                inMemoryTupleCount += buildPSizeInTups[pid];
+                inMemoryPartByteSize += bufferManager.getPhysicalSize(pid);
+            } else {
+                inMemoryTupleCount += probePSizeInTups[pid];
+                inMemoryPartByteSize += bufferManager.getPhysicalSize(pid);
+            }
+        }
+        if (spilledStatus.cardinality() > 0) {
+            buf.append("# of in-memory tuples:\t" + inMemoryTupleCount + "\tsize(MB):\t"
+                    + ((double) inMemoryPartByteSize / 1048576) + "avg #tuples per spilled part:\t"
+                    + (inMemoryTupleCount / spilledStatus.cardinality()) + "\tavg size per part(MB):\t"
+                    + ((double) inMemoryPartByteSize / 1048576 / (numOfPartitions - spilledStatus.cardinality()))
+                    + "\n");
+        }
+        if (inMemoryTupleCount + spilledTupleCount > 0) {
+            buf.append("# of all tuples:\t" + (inMemoryTupleCount + spilledTupleCount) + "\tsize(MB):\t"
+                    + ((double) (inMemoryPartByteSize + spilledPartByteSize) / 1048576) + " ratio of spilled tuples:\t"
+                    + (spilledTupleCount / (inMemoryTupleCount + spilledTupleCount)) + "\n");
+        } else {
+            buf.append("# of all tuples:\t" + (inMemoryTupleCount + spilledTupleCount) + "\tsize(MB):\t"
+                    + ((double) (inMemoryPartByteSize + spilledPartByteSize) / 1048576) + " ratio of spilled tuples:\t"
+                    + "N/A" + "\n");
+        }
+        return buf.toString();
+    }
 }